#!/usr/bin/env node
'use strict';

var require$$0 = require('node:url');
var vendor = require('./vendor.js');
var require$$6 = require('../external/@socketsecurity/registry/lib/debug');
var logger = require('../external/@socketsecurity/registry/lib/logger');
var utils = require('./utils.js');
var fs = require('node:fs/promises');
var require$$5 = require('node:module');
var constants = require('./constants.js');
var flags = require('./flags.js');
var words = require('../external/@socketsecurity/registry/lib/words');
var prompts = require('../external/@socketsecurity/registry/lib/prompts');
var fs$1 = require('node:fs');
var path = require('node:path');
var spawn = require('../external/@socketsecurity/registry/lib/spawn');
var fs$2 = require('../external/@socketsecurity/registry/lib/fs');
var strings = require('../external/@socketsecurity/registry/lib/strings');
var arrays = require('../external/@socketsecurity/registry/lib/arrays');
var registry = require('../external/@socketsecurity/registry');
var npm = require('../external/@socketsecurity/registry/lib/npm');
var packages = require('../external/@socketsecurity/registry/lib/packages');
var sorts = require('../external/@socketsecurity/registry/lib/sorts');
var regexps = require('../external/@socketsecurity/registry/lib/regexps');
var shadowNpmInject = require('./shadow-npm-inject.js');
var require$$7 = require('../external/@socketsecurity/registry/lib/objects');
var path$1 = require('../external/@socketsecurity/registry/lib/path');
var shadowNpmBin = require('./shadow-npm-bin.js');
var require$$8 = require('../external/@socketsecurity/registry/lib/promises');
var require$$1 = require('node:util');
var os = require('node:os');
var promises = require('node:stream/promises');

var _documentCurrentScript = typeof document !== 'undefined' ? document.currentScript : null;
async function fetchOrgAnalyticsData(time, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgAnalytics(time.toString()), {
    desc: 'analytics data'
  });
}

async function fetchRepoAnalyticsData(repo, time, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getRepoAnalytics(repo, time.toString()), {
    desc: 'analytics data'
  });
}

// Note: Widgets does not seem to actually work as code :'(

const require$5 = require$$5.createRequire(require('node:url').pathToFileURL(__filename).href);
const METRICS = ['total_critical_alerts', 'total_high_alerts', 'total_medium_alerts', 'total_low_alerts', 'total_critical_added', 'total_medium_added', 'total_low_added', 'total_high_added', 'total_critical_prevented', 'total_high_prevented', 'total_medium_prevented', 'total_low_prevented'];

// Note: This maps `new Date(date).getMonth()` to English three letters
const Months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'];
async function outputAnalytics(result, {
  filePath,
  outputKind,
  repo,
  scope,
  time
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(result));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'json') {
    const serialized = utils.serializeResultJson(result);
    if (filePath) {
      try {
        await fs.writeFile(filePath, serialized, 'utf8');
        logger.logger.success(`Data successfully written to ${filePath}`);
      } catch (e) {
        process.exitCode = 1;
        logger.logger.log(utils.serializeResultJson({
          ok: false,
          message: 'File Write Failure',
          cause: 'There was an error trying to write the json to disk'
        }));
      }
    } else {
      logger.logger.log(serialized);
    }
    return;
  }
  const fdata = scope === 'org' ? formatDataOrg(result.data) : formatDataRepo(result.data);
  if (outputKind === 'markdown') {
    const serialized = renderMarkdown(fdata, time, repo);

    // TODO: Do we want to write to file even if there was an error...?
    if (filePath) {
      try {
        await fs.writeFile(filePath, serialized, 'utf8');
        logger.logger.success(`Data successfully written to ${filePath}`);
      } catch (e) {
        logger.logger.error(e);
      }
    } else {
      logger.logger.log(serialized);
    }
  } else {
    displayAnalyticsScreen(fdata);
  }
}
function renderMarkdown(data, days, repoSlug) {
  return `
# Socket Alert Analytics

These are the Socket.dev analytics for the ${repoSlug ? `${repoSlug} repo` : 'org'} of the past ${days} days

${[['Total critical alerts', utils.mdTableStringNumber('Date', 'Counts', data['total_critical_alerts'])], ['Total high alerts', utils.mdTableStringNumber('Date', 'Counts', data['total_high_alerts'])], ['Total critical alerts added to the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_critical_added'])], ['Total high alerts added to the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_high_added'])], ['Total critical alerts prevented from the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_critical_prevented'])], ['Total high alerts prevented from the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_high_prevented'])], ['Total medium alerts prevented from the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_medium_prevented'])], ['Total low alerts prevented from the main branch', utils.mdTableStringNumber('Date', 'Counts', data['total_low_prevented'])]].map(([title, table]) => `
## ${title}

${table}
`.trim()).join('\n\n')}

## Top 5 alert types

${utils.mdTableStringNumber('Name', 'Counts', data['top_five_alert_types'])}
`.trim() + '\n';
}
function displayAnalyticsScreen(data) {
  const ScreenWidget = /*@__PURE__*/require$5('../external/blessed/lib/widgets/screen.js');
  // Lazily access constants.blessedOptions.
  const screen = new ScreenWidget({
    ...constants.blessedOptions
  });
  const GridLayout = /*@__PURE__*/require$5('../external/blessed-contrib/lib/layout/grid.js');
  const grid = new GridLayout({
    rows: 5,
    cols: 4,
    screen
  });
  renderLineCharts(grid, screen, 'Total critical alerts', [0, 0, 1, 2], data['total_critical_alerts']);
  renderLineCharts(grid, screen, 'Total high alerts', [0, 2, 1, 2], data['total_high_alerts']);
  renderLineCharts(grid, screen, 'Total critical alerts added to the main branch', [1, 0, 1, 2], data['total_critical_added']);
  renderLineCharts(grid, screen, 'Total high alerts added to the main branch', [1, 2, 1, 2], data['total_high_added']);
  renderLineCharts(grid, screen, 'Total critical alerts prevented from the main branch', [2, 0, 1, 2], data['total_critical_prevented']);
  renderLineCharts(grid, screen, 'Total high alerts prevented from the main branch', [2, 2, 1, 2], data['total_high_prevented']);
  renderLineCharts(grid, screen, 'Total medium alerts prevented from the main branch', [3, 0, 1, 2], data['total_medium_prevented']);
  renderLineCharts(grid, screen, 'Total low alerts prevented from the main branch', [3, 2, 1, 2], data['total_low_prevented']);
  const BarChart = /*@__PURE__*/require$5('../external/blessed-contrib/lib/widget/charts/bar.js');
  const bar = grid.set(4, 0, 1, 2, BarChart, {
    label: 'Top 5 alert types',
    barWidth: 10,
    barSpacing: 17,
    xOffset: 0,
    maxHeight: 9,
    barBgColor: 'magenta'
  });

  // Must append before setting data.
  screen.append(bar);
  bar.setData({
    titles: Object.keys(data.top_five_alert_types),
    data: Object.values(data.top_five_alert_types)
  });
  screen.render();
  // eslint-disable-next-line n/no-process-exit
  screen.key(['escape', 'q', 'C-c'], () => process.exit(0));
}
function formatDataRepo(data) {
  const sortedTopFiveAlerts = {};
  const totalTopAlerts = {};
  const formattedData = {};
  for (const metric of METRICS) {
    formattedData[metric] = {};
  }
  for (const entry of data) {
    const topFiveAlertTypes = entry['top_five_alert_types'];
    for (const type of Object.keys(topFiveAlertTypes)) {
      const count = topFiveAlertTypes[type] ?? 0;
      if (!totalTopAlerts[type]) {
        totalTopAlerts[type] = count;
      } else if (count > (totalTopAlerts[type] ?? 0)) {
        totalTopAlerts[type] = count;
      }
    }
  }
  for (const entry of data) {
    for (const metric of METRICS) {
      formattedData[metric][formatDate(entry['created_at'])] = entry[metric];
    }
  }
  const topFiveAlertEntries = Object.entries(totalTopAlerts).sort(([_keya, a], [_keyb, b]) => b - a).slice(0, 5);
  for (const [key, value] of topFiveAlertEntries) {
    sortedTopFiveAlerts[key] = value;
  }
  return {
    ...formattedData,
    top_five_alert_types: sortedTopFiveAlerts
  };
}
function formatDataOrg(data) {
  const sortedTopFiveAlerts = {};
  const totalTopAlerts = {};
  const formattedData = {};
  for (const metric of METRICS) {
    formattedData[metric] = {};
  }
  for (const entry of data) {
    const topFiveAlertTypes = entry['top_five_alert_types'];
    for (const type of Object.keys(topFiveAlertTypes)) {
      const count = topFiveAlertTypes[type] ?? 0;
      if (!totalTopAlerts[type]) {
        totalTopAlerts[type] = count;
      } else {
        totalTopAlerts[type] += count;
      }
    }
  }
  for (const metric of METRICS) {
    const formatted = formattedData[metric];
    for (const entry of data) {
      const date = formatDate(entry['created_at']);
      if (!formatted[date]) {
        formatted[date] = entry[metric];
      } else {
        formatted[date] += entry[metric];
      }
    }
  }
  const topFiveAlertEntries = Object.entries(totalTopAlerts).sort(([_keya, a], [_keyb, b]) => b - a).slice(0, 5);
  for (const [key, value] of topFiveAlertEntries) {
    sortedTopFiveAlerts[key] = value;
  }
  return {
    ...formattedData,
    top_five_alert_types: sortedTopFiveAlerts
  };
}
function formatDate(date) {
  return `${Months[new Date(date).getMonth()]} ${new Date(date).getDate()}`;
}
function renderLineCharts(grid, screen, title, coords, data) {
  const LineChart = /*@__PURE__*/require$5('../external/blessed-contrib/lib/widget/charts/line.js');
  const line = grid.set(...coords, LineChart, {
    style: {
      line: 'cyan',
      text: 'cyan',
      baseline: 'black'
    },
    xLabelPadding: 0,
    xPadding: 0,
    xOffset: 0,
    wholeNumbersOnly: true,
    legend: {
      width: 1
    },
    label: title
  });
  screen.append(line);
  const lineData = {
    x: Object.keys(data),
    y: Object.values(data)
  };
  line.setData([lineData]);
}

async function handleAnalytics({
  filePath,
  outputKind,
  repo,
  scope,
  time
}) {
  let result;
  if (scope === 'org') {
    result = await fetchOrgAnalyticsData(time);
  } else if (repo) {
    result = await fetchRepoAnalyticsData(repo, time);
  } else {
    result = {
      ok: false,
      message: 'Missing repository name in command'
    };
  }
  if (result.ok && !result.data.length) {
    result = {
      ok: true,
      message: `The analytics data for this ${scope === 'org' ? 'organization' : 'repository'} is not yet available.`,
      data: []
    };
  }
  await outputAnalytics(result, {
    filePath,
    outputKind,
    repo,
    scope,
    time
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$M
} = constants;
const config$M = {
  commandName: 'analytics',
  description: 'Look up analytics data',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    file: {
      type: 'string',
      description: 'Path to store result, only valid with --json/--markdown'
    }
  },
  help: (command, {
    flags
  }) => `
    Usage
      $ ${command} [options] [ "org" | "repo" <reponame>] [TIME]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: report:write

    The scope is either org or repo level, defaults to org.

    When scope is repo, a repo slug must be given as well.

    The TIME argument must be number 7, 30, or 90 and defaults to 30.

    Options
      ${utils.getFlagListOutput(flags)}

    Examples
      $ ${command} org 7
      $ ${command} repo test-repo 30
      $ ${command} 90
  `
};
const cmdAnalytics = {
  description: config$M.description,
  hidden: config$M.hidden,
  run: run$P
};
async function run$P(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$M,
    importMeta,
    parentName
  });

  // Supported inputs:
  // - []        (no args)
  // - ['org']
  // - ['org', '30']
  // - ['repo', 'name']
  // - ['repo', 'name', '30']
  // - ['30']
  // Validate final values in the next step
  let scope = 'org';
  let time = '30';
  let repoName = '';
  if (cli.input[0] === 'org') {
    if (cli.input[1]) {
      time = cli.input[1];
    }
  } else if (cli.input[0] === 'repo') {
    scope = 'repo';
    if (cli.input[1]) {
      repoName = cli.input[1];
    }
    if (cli.input[2]) {
      time = cli.input[2];
    }
  } else if (cli.input[0]) {
    time = cli.input[0];
  }
  const {
    file,
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const noLegacy = !cli.flags['scope'] && !cli.flags['repo'] && !cli.flags['time'];
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: scope === 'org' || !!repoName,
    message: 'When scope=repo, repo name should be the second argument',
    fail: 'missing'
  }, {
    nook: true,
    test: scope === 'org' || repoName !== '7' && repoName !== '30' && repoName !== '90',
    message: 'When scope is repo, the second arg should be repo, not time',
    fail: 'missing'
  }, {
    test: time === '7' || time === '30' || time === '90',
    message: 'The time filter must either be 7, 30 or 90',
    fail: 'invalid range set, see --help for command arg details.'
  }, {
    nook: true,
    test: !file || !!json || !!markdown,
    message: 'The `--file` flag is only valid when using `--json` or `--markdown`',
    fail: 'bad'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$M);
    return;
  }
  return await handleAnalytics({
    scope,
    time: time === '90' ? 90 : time === '30' ? 30 : 7,
    repo: repoName,
    outputKind,
    filePath: String(file || '')
  });
}

async function fetchAuditLog(config, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  const {
    logType,
    orgSlug,
    outputKind,
    page,
    perPage
  } = {
    __proto__: null,
    ...config
  };
  return await utils.handleApiCall(sockSdk.getAuditLogEvents(orgSlug, {
    // I'm not sure this is used at all.
    outputJson: String(outputKind === 'json'),
    // I'm not sure this is used at all.
    outputMarkdown: String(outputKind === 'markdown'),
    orgSlug,
    type: logType,
    page: String(page),
    per_page: String(perPage)
  }), {
    desc: `audit log for ${orgSlug}`
  });
}

const require$4 = require$$5.createRequire(require('node:url').pathToFileURL(__filename).href);
const {
  REDACTED
} = constants;
async function outputAuditLog(result, {
  logType,
  orgSlug,
  outputKind,
  page,
  perPage
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(await outputAsJson(result, {
      logType,
      orgSlug,
      page,
      perPage
    }));
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log(await outputAsMarkdown(result.data, {
      logType,
      orgSlug,
      page,
      perPage
    }));
    return;
  }
  await outputWithBlessed(result.data, orgSlug);
}
function formatResult(selectedRow, keepQuotes = false) {
  if (!selectedRow) {
    return '(none)';
  }
  // Format the object with spacing but keep the payload compact because
  // that can contain just about anything and spread many lines.
  const obj = {
    ...selectedRow,
    payload: 'REPLACEME'
  };
  const json = JSON.stringify(obj, null, 2).replace(/"payload": "REPLACEME"/, `"payload": ${JSON.stringify(selectedRow.payload ?? {})}`);
  if (keepQuotes) {
    return json;
  }
  return json.replace(/^\s*"([^"]+)?"/gm, '  $1');
}
async function outputAsJson(auditLogs, {
  logType,
  orgSlug,
  page,
  perPage
}) {
  if (!auditLogs.ok) {
    return utils.serializeResultJson(auditLogs);
  }
  return utils.serializeResultJson({
    ok: true,
    data: {
      desc: 'Audit logs for given query',
      // Lazily access constants.ENV.VITEST.
      generated: constants.ENV.VITEST ? REDACTED : new Date().toISOString(),
      logType,
      nextPage: auditLogs.data.nextPage,
      org: orgSlug,
      page,
      perPage,
      logs: auditLogs.data.results.map(log => {
        // Note: The subset is pretty arbitrary
        const {
          created_at,
          event_id,
          ip_address,
          type,
          user_agent,
          user_email
        } = log;
        return {
          event_id,
          created_at,
          ip_address,
          type,
          user_agent,
          user_email
        };
      })
    }
  });
}
async function outputAsMarkdown(auditLogs, {
  logType,
  orgSlug,
  page,
  perPage
}) {
  try {
    const table = utils.mdTable(auditLogs.results, ['event_id', 'created_at', 'type', 'user_email', 'ip_address', 'user_agent']);
    return `
# Socket Audit Logs

These are the Socket.dev audit logs as per requested query.
- org: ${orgSlug}
- type filter: ${logType || '(none)'}
- page: ${page}
- next page: ${auditLogs.nextPage}
- per page: ${perPage}
- generated: ${constants.ENV.VITEST ? REDACTED : new Date().toISOString()}

${table}
`;
  } catch (e) {
    process.exitCode = 1;
    logger.logger.fail('There was a problem converting the logs to Markdown, please try the `--json` flag');
    require$$6.debugFn('error', 'caught: markdown conversion error');
    require$$6.debugDir('inspect', {
      error: e
    });
    return 'Failed to generate the markdown report';
  }
}
async function outputWithBlessed(data, orgSlug) {
  const filteredLogs = data.results;
  const formattedOutput = filteredLogs.map(logs => [logs.event_id ?? '', utils.msAtHome(logs.created_at ?? ''), logs.type ?? '', logs.user_email ?? '', logs.ip_address ?? '', logs.user_agent ?? '']);
  const headers = [' Event id', ' Created at', ' Event type', ' User email', ' IP address', ' User agent'];

  // Note: this temporarily takes over the terminal (just like `man` does).
  const ScreenWidget = /*@__PURE__*/require$4('../external/blessed/lib/widgets/screen.js');
  // Lazily access constants.blessedOptions.
  const screen = new ScreenWidget({
    ...constants.blessedOptions
  });
  // Register these keys first so you can always exit, even when it gets stuck
  // If we don't do this and the code crashes, the user must hard-kill the
  // node process just to exit it. That's very bad UX.
  // eslint-disable-next-line n/no-process-exit
  screen.key(['escape', 'q', 'C-c'], () => process.exit(0));
  const TableWidget = /*@__PURE__*/require$4('../external/blessed-contrib/lib/widget/table.js');
  const tipsBoxHeight = 1; // 1 row for tips box
  const detailsBoxHeight = 20; // bottom N rows for details box. 20 gives 4 lines for condensed payload before it scrolls out of view

  const maxWidths = headers.map(s => s.length + 1);
  formattedOutput.forEach(row => {
    row.forEach((str, i) => {
      maxWidths[i] = Math.max(str.length, maxWidths[i] ?? str.length);
    });
  });
  const table = new TableWidget({
    keys: 'true',
    fg: 'white',
    selectedFg: 'white',
    selectedBg: 'magenta',
    interactive: 'true',
    label: `Audit Logs for ${orgSlug}`,
    width: '100%',
    top: 0,
    bottom: detailsBoxHeight + tipsBoxHeight,
    border: {
      type: 'line',
      fg: 'cyan'
    },
    columnWidth: maxWidths,
    //[10, 30, 40, 25, 15, 200],
    // Note: spacing works as long as you don't reserve more than total width
    columnSpacing: 4,
    truncate: '_'
  });
  const BoxWidget = /*@__PURE__*/require$4('../external/blessed/lib/widgets/box.js');
  const tipsBox = new BoxWidget({
    bottom: detailsBoxHeight,
    // sits just above the details box
    height: tipsBoxHeight,
    width: '100%',
    style: {
      fg: 'yellow',
      bg: 'black'
    },
    tags: true,
    content: `↑/↓: Move    Enter: Select    q/ESC: Quit`
  });
  const detailsBox = new BoxWidget({
    bottom: 0,
    height: detailsBoxHeight,
    width: '100%',
    border: {
      type: 'line',
      fg: 'cyan'
    },
    label: 'Details',
    content: formatResult(filteredLogs[0], true),
    style: {
      fg: 'white'
    }
  });
  table.setData({
    headers: headers,
    data: formattedOutput
  });

  // allow control the table with the keyboard
  table.focus();

  // Stacking order: table (top), tipsBox (middle), detailsBox (bottom)
  screen.append(table);
  screen.append(tipsBox);
  screen.append(detailsBox);

  // Update details box when selection changes
  table.rows.on('select item', () => {
    const selectedIndex = table.rows.selected;
    if (selectedIndex !== undefined && selectedIndex >= 0) {
      const selectedRow = filteredLogs[selectedIndex];
      detailsBox.setContent(formatResult(selectedRow));
      screen.render();
    }
  });
  screen.render();
  screen.key(['return'], () => {
    const selectedIndex = table.rows.selected;
    screen.destroy();
    const selectedRow = formattedOutput[selectedIndex] ? formatResult(filteredLogs[selectedIndex], true) : '(none)';
    logger.logger.log(`Last selection:\n${selectedRow.trim()}`);
  });
}

async function handleAuditLog({
  logType,
  orgSlug,
  outputKind,
  page,
  perPage
}) {
  const auditLogs = await fetchAuditLog({
    logType,
    orgSlug,
    outputKind,
    page,
    perPage
  });
  await outputAuditLog(auditLogs, {
    logType,
    orgSlug,
    outputKind,
    page,
    perPage
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$L,
  SOCKET_WEBSITE_URL: SOCKET_WEBSITE_URL$3
} = constants;
const config$L = {
  commandName: 'audit-log',
  description: 'Look up the audit log for an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    page: {
      type: 'number',
      description: 'Result page to fetch'
    },
    perPage: {
      type: 'number',
      default: 30,
      description: 'Results per page - default is 30'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [FILTER]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: audit-log:list

    This feature requires an Enterprise Plan. To learn more about getting access
    to this feature and many more, please visit ${SOCKET_WEBSITE_URL$3}/pricing

    The type FILTER arg is an enum. Defaults to any. It should be one of these:
      associateLabel, cancelInvitation, changeMemberRole, changePlanSubscriptionSeats,
      createApiToken, createLabel, deleteLabel, deleteLabelSetting, deleteReport,
      deleteRepository, disassociateLabel, joinOrganization, removeMember,
      resetInvitationLink, resetOrganizationSettingToDefault, rotateApiToken,
      sendInvitation, setLabelSettingToDefault, syncOrganization, transferOwnership,
      updateAlertTriage, updateApiTokenCommitter, updateApiTokenMaxQuota,
      updateApiTokenName', updateApiTokenScopes, updateApiTokenVisibility,
      updateLabelSetting, updateOrganizationSetting, upgradeOrganizationPlan

    The page arg should be a positive integer, offset 1. Defaults to 1.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} deleteReport --page 2 --perPage 10
  `
};
const cmdAuditLog = {
  description: config$L.description,
  hidden: config$L.hidden,
  run: run$O
};
async function run$O(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$L,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag,
    page,
    perPage
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['type'];
  let [typeFilter = ''] = cli.input;
  typeFilter = String(typeFilter);
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: /^[a-zA-Z]*$/.test(typeFilter),
    message: 'The filter must be an a-zA-Z string, it is an enum',
    fail: 'it was given but not a-zA-Z'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$L);
    return;
  }
  await handleAuditLog({
    orgSlug,
    outputKind,
    page: Number(page || 0),
    perPage: Number(perPage || 0),
    logType: typeFilter.charAt(0).toUpperCase() + typeFilter.slice(1)
  });
}

async function fetchCreateOrgFullScan(packagePaths, orgSlug, config, options) {
  const {
    branchName,
    commitHash,
    commitMessage,
    committers,
    pullRequest,
    repoName
  } = {
    __proto__: null,
    ...config
  };
  const {
    cwd = process.cwd(),
    defaultBranch,
    pendingHead,
    sdkOptions,
    tmp
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.createOrgFullScan(orgSlug, packagePaths, cwd, {
    ...(branchName ? {
      branch: branchName
    } : {}),
    ...(commitHash ? {
      commit_hash: commitHash
    } : {}),
    ...(commitMessage ? {
      commit_message: commitMessage
    } : {}),
    ...(committers ? {
      committers
    } : {}),
    make_default_branch: String(defaultBranch),
    ...(pullRequest ? {
      pull_request: String(pullRequest)
    } : {}),
    repo: repoName,
    set_as_pending_head: String(pendingHead),
    tmp: String(tmp)
  }), {
    desc: 'to create a scan'
  });
}

async function fetchSupportedScanFileNames(options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getSupportedScanFiles(), {
    desc: 'supported scan file types'
  });
}

/**
 * Finalize a tier1 reachability scan.
 *  - Associates the tier1 reachability scan metadata with the full scan.
 *  - Sets the tier1 reachability scan to "finalized" state.
 */
async function finalizeTier1Scan(tier1ReachabilityScanId, scanId) {
  // we do not use the SDK here because the tier1-reachability-scan/finalize is a hidden
  // endpoint that is not part of the OpenAPI specification.
  return await utils.sendApiRequest('tier1-reachability-scan/finalize', {
    method: 'POST',
    body: {
      tier1_reachability_scan_id: tier1ReachabilityScanId,
      report_run_id: scanId
    }
  });
}

/**
 * This fetches all the relevant pieces of data to generate a report, given a
 * full scan ID.
 */
async function fetchScanData(orgSlug, scanId, options) {
  const {
    includeLicensePolicy,
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  let policyStatus = 'requested...';
  let scanStatus = 'requested...';
  let finishedFetching = false;

  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  function updateScan(desc) {
    scanStatus = desc;
    updateProgress();
  }
  function updatePolicy(desc) {
    policyStatus = desc;
    updateProgress();
  }
  function updateProgress() {
    if (finishedFetching) {
      spinner.stop();
      logger.logger.info(`Scan result: ${scanStatus}. Security policy: ${policyStatus}.`);
    } else {
      spinner.start(`Scan result: ${scanStatus}. Security policy: ${policyStatus}.`);
    }
  }
  async function fetchScanResult() {
    const result = await utils.queryApiSafeText(`orgs/${orgSlug}/full-scans/${encodeURIComponent(scanId)}${includeLicensePolicy ? '?include_license_details=true' : ''}`);
    updateScan(`response received`);
    if (!result.ok) {
      return result;
    }
    const ndJsonString = result.data;

    // This is nd-json; each line is a json object.
    const lines = ndJsonString.split('\n').filter(Boolean);
    let ok = true;
    const data = lines.map(line => {
      try {
        return JSON.parse(line);
      } catch (e) {
        ok = false;
        require$$6.debugFn('error', 'caught: JSON.parse error');
        require$$6.debugDir('inspect', {
          error: e,
          line
        });
        return;
      }
    });
    if (ok) {
      updateScan('success');
      return {
        ok: true,
        data
      };
    }
    updateScan('received invalid JSON response');
    return {
      ok: false,
      message: 'Invalid Socket API response',
      cause: 'The Socket API responded with at least one line that was not valid JSON. Please report if this persists.'
    };
  }
  async function fetchSecurityPolicy() {
    const result = await utils.handleApiCallNoSpinner(sockSdk.getOrgSecurityPolicy(orgSlug), 'GetOrgSecurityPolicy');
    updatePolicy('received policy');
    return result;
  }
  updateProgress();
  const [scan, securityPolicy] = await Promise.all([fetchScanResult().catch(e => {
    updateScan('failure; unknown blocking error occurred');
    return {
      ok: false,
      message: 'Socket API error',
      cause: `Error requesting scan: ${e?.message || '(no error message found)'}${e?.cause ? ` (cause: ${e.cause})` : ''}`
    };
  }), fetchSecurityPolicy().catch(e => {
    updatePolicy('failure; unknown blocking error occurred');
    return {
      ok: false,
      message: 'Socket API error',
      cause: `Error requesting policy: ${e?.message || '(no error message found)'}${e?.cause ? ` (cause: ${e.cause})` : ''}`
    };
  })]).finally(() => {
    finishedFetching = true;
    updateProgress();
  });
  if (!scan.ok) {
    return scan;
  }
  if (!securityPolicy.ok) {
    return securityPolicy;
  }
  if (!Array.isArray(scan.data)) {
    return {
      ok: false,
      message: 'Failed to fetch',
      cause: 'Was unable to fetch scan result, bailing'
    };
  }
  return {
    ok: true,
    data: {
      scan: scan.data,
      securityPolicy: securityPolicy.data
    }
  };
}

// Note: The returned cresult will only be ok:false when the generation
//       failed. It won't reflect the healthy state.
function generateReport(scan, securityPolicy, {
  fold,
  orgSlug,
  reportLevel,
  scanId,
  short,
  spinner
}) {
  const now = Date.now();
  spinner?.start('Generating report...');

  // Create an object that includes:
  //   healthy: boolean
  //   worst violation level;
  //   per eco
  //     per package
  //       per version
  //         per offending file
  //           reported issue -> policy action

  // In the context of a report;
  // - the alert.severity is irrelevant
  // - the securityPolicyDefault is irrelevant
  // - the report defaults to healthy:true with no alerts
  // - the appearance of an alert will trigger the policy action;
  //   - error: healthy will end up as false, add alerts to report
  //   - warn: healthy unchanged, add alerts to report
  //   - monitor/ignore: no action
  //   - defer: unknown (no action)

  // Note: the server will emit alerts for license policy violations but
  //       those are only included if you set the flag when requesting the scan
  //       data. The alerts map to a single security policy key that determines
  //       what to do with any violation, regardless of the concrete license.
  //       That rule is called "License Policy Violation".
  // The license policy part is implicitly handled here. Either they are
  // included and may show up, or they are not and won't show up.

  const violations = new Map();
  let healthy = true;
  const securityRules = securityPolicy.securityPolicyRules;
  if (securityRules) {
    // Note: reportLevel: error > warn > monitor > ignore > defer
    scan.forEach(artifact => {
      const {
        alerts,
        name: pkgName = '<unknown>',
        type: ecosystem,
        version = '<unknown>'
      } = artifact;
      alerts?.forEach(alert => {
        const alertName = alert.type; // => policy[type]
        const action = securityRules[alertName]?.action || '';
        switch (action) {
          case 'error':
            {
              healthy = false;
              if (!short) {
                addAlert(artifact, violations, fold, ecosystem, pkgName, version, alert, action);
              }
              break;
            }
          case 'warn':
            {
              if (!short && reportLevel !== 'error') {
                addAlert(artifact, violations, fold, ecosystem, pkgName, version, alert, action);
              }
              break;
            }
          case 'monitor':
            {
              if (!short && reportLevel !== 'warn' && reportLevel !== 'error') {
                addAlert(artifact, violations, fold, ecosystem, pkgName, version, alert, action);
              }
              break;
            }
          case 'ignore':
            {
              if (!short && reportLevel !== 'warn' && reportLevel !== 'error' && reportLevel !== 'monitor') {
                addAlert(artifact, violations, fold, ecosystem, pkgName, version, alert, action);
              }
              break;
            }
          case 'defer':
            {
              // Not sure but ignore for now. Defer to later ;)
              if (!short && reportLevel === 'defer') {
                addAlert(artifact, violations, fold, ecosystem, pkgName, version, alert, action);
              }
              break;
            }
        }
      });
    });
  }
  spinner?.successAndStop(`Generated reported in ${Date.now() - now} ms`);
  if (short) {
    return {
      ok: true,
      data: {
        healthy
      }
    };
  }
  const report = {
    healthy,
    orgSlug,
    scanId,
    options: {
      fold,
      reportLevel
    },
    alerts: violations
  };
  if (!healthy) {
    return {
      ok: true,
      message: 'The report contains at least one alert that violates the policies set by your organization',
      data: report
    };
  }
  return {
    ok: true,
    data: report
  };
}
function createLeaf(art, alert, policyAction) {
  const leaf = {
    type: alert.type,
    policy: policyAction,
    url: utils.getSocketDevPackageOverviewUrlFromPurl(art),
    manifest: art.manifestFiles?.map(o => o.file) ?? []
  };
  return leaf;
}
function addAlert(art, violations, foldSetting, ecosystem, pkgName, version, alert, policyAction) {
  if (!violations.has(ecosystem)) {
    violations.set(ecosystem, new Map());
  }
  const ecomap = violations.get(ecosystem);
  if (foldSetting === 'pkg') {
    const existing = ecomap.get(pkgName);
    if (!existing || isStricterPolicy(existing.policy, policyAction)) {
      ecomap.set(pkgName, createLeaf(art, alert, policyAction));
    }
  } else {
    if (!ecomap.has(pkgName)) {
      ecomap.set(pkgName, new Map());
    }
    const pkgmap = ecomap.get(pkgName);
    if (foldSetting === 'version') {
      const existing = pkgmap.get(version);
      if (!existing || isStricterPolicy(existing.policy, policyAction)) {
        pkgmap.set(version, createLeaf(art, alert, policyAction));
      }
    } else {
      if (!pkgmap.has(version)) {
        pkgmap.set(version, new Map());
      }
      const file = alert.file || '<unknown>';
      const vermap = pkgmap.get(version);
      if (foldSetting === 'file') {
        const existing = vermap.get(file);
        if (!existing || isStricterPolicy(existing.policy, policyAction)) {
          vermap.set(file, createLeaf(art, alert, policyAction));
        }
      } else {
        if (!vermap.has(file)) {
          vermap.set(file, new Map());
        }
        const key = `${alert.type} at ${alert.start}:${alert.end}`;
        const filemap = vermap.get(file);
        const existing = filemap.get(key);
        if (!existing || isStricterPolicy(existing.policy, policyAction)) {
          filemap.set(key, createLeaf(art, alert, policyAction));
        }
      }
    }
  }
}
function isStricterPolicy(was, is) {
  // error > warn > monitor > ignore > defer > {unknown}
  if (was === 'error') {
    return false;
  }
  if (is === 'error') {
    return true;
  }
  if (was === 'warn') {
    return false;
  }
  if (is === 'warn') {
    return false;
  }
  if (was === 'monitor') {
    return false;
  }
  if (is === 'monitor') {
    return false;
  }
  if (was === 'ignore') {
    return false;
  }
  if (is === 'ignore') {
    return false;
  }
  if (was === 'defer') {
    return false;
  }
  if (is === 'defer') {
    return false;
  }
  // unreachable?
  return false;
}

async function outputScanReport(result, {
  filePath,
  fold,
  includeLicensePolicy,
  orgSlug,
  outputKind,
  reportLevel,
  scanId,
  short
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(result));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const scanReport = generateReport(result.data.scan, result.data.securityPolicy, {
    orgSlug,
    scanId,
    fold,
    reportLevel,
    short,
    // Lazily access constants.spinner.
    spinner: constants.spinner
  });
  if (!scanReport.ok) {
    // Note: this means generation failed, it does not reflect the healthy state
    process.exitCode = scanReport.code ?? 1;

    // If report generation somehow failed then .data should not be set.
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(scanReport));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(scanReport.message, scanReport.cause));
    return;
  }

  // I don't think we emit the default error message with banner for an unhealhty report, do we?
  // if (!scanReport.data.healhty) {
  //   logger.fail(failMsgWithBadge(scanReport.message, scanReport.cause))
  //   return
  // }

  if (outputKind === 'json' || outputKind === 'text' && filePath && filePath.endsWith('.json')) {
    const json = short ? utils.serializeResultJson(scanReport) : toJsonReport(scanReport.data, includeLicensePolicy);
    if (filePath && filePath !== '-') {
      logger.logger.log('Writing json report to', filePath);
      return await fs.writeFile(filePath, json);
    }
    logger.logger.log(json);
    return;
  }
  if (outputKind === 'markdown' || filePath && filePath.endsWith('.md')) {
    const md = short ? `healthy = ${scanReport.data.healthy}` : toMarkdownReport(scanReport.data,
    // not short so must be regular report
    includeLicensePolicy);
    if (filePath && filePath !== '-') {
      logger.logger.log('Writing markdown report to', filePath);
      return await fs.writeFile(filePath, md);
    }
    logger.logger.log(md);
    logger.logger.log('');
    return;
  }
  if (short) {
    logger.logger.log(scanReport.data.healthy ? 'OK' : 'ERR');
  } else {
    logger.logger.dir(scanReport.data, {
      depth: null
    });
  }
}
function toJsonReport(report, includeLicensePolicy) {
  const obj = utils.mapToObject(report.alerts);
  const newReport = {
    includeLicensePolicy,
    ...report,
    alerts: obj
  };
  return utils.serializeResultJson({
    ok: true,
    data: newReport
  });
}
function toMarkdownReport(report, includeLicensePolicy) {
  const flatData = Array.from(utils.walkNestedMap(report.alerts)).map(({
    keys,
    value
  }) => {
    const {
      manifest,
      policy,
      type,
      url
    } = value;
    return {
      'Alert Type': type,
      Package: keys[1] || '<unknown>',
      'Introduced by': keys[2] || '<unknown>',
      url,
      'Manifest file': manifest.join(', '),
      Policy: policy
    };
  });
  const md = `
# Scan Policy Report

This report tells you whether the results of a Socket scan results violate the
security${includeLicensePolicy ? ' or license' : ''} policy set by your organization.

## Health status

${report.healthy ? `The scan *PASSES* all requirements set by your security${includeLicensePolicy ? ' and license' : ''} policy.` : 'The scan *VIOLATES* one or more policies set to the "error" level.'}

## Settings

Configuration used to generate this report:

- Organization: ${report.orgSlug}
- Scan ID: ${report.scanId}
- Alert folding: ${report.options.fold === 'none' ? 'none' : `up to ${report.options.fold}`}
- Minimal policy level for alert to be included in report: ${report.options.reportLevel === 'defer' ? 'everything' : report.options.reportLevel}
- Include license alerts: ${includeLicensePolicy ? 'yes' : 'no'}

## Alerts

${report.alerts.size ? `All the alerts from the scan with a policy set to at least "${report.options.reportLevel}".` : `The scan contained no alerts with a policy set to at least "${report.options.reportLevel}".`}

${!report.alerts.size ? '' : utils.mdTable(flatData, ['Policy', 'Alert Type', 'Package', 'Introduced by', 'url', 'Manifest file'])}
  `.trim() + '\n';
  return md;
}

async function handleScanReport({
  filePath,
  fold,
  includeLicensePolicy,
  orgSlug,
  outputKind,
  reportLevel,
  scanId,
  short
}) {
  const scanDataCResult = await fetchScanData(orgSlug, scanId, {
    includeLicensePolicy
  });
  await outputScanReport(scanDataCResult, {
    filePath,
    fold,
    scanId: scanId,
    includeLicensePolicy,
    orgSlug,
    outputKind,
    reportLevel,
    short
  });
}

async function outputCreateNewScan(result, options) {
  const {
    interactive = false,
    outputKind = 'text',
    // Lazily access constants.spinner.
    spinner = constants.spinner
  } = {
    __proto__: null,
    ...options
  };
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  const wasSpinning = !!spinner?.isSpinning;
  spinner?.stop();
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    if (wasSpinning) {
      spinner.start();
    }
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    if (wasSpinning) {
      spinner.start();
    }
    return;
  }
  if (!result.data.id) {
    logger.logger.fail('Did not receive a scan ID from the API.');
    process.exitCode = 1;
  }
  if (outputKind === 'markdown') {
    logger.logger.log('# Create New Scan');
    logger.logger.log('');
    if (result.data.id) {
      logger.logger.log(`A [new Scan](${result.data.html_report_url}) was created with ID: ${result.data.id}`);
      logger.logger.log('');
    } else {
      logger.logger.log(`The server did not return a Scan ID while trying to create a new Scan. This could be an indication something went wrong.`);
    }
    logger.logger.log('');
    if (wasSpinning) {
      spinner.start();
    }
    return;
  }
  const link = vendor.yoctocolorsCjsExports.underline(vendor.yoctocolorsCjsExports.cyan(`${result.data.html_report_url}`));
  logger.logger.log('');
  logger.logger.log(`Available at: ${link}`);
  if (interactive && (await prompts.confirm({
    message: 'Would you like to open it in your browser?',
    default: false
  }, {
    spinner
  }))) {
    await vendor.open(`${result.data.html_report_url}`);
  }
  if (wasSpinning) {
    spinner.start();
  }
}

async function performReachabilityAnalysis(options) {
  const {
    branchName,
    cwd = process.cwd(),
    orgSlug,
    packagePaths,
    reachabilityOptions,
    repoName,
    spinner,
    uploadManifests = true
  } = {
    __proto__: null,
    ...options
  };
  let tarHash;
  if (uploadManifests && orgSlug && packagePaths) {
    // Setup SDK for uploading manifests
    const sockSdkCResult = await utils.setupSdk();
    if (!sockSdkCResult.ok) {
      return sockSdkCResult;
    }
    const sockSdk = sockSdkCResult.data;
    const wasSpinning = !!spinner?.isSpinning;

    // Exclude .socket.facts.json from upload.
    const filepathsToUpload = packagePaths.filter(p => !p.endsWith(constants.DOT_SOCKET_DOT_FACTS_JSON));
    spinner?.start('Uploading manifests for reachability analysis...');
    const uploadCResult = await utils.handleApiCall(sockSdk.uploadManifestFiles(orgSlug, filepathsToUpload), {
      desc: 'upload manifests',
      spinner
    });
    spinner?.stop();
    if (!uploadCResult.ok) {
      if (wasSpinning) {
        spinner.start();
      }
      return uploadCResult;
    }
    tarHash = uploadCResult.data?.tarHash;
    if (!tarHash) {
      if (wasSpinning) {
        spinner.start();
      }
      return {
        ok: false,
        message: 'Failed to get manifest tar hash',
        cause: 'Server did not return a tar hash for the uploaded manifests'
      };
    }
    spinner?.start();
    spinner?.success(`Manifests uploaded successfully. Tar hash: ${tarHash}`);
  }
  spinner?.start();
  spinner?.infoAndStop('Running reachability analysis with Coana...');

  // Build Coana arguments.
  const coanaArgs = ['run', cwd, '--output-dir', cwd, '--socket-mode', constants.DOT_SOCKET_DOT_FACTS_JSON, '--disable-report-submission', ...(reachabilityOptions.reachAnalysisTimeout ? ['--analysis-timeout', `${reachabilityOptions.reachAnalysisTimeout}`] : []), ...(reachabilityOptions.reachAnalysisMemoryLimit ? ['--memory-limit', `${reachabilityOptions.reachAnalysisMemoryLimit}`] : []), ...(reachabilityOptions.reachDisableAnalytics ? ['--disable-analytics-sharing'] : []), ...(tarHash ? ['--run-without-docker', '--manifests-tar-hash', tarHash] : []),
  // Empty reachEcosystems implies scan all ecosystems.
  ...(reachabilityOptions.reachEcosystems.length ? ['--purl-types', ...reachabilityOptions.reachEcosystems] : []), ...(reachabilityOptions.reachExcludePaths.length ? ['--exclude-dirs', ...reachabilityOptions.reachExcludePaths] : [])];

  // Build environment variables.
  const env = {
    ...process.env
  };
  if (repoName) {
    env['SOCKET_REPO_NAME'] = repoName;
  }
  if (branchName) {
    env['SOCKET_BRANCH_NAME'] = branchName;
  }

  // Run Coana with the manifests tar hash.
  const coanaResult = await utils.spawnCoana(coanaArgs, orgSlug, {
    cwd,
    env,
    spinner,
    stdio: 'inherit'
  });
  const wasSpinning = !!spinner?.isSpinning;
  if (wasSpinning) {
    spinner.start();
  }
  return coanaResult.ok ? {
    ok: true,
    data: {
      // Use the DOT_SOCKET_DOT_FACTS_JSON file for the scan.
      reachabilityReport: constants.DOT_SOCKET_DOT_FACTS_JSON,
      tier1ReachabilityScanId: utils.extractTier1ReachabilityScanId(constants.DOT_SOCKET_DOT_FACTS_JSON)
    }
  } : coanaResult;
}

// The point here is to attempt to detect the various supported manifest files
// the CLI can generate. This would be environments that we can't do server side

async function detectManifestActions(
// Passing in null means we attempt detection for every supported language
// regardless of local socket.json status. Sometimes we want that.
sockJson, cwd = process.cwd()) {
  const output = {
    cdxgen: false,
    // TODO
    count: 0,
    conda: false,
    gradle: false,
    sbt: false
  };
  if (sockJson?.defaults?.manifest?.sbt?.disabled) {
    require$$6.debugLog('notice', '[DEBUG] - sbt auto-detection is disabled in socket.json');
  } else if (fs$1.existsSync(path.join(cwd, 'build.sbt'))) {
    require$$6.debugLog('notice', '[DEBUG] - Detected a Scala sbt build file');
    output.sbt = true;
    output.count += 1;
  }
  if (sockJson?.defaults?.manifest?.gradle?.disabled) {
    require$$6.debugLog('notice', '[DEBUG] - gradle auto-detection is disabled in socket.json');
  } else if (fs$1.existsSync(path.join(cwd, 'gradlew'))) {
    require$$6.debugLog('notice', '[DEBUG] - Detected a gradle build file');
    output.gradle = true;
    output.count += 1;
  }
  if (sockJson?.defaults?.manifest?.conda?.disabled) {
    require$$6.debugLog('notice', '[DEBUG] - conda auto-detection is disabled in socket.json');
  } else {
    const envyml = path.join(cwd, 'environment.yml');
    const hasEnvyml = fs$1.existsSync(envyml);
    const envyaml = path.join(cwd, 'environment.yaml');
    const hasEnvyaml = !hasEnvyml && fs$1.existsSync(envyaml);
    if (hasEnvyml || hasEnvyaml) {
      require$$6.debugLog('notice', '[DEBUG] - Detected an environment.yml Conda file');
      output.conda = true;
      output.count += 1;
    }
  }
  return output;
}

async function convertGradleToMaven({
  bin,
  cwd,
  gradleOpts,
  verbose
}) {
  // TODO: Implement json/md.

  // Note: use resolve because the bin could be an absolute path, away from cwd
  // TODO: what about $PATH resolved commands? (`gradlew` without dir prefix)
  const rBin = path.resolve(cwd, bin);
  const binExists = fs$1.existsSync(rBin);
  const cwdExists = fs$1.existsSync(cwd);
  logger.logger.group('gradle2maven:');
  logger.logger.info(`- executing: \`${rBin}\``);
  if (!binExists) {
    logger.logger.warn(`Warning: It appears the executable could not be found. An error might be printed later because of that.`);
  }
  logger.logger.info(`- src dir: \`${cwd}\``);
  if (!cwdExists) {
    logger.logger.warn(`Warning: It appears the src dir could not be found. An error might be printed later because of that.`);
  }
  logger.logger.groupEnd();
  try {
    // Run gradlew with the init script we provide which should yield zero or more
    // pom files. We have to figure out where to store those pom files such that
    // we can upload them and predict them through the GitHub API. We could do a
    // .socket folder. We could do a socket.pom.gz with all the poms, although
    // I'd prefer something plain-text if it is to be committed.
    // Note: init.gradle will be exported by .config/rollup.dist.config.mjs
    const initLocation = path.join(constants.distPath, 'init.gradle');
    const commandArgs = ['--init-script', initLocation, ...gradleOpts, 'pom'];
    if (verbose) {
      logger.logger.log('[VERBOSE] Executing:', [bin], ', args:', commandArgs);
    }
    logger.logger.log(`Converting gradle to maven from \`${bin}\` on \`${cwd}\` ...`);
    const output = await execGradleWithSpinner(rBin, commandArgs, cwd);
    if (verbose) {
      logger.logger.group('[VERBOSE] gradle stdout:');
      logger.logger.log(output);
      logger.logger.groupEnd();
    }
    if (output.code !== 0) {
      process.exitCode = 1;
      logger.logger.fail(`Gradle exited with exit code ${output.code}`);
      // (In verbose mode, stderr was printed above, no need to repeat it)
      if (!verbose) {
        logger.logger.group('stderr:');
        logger.logger.error(output.stderr);
        logger.logger.groupEnd();
      }
      return;
    }
    logger.logger.success('Executed gradle successfully');
    logger.logger.log('Reported exports:');
    output.stdout.replace(/^POM file copied to: (.*)/gm, (_all, fn) => {
      logger.logger.log('- ', fn);
      return fn;
    });
    logger.logger.log('');
    logger.logger.log('Next step is to generate a Scan by running the `socket scan create` command on the same directory');
  } catch (e) {
    process.exitCode = 1;
    logger.logger.fail('There was an unexpected error while generating manifests' + (verbose ? '' : '  (use --verbose for details)'));
    if (verbose) {
      logger.logger.group('[VERBOSE] error:');
      logger.logger.log(e);
      logger.logger.groupEnd();
    }
  }
}
async function execGradleWithSpinner(bin, commandArgs, cwd) {
  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  let pass = false;
  try {
    logger.logger.info('(Running gradle can take a while, it depends on how long gradlew has to run)');
    logger.logger.info('(It will show no output, you can use --verbose to see its output)');
    spinner.start(`Running gradlew...`);
    const output = await spawn.spawn(bin, commandArgs, {
      // We can pipe the output through to have the user see the result
      // of running gradlew, but then we can't (easily) gather the output
      // to discover the generated files... probably a flag we should allow?
      // stdio: isDebug() ? 'inherit' : undefined,
      cwd
    });
    pass = true;
    const {
      code,
      stderr,
      stdout
    } = output;
    return {
      code,
      stdout,
      stderr
    };
  } finally {
    if (pass) {
      spinner.successAndStop('Gracefully completed gradlew execution.');
    } else {
      spinner.failAndStop('There was an error while trying to run gradlew.');
    }
  }
}

async function convertSbtToMaven({
  bin,
  cwd,
  out,
  sbtOpts,
  verbose
}) {
  // TODO: Implement json/md.

  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  logger.logger.group('sbt2maven:');
  logger.logger.info(`- executing: \`${bin}\``);
  logger.logger.info(`- src dir: \`${cwd}\``);
  logger.logger.groupEnd();
  try {
    spinner.start(`Converting sbt to maven from \`${bin}\` on \`${cwd}\`...`);

    // Run sbt with the init script we provide which should yield zero or more
    // pom files. We have to figure out where to store those pom files such that
    // we can upload them and predict them through the GitHub API. We could do a
    // .socket folder. We could do a socket.pom.gz with all the poms, although
    // I'd prefer something plain-text if it is to be committed.
    const output = await spawn.spawn(bin, ['makePom', ...sbtOpts], {
      cwd
    });
    spinner.stop();
    if (verbose) {
      logger.logger.group('[VERBOSE] sbt stdout:');
      logger.logger.log(output);
      logger.logger.groupEnd();
    }
    if (output.stderr) {
      process.exitCode = 1;
      logger.logger.fail('There were errors while running sbt');
      // (In verbose mode, stderr was printed above, no need to repeat it)
      if (!verbose) {
        logger.logger.group('[VERBOSE] stderr:');
        logger.logger.error(output.stderr);
        logger.logger.groupEnd();
      }
      return;
    }
    const poms = [];
    output.stdout.replace(/Wrote (.*?.pom)\n/g, (_all, fn) => {
      poms.push(fn);
      return fn;
    });
    if (!poms.length) {
      process.exitCode = 1;
      logger.logger.fail('There were no errors from sbt but it seems to not have generated any poms either');
      return;
    }
    // Move the pom file to ...? initial cwd? loc will be an absolute path, or dump to stdout
    // TODO: What do we do with multiple output files? Do we want to dump them to stdout? Raw or with separators or ?
    // TODO: Maybe we can add an option to target a specific file to dump to stdout.
    if (out === '-' && poms.length === 1) {
      logger.logger.log('Result:\n```');
      logger.logger.log(await fs$2.safeReadFile(poms[0]));
      logger.logger.log('```');
      logger.logger.success(`OK`);
    } else if (out === '-') {
      process.exitCode = 1;
      logger.logger.error('');
      logger.logger.fail('Requested output target was stdout but there are multiple generated files');
      logger.logger.error('');
      poms.forEach(fn => logger.logger.info('-', fn));
      if (poms.length > 10) {
        logger.logger.error('');
        logger.logger.fail('Requested output target was stdout but there are multiple generated files');
      }
      logger.logger.error('');
      logger.logger.info('Exiting now...');
      return;
    } else {
      // if (verbose) {
      //   logger.log(
      //     `Moving manifest file from \`${loc.replace(/^\/home\/[^/]*?\//, '~/')}\` to \`${out}\``
      //   )
      // } else {
      //   logger.log('Moving output pom file')
      // }
      // TODO: Do we prefer fs-extra? Renaming can be gnarly on windows and fs-extra's version is better.
      // await renamep(loc, out)
      logger.logger.success(`Generated ${poms.length} pom files`);
      poms.forEach(fn => logger.logger.log('-', fn));
      logger.logger.success(`OK`);
    }
  } catch (e) {
    process.exitCode = 1;
    spinner.stop();
    logger.logger.fail('There was an unexpected error while running this' + (verbose ? '' : ' (use --verbose for details)'));
    if (verbose) {
      logger.logger.group('[VERBOSE] error:');
      logger.logger.log(e);
      logger.logger.groupEnd();
    }
  }
}

function prepareContent(content) {
  return strings.stripAnsi(content.trim());
}
async function convertCondaToRequirements(filename, cwd, verbose) {
  let content;
  if (filename === '-') {
    if (verbose) {
      logger.logger.info(`[VERBOSE] reading input from stdin`);
    }
    const strings = [];
    content = await new Promise((resolve, reject) => {
      process.stdin.on('data', chunk => {
        const input = chunk.toString();
        strings.push(input);
      });
      process.stdin.on('end', () => {
        resolve(prepareContent(strings.join('')));
      });
      process.stdin.on('error', e => {
        if (verbose) {
          logger.logger.error('Unexpected error while reading from stdin:', e);
        }
        reject(e);
      });
      process.stdin.on('close', () => {
        if (strings.length) {
          if (verbose) {
            logger.logger.error('warning: stdin closed explicitly with some data received');
          }
          resolve(prepareContent(strings.join('')));
        } else {
          if (verbose) {
            logger.logger.error('stdin closed explicitly without data received');
          }
          reject(new Error('No data received from stdin'));
        }
      });
    });
    if (!content) {
      return {
        ok: false,
        message: 'Manifest Generation Failed',
        cause: 'No data received from stdin'
      };
    }
  } else {
    const filepath = path.join(cwd, filename);
    if (verbose) {
      logger.logger.info(`[VERBOSE] target: ${filepath}`);
    }
    if (!fs$1.existsSync(filepath)) {
      return {
        ok: false,
        message: 'Manifest Generation Failed',
        cause: `The file was not found at ${filepath}`
      };
    }
    content = fs$1.readFileSync(filepath, 'utf8');
    if (!content) {
      return {
        ok: false,
        message: 'Manifest Generation Failed',
        cause: `File at ${filepath} is empty`
      };
    }
  }
  return {
    ok: true,
    data: {
      content,
      pip: convertCondaToRequirementsFromInput(content)
    }
  };
}

// Just extract the first pip block, if one exists at all.
function convertCondaToRequirementsFromInput(input) {
  let collecting = false;
  let delim = '-';
  let indent = '';
  const keeping = [];
  for (const line of input.split('\n')) {
    const trimmed = line.trim();
    if (!trimmed) {
      // Ignore empty lines.
      continue;
    }
    if (collecting) {
      if (line.startsWith('#')) {
        // Ignore comment lines (keep?).
        continue;
      }
      if (line.startsWith(delim)) {
        // In this case we have a line with the same indentation as the
        // `- pip:` line, so we have reached the end of the pip block.
        break;
      }
      if (!indent) {
        // Store the indentation of the block.
        if (trimmed.startsWith('-')) {
          indent = line.split('-')[0] + '-';
          if (indent.length <= delim.length) {
            // The first line after the `pip:` line does not indent further
            // than that so the block is empty?
            break;
          }
        }
      }
      if (line.startsWith(indent)) {
        keeping.push(line.slice(indent.length).trim());
      } else {
        // Unexpected input. bail.
        break;
      }
    }
    // Note: the line may end with a line comment so don't === it.
    else if (trimmed.startsWith('- pip:')) {
      delim = line.split('-')[0] + '-';
      collecting = true;
    }
  }
  return prepareContent(keeping.join('\n'));
}

async function outputRequirements(result, outputKind, out) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(result));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'json') {
    const json = utils.serializeResultJson(result);
    if (out === '-') {
      logger.logger.log(json);
    } else {
      fs$1.writeFileSync(out, json, 'utf8');
    }
    return;
  }
  if (outputKind === 'markdown') {
    const arr = [];
    arr.push('# Converted Conda file');
    arr.push('');
    arr.push('This is the Conda `environment.yml` file converted to python `requirements.txt`:');
    arr.push('');
    arr.push('```file=requirements.txt');
    arr.push(result.data.pip);
    arr.push('```');
    arr.push('');
    const md = arr.join('\n');
    if (out === '-') {
      logger.logger.log(md);
    } else {
      fs$1.writeFileSync(out, md, 'utf8');
    }
    return;
  }
  if (out === '-') {
    logger.logger.log(result.data.pip);
    logger.logger.log('');
  } else {
    fs$1.writeFileSync(out, result.data.pip, 'utf8');
  }
}

async function handleManifestConda({
  cwd,
  filename,
  out,
  outputKind,
  verbose
}) {
  const data = await convertCondaToRequirements(filename, cwd, verbose);
  await outputRequirements(data, outputKind, out);
}

async function generateAutoManifest({
  cwd,
  detected,
  outputKind,
  verbose
}) {
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  if (verbose) {
    logger.logger.info('Using this socket.json for defaults:', sockJson);
  }
  if (!sockJson?.defaults?.manifest?.sbt?.disabled && detected.sbt) {
    logger.logger.log('Detected a Scala sbt build, generating pom files with sbt...');
    await convertSbtToMaven({
      // Note: `sbt` is more likely to be resolved against PATH env
      bin: sockJson.defaults?.manifest?.sbt?.bin ?? 'sbt',
      cwd,
      out: sockJson.defaults?.manifest?.sbt?.outfile ?? './socket.sbt.pom.xml',
      sbtOpts: sockJson.defaults?.manifest?.sbt?.sbtOpts?.split(' ').map(s => s.trim()).filter(Boolean) ?? [],
      verbose: Boolean(sockJson.defaults?.manifest?.sbt?.verbose)
    });
  }
  if (!sockJson?.defaults?.manifest?.gradle?.disabled && detected.gradle) {
    logger.logger.log('Detected a gradle build (Gradle, Kotlin, Scala), running default gradle generator...');
    await convertGradleToMaven({
      // Note: `gradlew` is more likely to be resolved against cwd.
      // Note: .resolve() won't butcher an absolute path.
      // TODO: `gradlew` (or anything else given) may want to resolve against PATH.
      bin: sockJson.defaults?.manifest?.gradle?.bin ? path.resolve(cwd, sockJson.defaults.manifest.gradle.bin) : path.join(cwd, 'gradlew'),
      cwd,
      verbose: Boolean(sockJson.defaults?.manifest?.gradle?.verbose),
      gradleOpts: sockJson.defaults?.manifest?.gradle?.gradleOpts?.split(' ').map(s => s.trim()).filter(Boolean) ?? []
    });
  }
  if (!sockJson?.defaults?.manifest?.conda?.disabled && detected.conda) {
    logger.logger.log('Detected an environment.yml file, running default Conda generator...');
    await handleManifestConda({
      cwd,
      filename: sockJson.defaults?.manifest?.conda?.infile ?? 'environment.yml',
      outputKind,
      out: sockJson.defaults?.manifest?.conda?.outfile ?? 'requirements.txt',
      verbose: Boolean(sockJson.defaults?.manifest?.conda?.verbose)
    });
  }
}

async function handleCreateNewScan({
  autoManifest,
  branchName,
  commitHash,
  commitMessage,
  committers,
  cwd,
  defaultBranch,
  interactive,
  orgSlug,
  outputKind,
  pendingHead,
  pullRequest,
  reach,
  readOnly,
  repoName,
  report,
  targets,
  tmp
}) {
  if (autoManifest) {
    logger.logger.info('Auto-generating manifest files ...');
    const sockJson = utils.readOrDefaultSocketJson(cwd);
    const detected = await detectManifestActions(sockJson, cwd);
    await generateAutoManifest({
      detected,
      cwd,
      outputKind,
      verbose: false
    });
    logger.logger.info('Auto-generation finished. Proceeding with Scan creation.');
  }
  const supportedFilesCResult = await fetchSupportedScanFileNames();
  if (!supportedFilesCResult.ok) {
    await outputCreateNewScan(supportedFilesCResult, {
      interactive,
      outputKind
    });
    return;
  }

  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  spinner.start('Searching for local files to include in scan...');
  const supportedFiles = supportedFilesCResult.data;
  const packagePaths = await utils.getPackageFilesForScan(targets, supportedFiles, {
    cwd
  });
  spinner.stop();
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: packagePaths.length > 0,
    fail: 'found no eligible files to scan',
    message: 'TARGET (file/dir) must contain matching / supported file types for a scan'
  });
  if (!wasValidInput) {
    return;
  }
  logger.logger.success(`Found ${packagePaths.length} local ${words.pluralize('file', packagePaths.length)}`);
  require$$6.debugDir('inspect', {
    packagePaths
  });
  if (readOnly) {
    logger.logger.log('[ReadOnly] Bailing now');
    return;
  }
  let scanPaths = packagePaths;
  let tier1ReachabilityScanId;

  // If reachability is enabled, perform reachability analysis.
  if (reach.runReachabilityAnalysis) {
    logger.logger.error('');
    logger.logger.info('Starting reachability analysis...');
    spinner.start();
    const reachResult = await performReachabilityAnalysis({
      branchName,
      cwd,
      orgSlug,
      packagePaths,
      reachabilityOptions: reach,
      repoName,
      spinner
    });
    spinner.stop();
    if (!reachResult.ok) {
      await outputCreateNewScan(reachResult, {
        interactive,
        outputKind
      });
      return;
    }
    logger.logger.success('Reachability analysis completed successfully');
    scanPaths = reachResult.data?.reachabilityReport ? [reachResult.data.reachabilityReport] : [];
    tier1ReachabilityScanId = reachResult.data?.tier1ReachabilityScanId;
  }
  const fullScanCResult = await fetchCreateOrgFullScan(scanPaths, orgSlug, {
    commitHash,
    commitMessage,
    committers,
    pullRequest,
    repoName,
    branchName
  }, {
    cwd,
    defaultBranch,
    pendingHead,
    tmp
  });
  const scanId = fullScanCResult.ok ? fullScanCResult.data?.id : undefined;
  if (reach && scanId && tier1ReachabilityScanId) {
    await finalizeTier1Scan(tier1ReachabilityScanId, scanId);
  }
  if (report && fullScanCResult.ok) {
    if (scanId) {
      await handleScanReport({
        filePath: '-',
        fold: 'version',
        includeLicensePolicy: true,
        orgSlug,
        outputKind,
        reportLevel: 'error',
        scanId,
        short: false
      });
    } else {
      await outputCreateNewScan({
        ok: false,
        message: 'Missing Scan ID',
        cause: 'Server did not respond with a scan ID',
        data: fullScanCResult.data
      }, {
        interactive,
        outputKind
      });
    }
  } else {
    spinner.stop();
    await outputCreateNewScan(fullScanCResult, {
      interactive,
      outputKind
    });
  }
}

async function handleCi(autoManifest) {
  const orgSlugCResult = await utils.getDefaultOrgSlug();
  if (!orgSlugCResult.ok) {
    process.exitCode = orgSlugCResult.code ?? 1;
    // Always assume json mode.
    logger.logger.log(utils.serializeResultJson(orgSlugCResult));
    return;
  }
  const orgSlug = orgSlugCResult.data;
  const cwd = process.cwd();
  // Lazily access constants.SOCKET_DEFAULT_BRANCH.
  const branchName = (await utils.gitBranch(cwd)) || constants.SOCKET_DEFAULT_BRANCH;
  // Lazily access constants.SOCKET_DEFAULT_REPOSITORY.
  const repoName = (await utils.getRepoName(cwd)) || constants.SOCKET_DEFAULT_REPOSITORY;
  await handleCreateNewScan({
    autoManifest,
    branchName,
    commitMessage: '',
    commitHash: '',
    committers: '',
    cwd,
    defaultBranch: false,
    interactive: false,
    orgSlug,
    outputKind: 'json',
    // When 'pendingHead' is true, it requires 'branchName' set and 'tmp' false.
    pendingHead: true,
    pullRequest: 0,
    reach: {
      runReachabilityAnalysis: false,
      reachDisableAnalytics: false,
      reachAnalysisTimeout: 0,
      reachAnalysisMemoryLimit: 0,
      reachEcosystems: [],
      reachExcludePaths: []
    },
    repoName,
    readOnly: false,
    report: true,
    targets: ['.'],
    // Don't set 'tmp' when 'pendingHead' is true.
    tmp: false
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$K
} = constants;
const config$K = {
  commandName: 'ci',
  description: 'Shorthand for `socket scan create --report --no-interactive`',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    autoManifest: {
      type: 'boolean',
      // Dev tools in CI environments are not likely to be set up, so this is safer.
      default: false,
      description: 'Auto generate manifest files where detected? See autoManifest flag in `socket scan create`'
    }
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    Options
      ${utils.getFlagListOutput(config$K.flags)}

    This command is intended to use in CI runs to allow automated systems to
    accept or reject a current build. It will use the default org of the
    Socket API token. The exit code will be non-zero when the scan does not pass
    your security policy.

    The --autoManifest flag does the same as the one from \`socket scan create\`
    but is not enabled by default since the CI is less likely to be set up with
    all the necessary dev tooling. Enable it if you want the scan to include
    locally generated manifests like for gradle and sbt.

    Examples
      $ ${command}
      $ ${command} --autoManifest
  `
};
const cmdCI = {
  description: config$K.description,
  hidden: config$K.hidden,
  run: run$N
};
async function run$N(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$K,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$K);
    return;
  }
  await handleCi(Boolean(cli.flags['autoManifest']));
}

async function discoverConfigValue(key) {
  // This will have to be a specific implementation per key because certain
  // keys should request information from particular API endpoints while
  // others should simply return their default value, like endpoint URL.

  if (key !== 'test' && !utils.isSupportedConfigKey(key)) {
    return {
      ok: false,
      message: 'Auto discover failed',
      cause: 'Requested key is not a valid config key.'
    };
  }
  if (key === 'apiBaseUrl') {
    // Return the default value
    return {
      ok: false,
      message: 'Auto discover failed',
      cause: "If you're unsure about the base endpoint URL then simply unset it."
    };
  }
  if (key === 'apiProxy') {
    // I don't think we can auto-discover this with any order of reliability..?
    return {
      ok: false,
      message: 'Auto discover failed',
      cause: 'When uncertain, unset this key. Otherwise ask your network administrator'
    };
  }
  if (key === 'apiToken') {
    return {
      ok: false,
      message: 'Auto discover failed',
      cause: 'You can find/create your API token in your Socket dashboard > settings > API tokens.\nYou should then use `socket login` to login instead of this command.'
    };
  }
  if (key === 'defaultOrg') {
    const hasApiToken = utils.hasDefaultToken();
    if (!hasApiToken) {
      return {
        ok: false,
        message: 'Auto discover failed',
        cause: 'No API token set, must have a token to resolve its default org.'
      };
    }
    const org = await getDefaultOrgFromToken();
    if (!org?.length) {
      return {
        ok: false,
        message: 'Auto discover failed',
        cause: 'Was unable to determine default org for the current API token.'
      };
    }
    if (Array.isArray(org)) {
      return {
        ok: true,
        data: org,
        message: 'These are the orgs that the current API token can access.'
      };
    }
    return {
      ok: true,
      data: org,
      message: 'This is the org that belongs to the current API token.'
    };
  }
  if (key === 'enforcedOrgs') {
    const hasApiToken = utils.hasDefaultToken();
    if (!hasApiToken) {
      return {
        ok: false,
        message: 'Auto discover failed',
        cause: 'No API token set, must have a token to resolve orgs to enforce.'
      };
    }
    const orgs = await getEnforceableOrgsFromToken();
    if (!orgs?.length) {
      return {
        ok: false,
        message: 'Auto discover failed',
        cause: 'Was unable to determine any orgs to enforce for the current API token.'
      };
    }
    return {
      ok: true,
      data: orgs,
      message: 'These are the orgs whose security policy you can enforce.'
    };
  }
  if (key === 'test') {
    return {
      ok: false,
      message: 'Auto discover failed',
      cause: 'congrats, you found the test key'
    };
  }

  // Mostly to please TS, because we're not telling it `key` is keyof LocalConfig
  return {
    ok: false,
    message: 'Auto discover failed',
    cause: 'unreachable?'
  };
}
async function getDefaultOrgFromToken() {
  const orgsCResult = await utils.fetchOrganization();
  if (!orgsCResult.ok) {
    return undefined;
  }
  const {
    organizations
  } = orgsCResult.data;
  const slugs = Array.from(Object.values(organizations)).map(o => o.slug);
  if (slugs.length === 0) {
    return undefined;
  }
  if (slugs.length === 1) {
    return slugs[0];
  }
  return slugs;
}
async function getEnforceableOrgsFromToken() {
  const orgsCResult = await utils.fetchOrganization();
  if (!orgsCResult.ok) {
    return undefined;
  }
  const {
    organizations
  } = orgsCResult.data;
  const slugs = Array.from(Object.values(organizations)).map(o => o.slug);
  if (!slugs.length) {
    return undefined;
  }
  return slugs;
}

async function outputConfigAuto(key, result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log(`# Auto discover config value`);
    logger.logger.log('');
    logger.logger.log(`Attempted to automatically discover the value for config key: "${key}"`);
    logger.logger.log('');
    if (result.ok) {
      logger.logger.log(`The discovered value is: "${result.data}"`);
      if (result.message) {
        logger.logger.log('');
        logger.logger.log(result.message);
      }
    }
    logger.logger.log('');
  } else {
    if (result.message) {
      logger.logger.log(result.message);
      logger.logger.log('');
    }
    logger.logger.log(`- ${key}: ${result.data}`);
    logger.logger.log('');
    if (utils.isReadOnlyConfig()) {
      logger.logger.log('(Unable to persist this value because the config is in read-only mode, meaning it was overridden through env or flag.)');
    } else if (key === 'defaultOrg') {
      const proceed = await prompts.select({
        message: 'Would you like to update the default org in local config to this value?',
        choices: (Array.isArray(result.data) ? result.data : [result.data]).map(slug => ({
          name: 'Yes [' + slug + ']',
          value: slug,
          description: `Use "${slug}" as the default organization`
        })).concat({
          name: 'No',
          value: '',
          description: 'Do not use any of these organizations'
        })
      });
      if (proceed) {
        logger.logger.log(`Setting defaultOrg to "${proceed}"...`);
        const updateResult = utils.updateConfigValue('defaultOrg', proceed);
        if (updateResult.ok) {
          logger.logger.log(`OK. Updated defaultOrg to "${proceed}".\nYou should no longer need to add the org to commands that normally require it.`);
        } else {
          logger.logger.log(utils.failMsgWithBadge(updateResult.message, updateResult.cause));
        }
      } else {
        logger.logger.log('OK. No changes made.');
      }
    } else if (key === 'enforcedOrgs') {
      const proceed = await prompts.select({
        message: 'Would you like to update the enforced orgs in local config to this value?',
        choices: (Array.isArray(result.data) ? result.data : [result.data]).map(slug => ({
          name: 'Yes [' + slug + ']',
          value: slug,
          description: `Enforce the security policy of "${slug}" on this machine`
        })).concat({
          name: 'No',
          value: '',
          description: 'Do not use any of these organizations'
        })
      });
      if (proceed) {
        logger.logger.log(`Setting enforcedOrgs key to "${proceed}"...`);
        const updateResult = utils.updateConfigValue('defaultOrg', proceed);
        if (updateResult.ok) {
          logger.logger.log(`OK. Updated enforcedOrgs to "${proceed}".`);
        } else {
          logger.logger.log(utils.failMsgWithBadge(updateResult.message, updateResult.cause));
        }
      } else {
        logger.logger.log('OK. No changes made.');
      }
    }
  }
}

async function handleConfigAuto({
  key,
  outputKind
}) {
  const result = await discoverConfigValue(key);
  await outputConfigAuto(key, result, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$J
} = constants;
const description$a = 'Automatically discover and set the correct value config item';
const hidden$2 = false;
const cmdConfigAuto = {
  description: description$a,
  hidden: hidden$2,
  run: run$M
};
async function run$M(argv, importMeta, {
  parentName
}) {
  const config = {
    commandName: 'auto',
    description: description$a,
    hidden: hidden$2,
    flags: {
      ...flags.commonFlags,
      ...flags.outputFlags
    },
    help: (command, config) => `
    Usage
      $ ${command} [options] KEY

    Options
      ${utils.getFlagListOutput(config.flags)}

    Attempt to automatically discover the correct value for a given config KEY.

    Examples
      $ ${command} defaultOrg

    Keys:
${utils.getSupportedConfigEntries().map(([key, desc]) => `     - ${key} -- ${desc}`).join('\n')}
  `
  };
  const cli = utils.meowOrExit({
    argv,
    config,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [key = ''] = cli.input;
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: key !== 'test' && utils.isSupportedConfigKey(key),
    message: 'Config key should be the first arg',
    fail: key ? 'invalid config key' : 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$J);
    return;
  }
  await handleConfigAuto({
    key: key,
    outputKind
  });
}

async function outputConfigGet(key, result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const readOnly = utils.isReadOnlyConfig();
  if (outputKind === 'markdown') {
    logger.logger.log(`# Config Value`);
    logger.logger.log('');
    logger.logger.log(`Config key '${key}' has value '${result.data}`);
    if (readOnly) {
      logger.logger.log('');
      logger.logger.log('Note: the config is in read-only mode, meaning at least one key was temporarily\n      overridden from an env var or command flag.');
    }
  } else {
    logger.logger.log(`${key}: ${result.data}`);
    if (readOnly) {
      logger.logger.log('');
      logger.logger.log('Note: the config is in read-only mode, meaning at least one key was temporarily overridden from an env var or command flag.');
    }
  }
}

async function handleConfigGet({
  key,
  outputKind
}) {
  const result = utils.getConfigValue(key);
  await outputConfigGet(key, result, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$I
} = constants;
const config$J = {
  commandName: 'get',
  description: 'Get the value of a local CLI config item',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] KEY

    Retrieve the value for given KEY at this time. If you have overridden the
    config then the value will come from that override.

    Options
      ${utils.getFlagListOutput(config.flags)}

    KEY is an enum. Valid keys:

${utils.getSupportedConfigEntries().map(([key, desc]) => `     - ${key} -- ${desc}`).join('\n')}

    Examples
      $ ${command} defaultOrg
  `
};
const cmdConfigGet = {
  description: config$J.description,
  hidden: config$J.hidden,
  run: run$L
};
async function run$L(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$J,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [key = ''] = cli.input;
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: key === 'test' || utils.isSupportedConfigKey(key),
    message: 'Config key should be the first arg',
    fail: key ? 'invalid config key' : 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$I);
    return;
  }
  await handleConfigGet({
    key: key,
    outputKind
  });
}

async function outputConfigList({
  full,
  outputKind
}) {
  const readOnly = utils.isReadOnlyConfig();
  const supportedConfigKeys = utils.getSupportedConfigKeys();
  if (outputKind === 'json') {
    let failed = false;
    const obj = {};
    for (const key of supportedConfigKeys) {
      const result = utils.getConfigValue(key);
      let value = result.data;
      if (!result.ok) {
        value = `Failed to retrieve: ${result.message}`;
        failed = true;
      } else if (!full && utils.isSensitiveConfigKey(key)) {
        value = '********';
      }
      if (full || value !== undefined) {
        obj[key] = value ?? '<none>';
      }
    }
    if (failed) {
      process.exitCode = 1;
    }
    logger.logger.log(utils.serializeResultJson(failed ? {
      ok: false,
      message: 'At least one config key failed to be fetched...',
      data: JSON.stringify({
        full,
        config: obj,
        readOnly
      })
    } : {
      ok: true,
      data: {
        full,
        config: obj,
        readOnly
      }
    }));
  } else {
    const maxWidth = supportedConfigKeys.reduce((a, b) => Math.max(a, b.length), 0);
    logger.logger.log('# Local CLI Config');
    logger.logger.log('');
    logger.logger.log(`This is the local CLI config (full=${!!full}):`);
    logger.logger.log('');
    for (const key of supportedConfigKeys) {
      const result = utils.getConfigValue(key);
      if (!result.ok) {
        logger.logger.log(`- ${key}: failed to read: ${result.message}`);
      } else {
        let value = result.data;
        if (!full && utils.isSensitiveConfigKey(key)) {
          value = '********';
        }
        if (full || value !== undefined) {
          logger.logger.log(`- ${key}:${' '.repeat(Math.max(0, maxWidth - key.length + 3))} ${Array.isArray(value) ? value.join(', ') || '<none>' : value ?? '<none>'}`);
        }
      }
    }
    if (readOnly) {
      logger.logger.log('');
      logger.logger.log('Note: the config is in read-only mode, meaning at least one key was temporarily\n      overridden from an env var or command flag.');
    }
  }
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$H
} = constants;
const config$I = {
  commandName: 'list',
  description: 'Show all local CLI config items and their values',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    full: {
      type: 'boolean',
      default: false,
      description: 'Show full tokens in plaintext (unsafe)'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
  `
};
const cmdConfigList = {
  description: config$I.description,
  hidden: config$I.hidden,
  run: run$K
};
async function run$K(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$I,
    importMeta,
    parentName
  });
  const {
    full,
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$H);
    return;
  }
  await outputConfigList({
    full: !!full,
    outputKind
  });
}

async function outputConfigSet(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log(`# Update config`);
    logger.logger.log('');
    logger.logger.log(result.message);
    if (result.data) {
      logger.logger.log('');
      logger.logger.log(result.data);
    }
  } else {
    logger.logger.log(`OK`);
    logger.logger.log(result.message);
    if (result.data) {
      logger.logger.log('');
      logger.logger.log(result.data);
    }
  }
}

async function handleConfigSet({
  key,
  outputKind,
  value
}) {
  const result = utils.updateConfigValue(key, value);
  await outputConfigSet(result, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$G
} = constants;
const description$9 = 'Update the value of a local CLI config item';
const hidden$1 = false;
const cmdConfigSet = {
  description: description$9,
  hidden: hidden$1,
  run: run$J
};
async function run$J(argv, importMeta, {
  parentName
}) {
  const config = {
    commandName: 'set',
    description: description$9,
    hidden: hidden$1,
    flags: {
      ...flags.commonFlags,
      ...flags.outputFlags
    },
    help: (command, config) => `
    Usage
      $ ${command} [options] <KEY> <VALUE>

    Options
      ${utils.getFlagListOutput(config.flags)}

    This is a crude way of updating the local configuration for this CLI tool.

    Note that updating a value here is nothing more than updating a key/value
    store entry. No validation is happening. The server may reject your values
    in some cases. Use at your own risk.

    Note: use \`socket config unset\` to restore to defaults. Setting a key
    to \`undefined\` will not allow default values to be set on it.

    Keys:

${utils.getSupportedConfigEntries().map(([key, desc]) => `     - ${key} -- ${desc}`).join('\n')}

    Examples
      $ ${command} apiProxy https://example.com
  `
  };
  const cli = utils.meowOrExit({
    argv,
    config,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [key = '', ...rest] = cli.input;
  const value = rest.join(' ');
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: key === 'test' || utils.isSupportedConfigKey(key),
    message: 'Config key should be the first arg',
    fail: key ? 'invalid config key' : 'missing'
  }, {
    test: !!value,
    // This is a string, empty string is not ok
    message: 'Key value should be the remaining args (use `unset` to unset a value)',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$G);
    return;
  }
  await handleConfigSet({
    key: key,
    outputKind,
    value
  });
}

async function outputConfigUnset(updateResult, outputKind) {
  if (!updateResult.ok) {
    process.exitCode = updateResult.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(updateResult));
    return;
  }
  if (!updateResult.ok) {
    logger.logger.fail(utils.failMsgWithBadge(updateResult.message, updateResult.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log(`# Update config`);
    logger.logger.log('');
    logger.logger.log(updateResult.message);
    if (updateResult.data) {
      logger.logger.log('');
      logger.logger.log(updateResult.data);
    }
  } else {
    logger.logger.log(`OK`);
    logger.logger.log(updateResult.message);
    if (updateResult.data) {
      logger.logger.log('');
      logger.logger.log(updateResult.data);
    }
  }
}

async function handleConfigUnset({
  key,
  outputKind
}) {
  const updateResult = utils.updateConfigValue(key, undefined);
  await outputConfigUnset(updateResult, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$F
} = constants;
const description$8 = 'Clear the value of a local CLI config item';
const hidden = false;
const cmdConfigUnset = {
  description: description$8,
  hidden,
  run: run$I
};
async function run$I(argv, importMeta, {
  parentName
}) {
  const config = {
    commandName: 'unset',
    description: description$8,
    hidden,
    flags: {
      ...flags.commonFlags,
      ...flags.outputFlags
    },
    help: (command, config) => `
    Usage
      $ ${command} [options] <KEY> <VALUE>

    Options
      ${utils.getFlagListOutput(config.flags)}

    Removes a value from a config key, allowing the default value to be used
    for it instead.

    Keys:

${utils.getSupportedConfigEntries().map(([key, desc]) => `     - ${key} -- ${desc}`).join('\n')}

    Examples
      $ ${command} defaultOrg
  `
  };
  const cli = utils.meowOrExit({
    argv,
    config,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [key = ''] = cli.input;
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: key === 'test' || utils.isSupportedConfigKey(key),
    message: 'Config key should be the first arg',
    fail: key ? 'invalid config key' : 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$F);
    return;
  }
  await handleConfigUnset({
    key: key,
    outputKind
  });
}

const description$7 = 'Manage Socket CLI configuration';
const cmdConfig = {
  description: description$7,
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      auto: cmdConfigAuto,
      get: cmdConfigGet,
      list: cmdConfigList,
      set: cmdConfigSet,
      unset: cmdConfigUnset
    }, {
      argv,
      description: description$7,
      importMeta,
      name: `${parentName} config`
    });
  }
};

async function coanaFix(fixConfig) {
  const {
    ghsas
  } = fixConfig;
  if (!ghsas.length) {
    return {
      ok: true,
      data: {
        fixed: false
      }
    };
  }
  const {
    cwd,
    orgSlug,
    spinner
  } = fixConfig;
  spinner?.start();
  const sockSdkCResult = await utils.setupSdk();
  let lastCResult = sockSdkCResult;
  const sockSdk = sockSdkCResult.ok ? sockSdkCResult.data : undefined;
  const supportedFilesCResult = sockSdk ? await fetchSupportedScanFileNames() : undefined;
  if (supportedFilesCResult) {
    lastCResult = supportedFilesCResult;
  }
  const supportedFiles = supportedFilesCResult?.ok ? supportedFilesCResult.data : undefined;
  const packagePaths = supportedFiles ? await utils.getPackageFilesForScan(['.'], supportedFiles, {
    cwd
  }) : [];
  const uploadCResult = sockSdk ? await utils.handleApiCall(sockSdk?.uploadManifestFiles(orgSlug, packagePaths), {
    desc: 'upload manifests'
  }) : undefined;
  if (uploadCResult) {
    lastCResult = uploadCResult;
  }
  const tarHash = uploadCResult?.ok ? uploadCResult.data.tarHash : '';
  if (!tarHash) {
    spinner?.stop();
    return lastCResult;
  }
  const isAllOrAuto = ghsas.length === 1 && (ghsas[0] === 'all' || ghsas[0] === 'auto');
  const ids = isAllOrAuto ? ['all'] : ghsas;
  const fixCResult = ids.length ? await utils.spawnCoana(['compute-fixes-and-upgrade-purls', cwd, '--manifests-tar-hash', tarHash, '--apply-fixes-to', ...ids, ...fixConfig.unknownFlags], fixConfig.orgSlug, {
    cwd,
    spinner
  }) : undefined;
  if (fixCResult) {
    lastCResult = fixCResult;
  }
  spinner?.stop();
  require$$6.debugDir('inspect', {
    lastCResult
  });
  return lastCResult.ok ? {
    ok: true,
    data: {
      fixed: true
    }
  } : lastCResult;
}

function formatBranchName(name) {
  return name.replace(/[^-a-zA-Z0-9/._-]+/g, '+');
}
function createSocketBranchParser(options) {
  const pattern = getSocketBranchPattern(options);
  return function parse(branch) {
    const match = pattern.exec(branch);
    if (!match) {
      return null;
    }
    const {
      1: type,
      2: workspace,
      3: fullName,
      4: version,
      5: newVersion
    } = match;
    return {
      fullName,
      newVersion: vendor.semverExports.coerce(newVersion.replaceAll('+', '.'))?.version,
      type,
      workspace,
      version: vendor.semverExports.coerce(version.replaceAll('+', '.'))?.version
    };
  };
}
const genericSocketBranchParser = createSocketBranchParser();
function getSocketBranchFullNameComponent(pkgName) {
  const purlObj = utils.getPurlObject(typeof pkgName === 'string' && !pkgName.startsWith('pkg:') ? vendor.packageurlJsExports.PackageURL.fromString(`pkg:unknown/${pkgName}`) : pkgName);
  const branchMaybeNamespace = purlObj.namespace ? `${formatBranchName(purlObj.namespace)}--` : '';
  return `${branchMaybeNamespace}${formatBranchName(purlObj.name)}`;
}
function getSocketBranchName(purl, newVersion, workspace) {
  const purlObj = utils.getPurlObject(purl);
  const branchType = getSocketBranchPurlTypeComponent(purlObj);
  const branchWorkspace = getSocketBranchWorkspaceComponent(workspace);
  const branchFullName = getSocketBranchFullNameComponent(purlObj);
  const branchVersion = getSocketBranchPackageVersionComponent(purlObj.version);
  const branchNewVersion = formatBranchName(newVersion);
  return `socket/${branchType}/${branchWorkspace}/${branchFullName}_${branchVersion}_${branchNewVersion}`;
}
function getSocketBranchPackageVersionComponent(version) {
  const purlObj = utils.getPurlObject(typeof version === 'string' && !version.startsWith('pkg:') ? vendor.packageurlJsExports.PackageURL.fromString(`pkg:unknown/unknown@${version}`) : version);
  return formatBranchName(purlObj.version);
}
function getSocketBranchPattern(options) {
  const {
    newVersion,
    purl,
    workspace
  } = {
    __proto__: null,
    ...options
  };
  const purlObj = purl ? utils.getPurlObject(purl) : null;
  const escType = purlObj ? regexps.escapeRegExp(purlObj.type) : '[^/]+';
  const escWorkspace = workspace ? `${regexps.escapeRegExp(formatBranchName(workspace))}` : '.+';
  const escMaybeNamespace = purlObj?.namespace ? `${regexps.escapeRegExp(formatBranchName(purlObj.namespace))}--` : '';
  const escFullName = purlObj ? `${escMaybeNamespace}${regexps.escapeRegExp(formatBranchName(purlObj.name))}` : '[^/_]+';
  const escVersion = purlObj ? regexps.escapeRegExp(formatBranchName(purlObj.version)) : '[^_]+';
  const escNewVersion = newVersion ? regexps.escapeRegExp(formatBranchName(newVersion)) : '[^_]+';
  return new RegExp(`^socket/(${escType})/(${escWorkspace})/(${escFullName})_(${escVersion})_(${escNewVersion})$`);
}
function getSocketBranchPurlTypeComponent(purl) {
  const purlObj = utils.getPurlObject(purl);
  return formatBranchName(purlObj.type);
}
function getSocketBranchWorkspaceComponent(workspace) {
  return workspace ? formatBranchName(workspace) : 'root';
}
function getSocketCommitMessage(purl, newVersion, workspace) {
  const purlObj = utils.getPurlObject(purl);
  const fullName = utils.getPkgFullNameFromPurl(purlObj);
  return `socket: Bump ${fullName} from ${purlObj.version} to ${newVersion}${workspace ? ` in ${workspace}` : ''}`;
}
function getSocketPullRequestBody(purl, newVersion, workspace) {
  const purlObj = utils.getPurlObject(purl);
  const fullName = utils.getPkgFullNameFromPurl(purlObj);
  const pkgOverviewUrl = utils.getSocketDevPackageOverviewUrlFromPurl(purlObj);
  return `Bump [${fullName}](${pkgOverviewUrl}) from ${purlObj.version} to ${newVersion}${workspace ? ` in ${workspace}` : ''}.`;
}
function getSocketPullRequestTitle(purl, newVersion, workspace) {
  const purlObj = utils.getPurlObject(purl);
  const fullName = utils.getPkgFullNameFromPurl(purlObj);
  return `Bump ${fullName} from ${purlObj.version} to ${newVersion}${workspace ? ` in ${workspace}` : ''}`;
}

function getPrsForPurl(fixEnv, partialPurl) {
  if (!fixEnv) {
    return [];
  }
  const prs = [];
  const partialPurlObj = utils.getPurlObject(partialPurl);
  const branchFullName = getSocketBranchFullNameComponent(partialPurlObj);
  const branchPurlType = getSocketBranchPurlTypeComponent(partialPurlObj);
  for (const pr of fixEnv.prs) {
    const parsedBranch = genericSocketBranchParser(pr.headRefName);
    if (branchPurlType === parsedBranch?.type && branchFullName === parsedBranch?.fullName) {
      prs.push(pr);
    }
  }
  if (require$$6.isDebug('notice,silly')) {
    const fullName = packages.resolvePackageName(partialPurlObj);
    if (prs.length) {
      require$$6.debugFn('notice', `found: ${prs.length} PRs for ${fullName}`);
      require$$6.debugDir('silly', {
        prs
      });
    } else if (fixEnv.prs.length) {
      require$$6.debugFn('notice', `miss: 0 PRs found for ${fullName}`);
    }
  }
  return prs;
}

let _octokit;
function getOctokit() {
  if (_octokit === undefined) {
    // Lazily access constants.ENV.SOCKET_CLI_GITHUB_TOKEN.
    const {
      SOCKET_CLI_GITHUB_TOKEN
    } = constants.ENV;
    if (!SOCKET_CLI_GITHUB_TOKEN) {
      require$$6.debugFn('notice', 'miss: SOCKET_CLI_GITHUB_TOKEN env var');
    }
    const octokitOptions = {
      auth: SOCKET_CLI_GITHUB_TOKEN,
      // Lazily access constants.ENV.GITHUB_API_URL.
      baseUrl: constants.ENV.GITHUB_API_URL
    };
    require$$6.debugDir('inspect', {
      octokitOptions
    });
    _octokit = new vendor.Octokit(octokitOptions);
  }
  return _octokit;
}
let _octokitGraphql;
function getOctokitGraphql() {
  if (!_octokitGraphql) {
    // Lazily access constants.ENV.SOCKET_CLI_GITHUB_TOKEN.
    const {
      SOCKET_CLI_GITHUB_TOKEN
    } = constants.ENV;
    if (!SOCKET_CLI_GITHUB_TOKEN) {
      require$$6.debugFn('notice', 'miss: SOCKET_CLI_GITHUB_TOKEN env var');
    }
    _octokitGraphql = vendor.graphql2.defaults({
      headers: {
        authorization: `token ${SOCKET_CLI_GITHUB_TOKEN}`
      }
    });
  }
  return _octokitGraphql;
}
async function cacheFetch(key, fetcher, ttlMs) {
  // Optionally disable cache.
  // Lazily access constants.ENV.DISABLE_GITHUB_CACHE.
  if (constants.ENV.DISABLE_GITHUB_CACHE) {
    return await fetcher();
  }
  let data = await readCache(key, ttlMs);
  if (!data) {
    data = await fetcher();
    await writeCache(key, data);
  }
  return data;
}
async function readCache(key,
// 5 minute in milliseconds time to live (TTL).
ttlMs = 5 * 60 * 1000) {
  // Lazily access constants.githubCachePath.
  const cacheJsonPath = path.join(constants.githubCachePath, `${key}.json`);
  const stat = fs$2.safeStatsSync(cacheJsonPath);
  if (stat) {
    const isExpired = Date.now() - stat.mtimeMs > ttlMs;
    if (!isExpired) {
      return await fs$2.readJson(cacheJsonPath);
    }
  }
  return null;
}
async function writeCache(key, data) {
  // Lazily access constants.githubCachePath.
  const {
    githubCachePath
  } = constants;
  const cacheJsonPath = path.join(githubCachePath, `${key}.json`);
  if (!fs$1.existsSync(githubCachePath)) {
    await fs$1.promises.mkdir(githubCachePath, {
      recursive: true
    });
  }
  await fs$2.writeJson(cacheJsonPath, data);
}
async function cleanupPrs(owner, repo, options) {
  const contextualMatches = await getSocketPrsWithContext(owner, repo, options);
  if (!contextualMatches.length) {
    return [];
  }
  const cachesToSave = new Map();
  const {
    newVersion
  } = {
    __proto__: null,
    ...options
  };
  const branchParser = createSocketBranchParser(options);
  const octokit = getOctokit();
  const settledMatches = await Promise.allSettled(contextualMatches.map(async ({
    context,
    match
  }) => {
    const {
      number: prNum
    } = match;
    const prRef = `PR #${prNum}`;
    const parsedBranch = branchParser(match.headRefName);
    const prToVersion = parsedBranch?.newVersion;

    // Close older PRs.
    if (prToVersion && newVersion && vendor.semverExports.lt(prToVersion, newVersion)) {
      try {
        await octokit.pulls.update({
          owner,
          repo,
          pull_number: prNum,
          state: 'closed'
        });
        require$$6.debugFn('notice', `pr: closing ${prRef} for ${prToVersion}`);
        // Remove entry from parent object.
        context.parent.splice(context.index, 1);
        // Mark cache to be saved.
        cachesToSave.set(context.cacheKey, context.data);
        return null;
      } catch (e) {
        require$$6.debugFn('error', `pr: failed to close ${prRef} for ${prToVersion}\n`, e?.message || 'unknown error');
      }
    }
    // Update stale PRs.
    // https://docs.github.com/en/graphql/reference/enums#mergestatestatus
    if (match.mergeStateStatus === 'BEHIND') {
      try {
        await octokit.repos.merge({
          owner,
          repo,
          base: match.headRefName,
          head: match.baseRefName
        });
        require$$6.debugFn('notice', `pr: updating stale ${prRef}`);
        // Update entry entry.
        if (context.apiType === 'graphql') {
          context.entry.mergeStateStatus = 'CLEAN';
        } else if (context.apiType === 'rest') {
          context.entry.mergeable_state = 'clean';
        }
        // Mark cache to be saved.
        cachesToSave.set(context.cacheKey, context.data);
      } catch (e) {
        const message = e?.message || 'Unknown error';
        require$$6.debugFn('error', `pr: failed to update ${prRef} - ${message}`);
      }
    }
    return match;
  }));
  if (cachesToSave.size) {
    await Promise.allSettled(Array.from(cachesToSave).map(({
      0: key,
      1: data
    }) => writeCache(key, data)));
  }
  const fulfilledMatches = settledMatches.filter(r => r.status === 'fulfilled' && r.value);
  return fulfilledMatches.map(r => r.value.match);
}
async function enablePrAutoMerge({
  node_id: prId
}) {
  const octokitGraphql = getOctokitGraphql();
  let error;
  try {
    const response = await octokitGraphql(`
      mutation EnableAutoMerge($pullRequestId: ID!) {
        enablePullRequestAutoMerge(input: {
          pullRequestId: $pullRequestId,
          mergeMethod: SQUASH
        }) {
          pullRequest {
            number
          }
        }
      }`, {
      pullRequestId: prId
    });
    const respPrNumber = response?.enablePullRequestAutoMerge?.pullRequest?.number;
    if (respPrNumber) {
      return {
        enabled: true
      };
    }
  } catch (e) {
    error = e;
  }
  if (error instanceof vendor.GraphqlResponseError && Array.isArray(error.errors) && error.errors.length) {
    const details = error.errors.map(({
      message: m
    }) => m.trim());
    return {
      enabled: false,
      details
    };
  }
  return {
    enabled: false
  };
}
async function getSocketPrs(owner, repo, options) {
  return (await getSocketPrsWithContext(owner, repo, options)).map(d => d.match);
}
async function getSocketPrsWithContext(owner, repo, options) {
  const {
    author,
    states: statesValue = 'all'
  } = {
    __proto__: null,
    ...options
  };
  const branchPattern = getSocketBranchPattern(options);
  const checkAuthor = strings.isNonEmptyString(author);
  const octokit = getOctokit();
  const octokitGraphql = getOctokitGraphql();
  const contextualMatches = [];
  const states = (typeof statesValue === 'string' ? statesValue.toLowerCase() === 'all' ? ['OPEN', 'CLOSED', 'MERGED'] : [statesValue] : statesValue).map(s => s.toUpperCase());
  try {
    // Optimistically fetch only the first 50 open PRs using GraphQL to minimize
    // API quota usage. Fallback to REST if no matching PRs are found.
    const gqlCacheKey = `${repo}-pr-graphql-snapshot`;
    const gqlResp = await cacheFetch(gqlCacheKey, () => octokitGraphql(`
          query($owner: String!, $repo: String!, $states: [PullRequestState!]) {
            repository(owner: $owner, name: $repo) {
              pullRequests(first: 50, states: $states, orderBy: {field: CREATED_AT, direction: DESC}) {
                nodes {
                  author {
                    login
                  }
                  baseRefName
                  headRefName
                  mergeStateStatus
                  number
                  state
                  title
                }
              }
            }
          }
          `, {
      owner,
      repo,
      states
    }));
    const nodes = gqlResp?.repository?.pullRequests?.nodes ?? [];
    for (let i = 0, {
        length
      } = nodes; i < length; i += 1) {
      const node = nodes[i];
      const login = node.author?.login;
      const matchesAuthor = checkAuthor ? login === author : true;
      const matchesBranch = branchPattern.test(node.headRefName);
      if (matchesAuthor && matchesBranch) {
        contextualMatches.push({
          context: {
            apiType: 'graphql',
            cacheKey: gqlCacheKey,
            data: gqlResp,
            entry: node,
            index: i,
            parent: nodes
          },
          match: {
            ...node,
            author: login ?? '<unknown>'
          }
        });
      }
    }
  } catch {}
  if (contextualMatches.length) {
    return contextualMatches;
  }

  // Fallback to REST if GraphQL found no matching PRs.
  let allPrs;
  const cacheKey = `${repo}-pull-requests`;
  try {
    allPrs = await cacheFetch(cacheKey, async () => await octokit.paginate(octokit.pulls.list, {
      owner,
      repo,
      state: 'all',
      per_page: 100
    }));
  } catch {}
  if (!allPrs) {
    return contextualMatches;
  }
  for (let i = 0, {
      length
    } = allPrs; i < length; i += 1) {
    const pr = allPrs[i];
    const login = pr.user?.login;
    const headRefName = pr.head.ref;
    const matchesAuthor = checkAuthor ? login === author : true;
    const matchesBranch = branchPattern.test(headRefName);
    if (matchesAuthor && matchesBranch) {
      // Upper cased mergeable_state is equivalent to mergeStateStatus.
      // https://docs.github.com/en/rest/pulls/pulls?apiVersion=2022-11-28#get-a-pull-request
      const mergeStateStatus = pr.mergeable_state?.toUpperCase?.() ?? 'UNKNOWN';
      // The REST API does not have a distinct merged state for pull requests.
      // Instead, a merged pull request is represented as a closed pull request
      // with a non-null merged_at timestamp.
      const state = pr.merged_at ? 'MERGED' : pr.state.toUpperCase();
      contextualMatches.push({
        context: {
          apiType: 'rest',
          cacheKey,
          data: allPrs,
          entry: pr,
          index: i,
          parent: allPrs
        },
        match: {
          author: login ?? '<unknown>',
          baseRefName: pr.base.ref,
          headRefName,
          mergeStateStatus,
          number: pr.number,
          state,
          title: pr.title
        }
      });
    }
  }
  return contextualMatches;
}
async function openPr(owner, repo, branch, purl, newVersion, options) {
  const {
    baseBranch = 'main',
    workspace
  } = {
    __proto__: null,
    ...options
  };
  const purlObj = utils.getPurlObject(purl);
  const octokit = getOctokit();
  try {
    const octokitPullsCreateParams = {
      owner,
      repo,
      title: getSocketPullRequestTitle(purlObj, newVersion, workspace),
      head: branch,
      base: baseBranch,
      body: getSocketPullRequestBody(purlObj, newVersion, workspace)
    };
    require$$6.debugDir('inspect', {
      octokitPullsCreateParams
    });
    return await octokit.pulls.create(octokitPullsCreateParams);
  } catch (e) {
    let message = `Failed to open pull request`;
    const errors = e instanceof vendor.RequestError ? e.response?.data?.['errors'] : undefined;
    if (Array.isArray(errors) && errors.length) {
      const details = errors.map(d => `- ${d.message?.trim() ?? `${d.resource}.${d.field} (${d.code})`}`).join('\n');
      message += `:\n${details}`;
    }
    require$$6.debugFn('error', message);
  }
  return null;
}
async function setGitRemoteGithubRepoUrl(owner, repo, token, cwd = process.cwd()) {
  const {
    host
  } = new URL(constants.ENV.GITHUB_SERVER_URL);
  const url = `https://x-access-token:${token}@${host}/${owner}/${repo}`;
  const stdioIgnoreOptions = {
    cwd,
    stdio: require$$6.isDebug('stdio') ? 'inherit' : 'ignore'
  };
  const quotedCmd = `\`git remote set-url origin ${url}\``;
  require$$6.debugFn('stdio', `spawn: ${quotedCmd}`);
  try {
    await spawn.spawn('git', ['remote', 'set-url', 'origin', url], stdioIgnoreOptions);
    return true;
  } catch (e) {
    require$$6.debugFn('error', `caught: ${quotedCmd} failed`);
    require$$6.debugDir('inspect', {
      error: e
    });
  }
  return false;
}

function ciRepoInfo() {
  // Lazily access constants.ENV.GITHUB_REPOSITORY.
  const {
    GITHUB_REPOSITORY
  } = constants.ENV;
  if (!GITHUB_REPOSITORY) {
    require$$6.debugFn('notice', 'miss: GITHUB_REPOSITORY env var');
  }
  const ownerSlashRepo = GITHUB_REPOSITORY;
  const slashIndex = ownerSlashRepo.indexOf('/');
  if (slashIndex === -1) {
    return null;
  }
  return {
    owner: ownerSlashRepo.slice(0, slashIndex),
    repo: ownerSlashRepo.slice(slashIndex + 1)
  };
}
async function getFixEnv() {
  const baseBranch = await utils.getBaseBranch();
  const gitEmail = constants.ENV.SOCKET_CLI_GIT_USER_EMAIL;
  const gitUser = constants.ENV.SOCKET_CLI_GIT_USER_NAME;
  const githubToken = constants.ENV.SOCKET_CLI_GITHUB_TOKEN;
  const isCi = !!(constants.ENV.CI && gitEmail && gitUser && githubToken);
  if (
  // If isCi is false,
  !isCi && (
  // but some CI checks are passing,
  constants.ENV.CI || gitEmail || gitUser || githubToken) &&
  // then log about it when in debug mode.
  require$$6.isDebug('notice')) {
    const envVars = [...(constants.ENV.CI ? [] : ['process.env.CI']), ...(gitEmail ? [] : ['process.env.SOCKET_CLI_GIT_USER_EMAIL']), ...(gitUser ? [] : ['process.env.SOCKET_CLI_GIT_USER_NAME']), ...(githubToken ? [] : ['process.env.GITHUB_TOKEN'])];
    require$$6.debugFn('notice', `miss: fixEnv.isCi is false, expected ${arrays.joinAnd(envVars)} to be set`);
  }
  let repoInfo = null;
  if (isCi) {
    repoInfo = ciRepoInfo();
  }
  if (!repoInfo) {
    if (isCi) {
      require$$6.debugFn('notice', 'falling back to `git remote get-url origin`');
    }
    repoInfo = await utils.getRepoInfo();
  }
  const prs = isCi && repoInfo ? await getSocketPrs(repoInfo.owner, repoInfo.repo, {
    author: gitUser,
    states: 'all'
  }) : [];
  return {
    baseBranch,
    gitEmail,
    githubToken,
    gitUser,
    isCi,
    prs,
    repoInfo
  };
}

async function getActualTree(cwd = process.cwd()) {
  try {
    // @npmcli/arborist DOES have partial support for pnpm structured node_modules
    // folders. However, support is iffy resulting in unhappy paths of errors and hangs.
    // So, to avoid unhappy paths, we restrict our usage to --dry-run loading of the
    // node_modules folder.
    const arb = new shadowNpmInject.Arborist({
      path: cwd,
      ...shadowNpmInject.SAFE_NO_SAVE_ARBORIST_REIFY_OPTIONS_OVERRIDES
    });
    return {
      actualTree: await arb.loadActual()
    };
  } catch (e) {
    return {
      error: e
    };
  }
}

const {
  BUN: BUN$4,
  NPM: NPM$6,
  OVERRIDES: OVERRIDES$2,
  PNPM: PNPM$7,
  RESOLUTIONS: RESOLUTIONS$1,
  VLT: VLT$5,
  YARN_BERRY: YARN_BERRY$4,
  YARN_CLASSIC: YARN_CLASSIC$4
} = constants;
function getOverridesDataBun(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[RESOLUTIONS$1] ?? {};
  return {
    type: YARN_BERRY$4,
    overrides
  };
}

// npm overrides documentation:
// https://docs.npmjs.com/cli/v10/configuring-npm/package-json#overrides
function getOverridesDataNpm(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[OVERRIDES$2] ?? {};
  return {
    type: NPM$6,
    overrides
  };
}

// pnpm overrides documentation:
// https://pnpm.io/package_json#pnpmoverrides
function getOverridesDataPnpm(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[PNPM$7]?.[OVERRIDES$2] ?? {};
  return {
    type: PNPM$7,
    overrides
  };
}
function getOverridesDataVlt(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[OVERRIDES$2] ?? {};
  return {
    type: VLT$5,
    overrides
  };
}

// Yarn resolutions documentation:
// https://yarnpkg.com/configuration/manifest#resolutions
function getOverridesDataYarn(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[RESOLUTIONS$1] ?? {};
  return {
    type: YARN_BERRY$4,
    overrides
  };
}

// Yarn resolutions documentation:
// https://classic.yarnpkg.com/en/docs/selective-version-resolutions
function getOverridesDataYarnClassic(pkgEnvDetails, pkgJson = pkgEnvDetails.editablePkgJson.content) {
  const overrides = pkgJson?.[RESOLUTIONS$1] ?? {};
  return {
    type: YARN_CLASSIC$4,
    overrides
  };
}
function getOverridesData(pkgEnvDetails, pkgJson) {
  switch (pkgEnvDetails.agent) {
    case BUN$4:
      return getOverridesDataBun(pkgEnvDetails, pkgJson);
    case PNPM$7:
      return getOverridesDataPnpm(pkgEnvDetails, pkgJson);
    case VLT$5:
      return getOverridesDataVlt(pkgEnvDetails, pkgJson);
    case YARN_BERRY$4:
      return getOverridesDataYarn(pkgEnvDetails, pkgJson);
    case YARN_CLASSIC$4:
      return getOverridesDataYarnClassic(pkgEnvDetails, pkgJson);
    case NPM$6:
    default:
      return getOverridesDataNpm(pkgEnvDetails, pkgJson);
  }
}

const noopHandler = () => {};
async function agentFix(pkgEnvDetails, actualTree, alertsMap, installer, {
  afterInstall = noopHandler,
  afterUpdate = noopHandler,
  beforeInstall = noopHandler,
  revertInstall = noopHandler
}, fixConfig) {
  const {
    pkgPath: rootPath
  } = pkgEnvDetails;
  const fixEnv = await getFixEnv();
  require$$6.debugDir('inspect', {
    fixEnv
  });
  const {
    autoMerge,
    cwd,
    limit,
    minSatisfying,
    prCheck,
    rangeStyle,
    spinner,
    test,
    testScript
  } = fixConfig;
  let count = 0;
  const infoByPartialPurl = utils.getCveInfoFromAlertsMap(alertsMap, {
    exclude: {
      upgradable: true
    }
  });
  if (!infoByPartialPurl) {
    spinner?.stop();
    logger.logger.info('No fixable vulns found.');
    if (alertsMap.size) {
      require$$6.debugDir('inspect', {
        alertsMap
      });
    } else {
      require$$6.debugFn('inspect', '{ alertsMap: Map(0) {} }');
    }
    return {
      ok: true,
      data: {
        fixed: false
      }
    };
  }
  if (require$$6.isDebug('notice,inspect')) {
    spinner?.stop();
    const partialPurls = Array.from(infoByPartialPurl.keys());
    const {
      length: purlsCount
    } = partialPurls;
    require$$6.debugFn('notice', `found: ${purlsCount} ${words.pluralize('PURL', purlsCount)} with CVEs`);
    require$$6.debugDir('inspect', {
      partialPurls
    });
    spinner?.start();
  }

  // Lazily access constants.packumentCache.
  const {
    packumentCache
  } = constants;
  const workspacePkgJsonPaths = await utils.globWorkspace(pkgEnvDetails.agent, rootPath);
  const pkgJsonPaths = [...workspacePkgJsonPaths,
  // Process the workspace root last since it will add an override to package.json.
  pkgEnvDetails.editablePkgJson.filename];
  const sortedInfoEntries = Array.from(infoByPartialPurl.entries()).sort((a, b) => sorts.naturalCompare(a[0], b[0]));
  const cleanupInfoEntriesLoop = () => {
    logger.logger.dedent();
    spinner?.dedent();
    packumentCache.clear();
  };
  const getModifiedFiles = async (cwd = process.cwd()) => {
    const unstagedCResult = await utils.gitUnstagedModifiedFiles(cwd);
    return unstagedCResult.ok ? unstagedCResult.data.filter(filepath => {
      const basename = path.basename(filepath);
      return basename === 'package.json' || basename === pkgEnvDetails.lockName;
    }) : [];
  };
  const handleInstallFail = error => {
    cleanupInfoEntriesLoop();
    spinner?.stop();
    return {
      ok: false,
      message: 'Install failed',
      cause: `${pkgEnvDetails.agent} install failed${error ? `; ${error}` : ''}`
    };
  };
  const hasModifiedFiles = async (cwd = process.cwd()) => {
    return (await getModifiedFiles(cwd)).length > 0;
  };
  spinner?.stop();
  infoEntriesLoop: for (let i = 0, {
      length
    } = sortedInfoEntries; i < length; i += 1) {
    const isLastInfoEntry = i === length - 1;
    const infoEntry = sortedInfoEntries[i];
    const partialPurlObj = utils.getPurlObject(infoEntry[0]);
    const name = packages.resolvePackageName(partialPurlObj);
    const infos = Array.from(infoEntry[1].values());
    if (!infos.length) {
      require$$6.debugFn('notice', `miss: CVEs expected, but not found, for ${name}`);
      continue infoEntriesLoop;
    }
    logger.logger.log(`Processing '${name}'`);
    logger.logger.indent();
    spinner?.indent();
    if (registry.getManifestData(partialPurlObj.type, name)) {
      require$$6.debugFn('notice', `found: Socket Optimize variant for ${name}`);
    }
    // eslint-disable-next-line no-await-in-loop
    const packument = await packages.fetchPackagePackument(name);
    if (!packument) {
      logger.logger.warn(`Unexpected condition: No packument found for ${name}.\n`);
      cleanupInfoEntriesLoop();
      // Skip to next package.
      continue infoEntriesLoop;
    }
    require$$6.debugDir('inspect', {
      infos
    });
    const availableVersions = Object.keys(packument.versions);
    const prs = getPrsForPurl(fixEnv, infoEntry[0]);
    const warningsForAfter = new Set();
    let changed = false;
    // eslint-disable-next-line no-unused-labels
    for (let j = 0, {
        length: length_j
      } = pkgJsonPaths; j < length_j; j += 1) {
      const isLastPkgJsonPath = j === length_j - 1;
      const pkgJsonPath = pkgJsonPaths[j];
      const pkgPath = path.dirname(pkgJsonPath);
      const isWorkspaceRoot = pkgJsonPath === pkgEnvDetails.editablePkgJson.filename;
      const workspace = isWorkspaceRoot ? 'root' : path.relative(rootPath, pkgPath);
      // actualTree may not be defined on the first iteration of pkgJsonPathsLoop.
      if (!actualTree) {
        if (!fixEnv.isCi) {
          // eslint-disable-next-line no-await-in-loop
          await utils.removeNodeModules(cwd);
        }
        if (fixEnv.isCi && fs$1.existsSync(path.join(rootPath, 'node_modules'))) {
          // eslint-disable-next-line no-await-in-loop
          const treeResult = await getActualTree(cwd);
          const maybeActualTree = treeResult.actualTree;
          if (!maybeActualTree) {
            // Exit early if install fails.
            return handleInstallFail(treeResult.error);
          }
          actualTree = maybeActualTree;
        } else {
          // eslint-disable-next-line no-await-in-loop
          const installResult = await installer(pkgEnvDetails, {
            cwd,
            spinner
          });
          const maybeActualTree = installResult.actualTree;
          if (!maybeActualTree) {
            // Exit early if install fails.
            return handleInstallFail(installResult.error);
          }
          actualTree = maybeActualTree;
        }
        if (!fs$1.existsSync(pkgEnvDetails.lockPath)) {
          // Exit early if lockfile is missing.
          return handleInstallFail(new Error(`Missing lockfile at ${pkgEnvDetails.lockPath}`));
        }
      }
      const oldVersions = arrays.arrayUnique(shadowNpmInject.findPackageNodes(actualTree, name).map(n => n.version).filter(Boolean));
      if (!oldVersions.length) {
        require$$6.debugFn('notice', `skip: ${name} not found`);
        cleanupInfoEntriesLoop();
        // Skip to next package.
        continue infoEntriesLoop;
      }

      // Always re-read the editable package.json to avoid stale mutations
      // across iterations.
      // eslint-disable-next-line no-await-in-loop
      const editablePkgJson = await packages.readPackageJson(pkgJsonPath, {
        editable: true
      });
      const seenBranches = new Set();
      const seenVersions = new Set();
      let hasAnnouncedWorkspace = false;
      let workspaceLogCallCount = logger.logger.logCallCount;
      if (require$$6.isDebug('notice')) {
        require$$6.debugFn('notice', `check: workspace ${workspace}`);
        hasAnnouncedWorkspace = true;
        workspaceLogCallCount = logger.logger.logCallCount;
      }
      oldVersionsLoop: for (const oldVersion of oldVersions) {
        const oldId = `${name}@${oldVersion}`;
        const oldPurl = utils.idToPurl(oldId, partialPurlObj.type);
        const node = shadowNpmInject.findPackageNode(actualTree, name, oldVersion);
        if (!node) {
          require$$6.debugFn('notice', `skip: ${oldId} not found`);
          continue oldVersionsLoop;
        }
        infosLoop: for (const {
          firstPatchedVersionIdentifier,
          vulnerableVersionRange
        } of infos) {
          const newVersion = shadowNpmInject.findBestPatchVersion(node, availableVersions, {
            minSatisfying,
            vulnerableVersionRange
          });
          const newVersionPackument = newVersion ? packument.versions[newVersion] : undefined;
          if (!(newVersion && newVersionPackument)) {
            warningsForAfter.add(`${oldId} not updated: requires >=${firstPatchedVersionIdentifier}`);
            continue infosLoop;
          }
          if (seenVersions.has(newVersion)) {
            continue infosLoop;
          }
          if (vendor.semverExports.gte(oldVersion, newVersion)) {
            require$$6.debugFn('silly', `skip: ${oldId} is >= ${newVersion}`);
            continue infosLoop;
          }
          const branch = getSocketBranchName(oldPurl, newVersion, workspace);
          if (seenBranches.has(branch)) {
            continue infosLoop;
          }
          const pr = prCheck ? prs.find(p => p.headRefName === branch) : undefined;
          if (pr) {
            require$$6.debugFn('notice', `skip: PR #${pr.number} for ${name}@${newVersion} exists`);
            seenBranches.add(branch);
            continue infosLoop;
          }
          if (fixEnv.isCi && (
          // eslint-disable-next-line no-await-in-loop
          await utils.gitRemoteBranchExists(branch, cwd))) {
            require$$6.debugFn('notice', `skip: remote branch "${branch}" for ${name}@${newVersion} exists`);
            seenBranches.add(branch);
            continue infosLoop;
          }
          const {
            overrides: oldOverrides
          } = getOverridesData(pkgEnvDetails, editablePkgJson.content);
          let refRange = oldOverrides?.[`${name}@${vulnerableVersionRange}`];
          if (!strings.isNonEmptyString(refRange)) {
            refRange = oldOverrides?.[name];
          }
          if (!strings.isNonEmptyString(refRange)) {
            refRange = oldVersion;
          }

          // eslint-disable-next-line no-await-in-loop
          await beforeInstall(editablePkgJson, packument, oldVersion, newVersion, vulnerableVersionRange, fixConfig);
          shadowNpmInject.updatePackageJsonFromNode(editablePkgJson, actualTree, node, newVersion, rangeStyle);

          // eslint-disable-next-line no-await-in-loop
          await editablePkgJson.save({
            ignoreWhitespace: true
          });

          // eslint-disable-next-line no-await-in-loop
          await afterUpdate(editablePkgJson, packument, oldVersion, newVersion, vulnerableVersionRange, fixConfig);

          // eslint-disable-next-line no-await-in-loop
          if (!(await hasModifiedFiles(cwd))) {
            require$$6.debugFn('notice', `skip: no changes for ${name}@${newVersion}`);
            seenVersions.add(newVersion);
            // Reset things just in case.
            if (fixEnv.isCi) {
              // eslint-disable-next-line no-await-in-loop
              await utils.gitResetAndClean(fixEnv.baseBranch, cwd);
              // eslint-disable-next-line no-await-in-loop
              await utils.gitCheckoutBranch(fixEnv.baseBranch, cwd);
            }
            continue infosLoop;
          }
          spinner?.start();
          if (!hasAnnouncedWorkspace) {
            hasAnnouncedWorkspace = true;
            workspaceLogCallCount = logger.logger.logCallCount;
          }
          const newId = `${name}@${utils.applyRange(refRange, newVersion, rangeStyle)}`;
          spinner?.info(`Installing ${newId} in ${workspace}.`);
          let error;
          let errored = false;
          try {
            // eslint-disable-next-line no-await-in-loop
            const installResult = await installer(pkgEnvDetails, {
              cwd,
              spinner
            });
            const maybeActualTree = installResult.actualTree;
            if (!maybeActualTree) {
              errored = true;
              error = installResult.error;
            } else if (!fs$1.existsSync(pkgEnvDetails.lockPath)) {
              errored = true;
              error = new Error(`Missing lockfile at ${pkgEnvDetails.lockPath}`);
            } else {
              actualTree = maybeActualTree;
              // eslint-disable-next-line no-await-in-loop
              await afterInstall(editablePkgJson, packument, oldVersion, newVersion, vulnerableVersionRange, fixConfig);
              if (test) {
                spinner?.info(`Testing ${newId} in ${workspace}.`);
                // eslint-disable-next-line no-await-in-loop
                await npm.runNpmScript(testScript, [], {
                  spinner,
                  stdio: 'ignore'
                });
              }
              spinner?.success(`Fixed ${name} in ${workspace}.`);
              seenVersions.add(newVersion);
            }
          } catch (e) {
            error = e;
            errored = true;
          }
          spinner?.stop();

          // Check repoInfo to make TypeScript happy.
          if (!errored && fixEnv.isCi && fixEnv.repoInfo) {
            require$$6.debugFn('notice', 'pr: creating');
            try {
              const pushed =
              // eslint-disable-next-line no-await-in-loop
              (await utils.gitCreateBranch(branch, cwd)) && (
              // eslint-disable-next-line no-await-in-loop
              await utils.gitCheckoutBranch(branch, cwd)) && (
              // eslint-disable-next-line no-await-in-loop
              await utils.gitCommit(getSocketCommitMessage(oldPurl, newVersion, workspace),
              // eslint-disable-next-line no-await-in-loop
              await getModifiedFiles(cwd), {
                cwd,
                email: fixEnv.gitEmail,
                user: fixEnv.gitUser
              })) && (
              // eslint-disable-next-line no-await-in-loop
              await utils.gitPushBranch(branch, cwd));
              if (!pushed) {
                logger.logger.warn('Unexpected condition: Push failed, skipping PR creation.');
                // eslint-disable-next-line no-await-in-loop
                await utils.gitResetAndClean(fixEnv.baseBranch, cwd);
                // eslint-disable-next-line no-await-in-loop
                await utils.gitCheckoutBranch(fixEnv.baseBranch, cwd);
                // eslint-disable-next-line no-await-in-loop
                await utils.gitDeleteBranch(branch, cwd);
                // eslint-disable-next-line no-await-in-loop
                const installResult = await installer(pkgEnvDetails, {
                  cwd,
                  spinner
                });
                const maybeActualTree = installResult.actualTree;
                if (!maybeActualTree) {
                  // Exit early if install fails.
                  return handleInstallFail(installResult.error);
                }
                if (!fs$1.existsSync(pkgEnvDetails.lockPath)) {
                  // Exit early if lockfile is missing.
                  return handleInstallFail(new Error(`Missing lockfile at ${pkgEnvDetails.lockPath}`));
                }
                actualTree = maybeActualTree;
                continue infosLoop;
              }
              seenBranches.add(branch);

              // eslint-disable-next-line no-await-in-loop
              await Promise.allSettled([setGitRemoteGithubRepoUrl(fixEnv.repoInfo.owner, fixEnv.repoInfo.repo, fixEnv.githubToken, cwd), cleanupPrs(fixEnv.repoInfo.owner, fixEnv.repoInfo.repo, {
                newVersion,
                purl: oldPurl,
                workspace
              })]);
              // eslint-disable-next-line no-await-in-loop
              const prResponse = await openPr(fixEnv.repoInfo.owner, fixEnv.repoInfo.repo, branch, oldPurl, newVersion, {
                baseBranch: fixEnv.baseBranch,
                cwd,
                workspace
              });
              if (prResponse) {
                const {
                  data
                } = prResponse;
                const prRef = `PR #${data.number}`;
                logger.logger.success(`Opened ${prRef}.`);
                if (autoMerge) {
                  logger.logger.indent();
                  spinner?.indent();
                  // eslint-disable-next-line no-await-in-loop
                  const {
                    details,
                    enabled
                  } = await enablePrAutoMerge(data);
                  if (enabled) {
                    logger.logger.info(`Auto-merge enabled for ${prRef}.`);
                  } else {
                    const message = `Failed to enable auto-merge for ${prRef}${details ? `:\n${details.map(d => ` - ${d}`).join('\n')}` : '.'}`;
                    logger.logger.error(message);
                  }
                  logger.logger.dedent();
                  spinner?.dedent();
                }
              }
            } catch (e) {
              error = e;
              errored = true;
            }
          } else if (fixEnv.isCi) {
            require$$6.debugFn('notice', 'skip: PR creation');
          }
          if (fixEnv.isCi) {
            spinner?.start();
            // eslint-disable-next-line no-await-in-loop
            await utils.gitResetAndClean(branch, cwd);
            // eslint-disable-next-line no-await-in-loop
            await utils.gitCheckoutBranch(fixEnv.baseBranch, cwd);
            // eslint-disable-next-line no-await-in-loop
            const installResult = await installer(pkgEnvDetails, {
              cwd,
              spinner
            });
            spinner?.stop();
            const maybeActualTree = installResult.actualTree;
            if (maybeActualTree) {
              actualTree = maybeActualTree;
            } else {
              errored = true;
              error = installResult.error;
            }
          }
          if (errored) {
            if (!fixEnv.isCi) {
              spinner?.start();
              // eslint-disable-next-line no-await-in-loop
              await revertInstall(editablePkgJson, packument, oldVersion, newVersion, vulnerableVersionRange, fixConfig);
              // eslint-disable-next-line no-await-in-loop
              await Promise.all([utils.removeNodeModules(cwd), editablePkgJson.save({
                ignoreWhitespace: true
              })]);
              // eslint-disable-next-line no-await-in-loop
              const installResult = await installer(pkgEnvDetails, {
                cwd,
                spinner
              });
              spinner?.stop();
              const maybeActualTree = installResult.actualTree;
              if (!maybeActualTree) {
                // Exit early if install fails.
                return handleInstallFail(installResult.error);
              }
              actualTree = maybeActualTree;
            }
            return {
              ok: false,
              message: 'Update failed',
              cause: `Update failed for ${oldId} in ${workspace}${error ? `; ${error}` : ''}`
            };
          } else {
            changed = true;
          }
          require$$6.debugFn('notice', 'increment: count', count + 1);
          if (++count >= limit) {
            cleanupInfoEntriesLoop();
            // Exit main loop.
            break infoEntriesLoop;
          }
        }
      }
      if (!isLastPkgJsonPath && logger.logger.logCallCount > workspaceLogCallCount) {
        logger.logger.logNewline();
      }
    }
    for (const warningText of warningsForAfter) {
      logger.logger.warn(warningText);
    }
    if (!changed && !warningsForAfter.size) {
      logger.logger.info('No vulnerable versions found.');
    }
    if (!isLastInfoEntry) {
      logger.logger.logNewline();
    }
    cleanupInfoEntriesLoop();
  }
  spinner?.stop();

  // Or, did we change anything?
  return {
    ok: true,
    data: {
      fixed: true
    }
  };
}

const CMD_NAME$1 = 'socket fix';
function getFixAlertsMapOptions(options = {}) {
  return {
    __proto__: null,
    consolidate: true,
    nothrow: true,
    ...options,
    include: {
      __proto__: null,
      existing: true,
      unfixable: false,
      upgradable: false,
      ...options?.include
    }
  };
}

async function install$1(pkgEnvDetails, options) {
  const {
    args: extraArgs,
    cwd,
    spinner
  } = {
    __proto__: null,
    ...options
  };
  const useDebug = require$$6.isDebug('stdio');
  const args = [
  // If "true", npm does not run scripts specified in package.json files.
  // Note that commands explicitly intended to run a particular script, such
  // as `npm start`, `npm stop`, `npm restart`, `npm test`, and `npm run` will
  // still run their intended script if `ignore-scripts` is set, but they will
  // not run any pre- or post-scripts.
  // https://docs.npmjs.com/cli/v11/commands/npm-install#ignore-scripts
  '--ignore-scripts',
  // When "true" submit audit reports alongside the current npm command to the
  // default registry and all registries configured for scopes. See the
  // documentation for `npm audit` for details on what is submitted.
  // https://docs.npmjs.com/cli/v11/commands/npm-install#audit
  '--no-audit',
  // When "true" displays the message at the end of each `npm install` acknowledging
  // the number of dependencies looking for funding. See `npm fund` for details.
  // https://docs.npmjs.com/cli/v11/commands/npm-install#fund
  '--no-fund',
  // When set to "true", npm will display a progress bar during time intensive
  // operations, if `process.stderr` is a TTY. Set to "false" to suppress the
  // progress bar.
  // https://docs.npmjs.com/cli/v8/using-npm/config#progress
  '--no-progress',
  // What level of logs to report. All logs are written to a debug log, with
  // the path to that file printed if the execution of a command fails. The
  // default is "notice".
  // https://docs.npmjs.com/cli/v8/using-npm/config#loglevel
  ...(useDebug ? [] : ['--silent']), ...(extraArgs ?? [])];
  const wasSpinning = !!spinner?.isSpinning;
  spinner?.stop();
  const quotedCmd = `\`${pkgEnvDetails.agent} install ${args.join(' ')}\``;
  require$$6.debugFn('stdio', `spawn: ${quotedCmd}`);
  try {
    await utils.runAgentInstall(pkgEnvDetails, {
      args,
      spinner,
      stdio: useDebug ? 'inherit' : 'ignore'
    });
  } catch (error) {
    const result = {
      error
    };
    require$$6.debugFn('error', `caught: ${quotedCmd} failed`);
    require$$6.debugDir('inspect', result);
    return result;
  }
  const treeResult = await getActualTree(cwd);
  if (treeResult.actualTree) {
    if (wasSpinning) {
      spinner.start();
    }
    return treeResult;
  }
  require$$6.debugFn('error', 'caught: await arb.loadActual() error');
  require$$6.debugDir('inspect', treeResult);
  if (wasSpinning) {
    spinner.start();
  }
  return treeResult;
}
async function npmFix(pkgEnvDetails, fixConfig) {
  const {
    purls,
    spinner
  } = fixConfig;
  spinner?.start();
  const flatConfig = await utils.getNpmConfig({
    npmVersion: pkgEnvDetails.agentVersion
  });
  let actualTree;
  let alertsMap;
  try {
    if (purls.length) {
      alertsMap = await utils.getAlertsMapFromPurls(purls, getFixAlertsMapOptions());
    } else {
      let arb;
      try {
        arb = new shadowNpmInject.Arborist({
          path: pkgEnvDetails.pkgPath,
          ...flatConfig,
          ...shadowNpmInject.SAFE_WITH_SAVE_ARBORIST_REIFY_OPTIONS_OVERRIDES
        });
        // Calling arb.reify() creates the arb.diff object, nulls-out arb.idealTree,
        // and populates arb.actualTree.
        actualTree = await arb.reify();
      } catch (e) {
        spinner?.stop();
        require$$6.debugFn('error', 'caught: await arb.reify() error');
        require$$6.debugDir('inspect', {
          error: e
        });
        return {
          ok: false,
          message: 'npm error',
          cause: e?.message || 'Unknown npm error.'
        };
      }
      alertsMap = await shadowNpmInject.getAlertsMapFromArborist(arb, getFixAlertsMapOptions());
    }
  } catch (e) {
    spinner?.stop();
    require$$6.debugFn('error', 'caught: Socket batch PURL API error');
    require$$6.debugDir('inspect', {
      error: e
    });
    return {
      ok: false,
      message: 'Socket API error',
      cause: e?.message || 'Unknown Socket batch PURL API error.'
    };
  }
  let revertData;
  return await agentFix(pkgEnvDetails, actualTree, alertsMap, install$1, {
    async beforeInstall(editablePkgJson) {
      revertData = {
        // Track existing dependencies in the root package.json to revert to later.
        ...(editablePkgJson.content.dependencies && {
          dependencies: {
            ...editablePkgJson.content.dependencies
          }
        }),
        ...(editablePkgJson.content.optionalDependencies && {
          optionalDependencies: {
            ...editablePkgJson.content.optionalDependencies
          }
        }),
        ...(editablePkgJson.content.peerDependencies && {
          peerDependencies: {
            ...editablePkgJson.content.peerDependencies
          }
        })
      };
    },
    async afterUpdate(editablePkgJson, packument, oldVersion, newVersion) {
      // Exit early if not the root workspace.
      if (editablePkgJson.filename !== pkgEnvDetails.editablePkgJson.filename) {
        return;
      }
      // Update package-lock.json using @npmcli/arborist.
      const arb = new shadowNpmInject.Arborist({
        path: pkgEnvDetails.pkgPath,
        ...flatConfig,
        ...shadowNpmInject.SAFE_WITH_SAVE_ARBORIST_REIFY_OPTIONS_OVERRIDES
      });
      // Build the ideal tree of nodes that are used to generated the saved
      // package-lock.json
      const idealTree = await arb.buildIdealTree();
      const node = shadowNpmInject.findPackageNode(idealTree, packument.name, oldVersion);
      if (node) {
        // Update the ideal tree node.
        shadowNpmInject.updateNode(node, newVersion, packument.versions[newVersion]);
        // Save package-lock.json lockfile.
        await arb.reify();
      }
    },
    async revertInstall(editablePkgJson) {
      if (revertData) {
        // Revert package.json.
        editablePkgJson.update(revertData);
        await editablePkgJson.save({
          ignoreWhitespace: true
        });
      }
    }
  }, fixConfig);
}

async function outputFixResult(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('');
  logger.logger.success('Finished!');
}

const {
  OVERRIDES: OVERRIDES$1,
  PNPM: PNPM$6
} = constants;
async function install(pkgEnvDetails, options) {
  const {
    args: extraArgs,
    cwd,
    spinner
  } = {
    __proto__: null,
    ...options
  };
  const args = [
  // Do not execute any scripts defined in the project package.json and its dependencies.
  // https://pnpm.io/9.x/cli/install#--ignore-scripts
  '--ignore-scripts',
  // Enable pnpm updates to pnpm-lock.yaml in CI environments.
  // https://pnpm.io/cli/install#--frozen-lockfile
  '--no-frozen-lockfile',
  // Enable a non-interactive pnpm install
  // https://github.com/pnpm/pnpm/issues/6778
  '--config.confirmModulesPurge=false', ...(extraArgs ?? [])];
  const wasSpinning = !!spinner?.isSpinning;
  spinner?.stop();
  const quotedCmd = `\`${pkgEnvDetails.agent} install ${args.join(' ')}\``;
  require$$6.debugFn('stdio', `spawn: ${quotedCmd}`);
  try {
    await utils.runAgentInstall(pkgEnvDetails, {
      args,
      spinner,
      stdio: require$$6.isDebug('stdio') ? 'inherit' : 'ignore'
    });
  } catch (error) {
    const result = {
      error
    };
    require$$6.debugFn('error', `caught: ${quotedCmd} failed`);
    require$$6.debugDir('inspect', result);
    return result;
  }
  const treeResult = await getActualTree(cwd);
  if (treeResult.actualTree) {
    if (wasSpinning) {
      spinner.start();
    }
    return treeResult;
  }
  require$$6.debugFn('error', 'caught: await arb.loadActual() error');
  require$$6.debugDir('inspect', treeResult);
  if (wasSpinning) {
    spinner.start();
  }
  return treeResult;
}
async function pnpmFix(pkgEnvDetails, fixConfig) {
  const {
    cwd,
    purls,
    spinner
  } = fixConfig;
  spinner?.start();
  let actualTree;
  let lockSrc = pkgEnvDetails.lockSrc;
  let lockfile = utils.parsePnpmLockfile(lockSrc);
  // Update pnpm-lock.yaml if its version is older than what the installed pnpm
  // produces.
  if (pkgEnvDetails.agentVersion.major >= 10 && (utils.parsePnpmLockfileVersion(lockfile?.lockfileVersion)?.major ?? 0) <= 6) {
    const installResult = await install(pkgEnvDetails, {
      args: ['--lockfile-only'],
      cwd,
      spinner
    });
    const maybeActualTree = installResult.actualTree;
    if (maybeActualTree) {
      lockSrc = (await utils.readLockfile(pkgEnvDetails.lockPath)) ?? '';
    } else {
      lockSrc = '';
    }
    if (lockSrc) {
      actualTree = maybeActualTree;
      lockfile = utils.parsePnpmLockfile(lockSrc);
    } else {
      lockfile = null;
    }
  }

  // Exit early if pnpm-lock.yaml is not found or usable.
  // Check !lockSrc to make TypeScript happy.
  if (!lockfile || !lockSrc) {
    spinner?.stop();
    return {
      ok: false,
      message: 'Missing lockfile',
      cause: 'Required pnpm-lock.yaml not found or usable'
    };
  }
  let alertsMap;
  try {
    alertsMap = purls.length ? await utils.getAlertsMapFromPurls(purls, getFixAlertsMapOptions()) : await utils.getAlertsMapFromPnpmLockfile(lockfile, getFixAlertsMapOptions());
  } catch (e) {
    spinner?.stop();
    require$$6.debugFn('error', 'caught: Socket batch PURL API error');
    require$$6.debugDir('inspect', {
      error: e
    });
    return {
      ok: false,
      message: 'Socket API error',
      cause: e?.message || 'Unknown Socket batch PURL API error.'
    };
  }
  let revertData;
  let revertOverrides;
  let revertOverridesSrc = '';
  return await agentFix(pkgEnvDetails, actualTree, alertsMap, install, {
    async beforeInstall(editablePkgJson, packument, oldVersion, newVersion, vulnerableVersionRange, options) {
      lockSrc = (await utils.readLockfile(pkgEnvDetails.lockPath)) ?? '';

      // Update overrides for the root workspace.
      if (editablePkgJson.filename === pkgEnvDetails.editablePkgJson.filename) {
        const {
          overrides: oldOverrides
        } = getOverridesDataPnpm(pkgEnvDetails, editablePkgJson.content);
        const oldPnpmSection = editablePkgJson.content[PNPM$6];
        const overrideKey = `${packument.name}@${vulnerableVersionRange}`;
        revertOverridesSrc = utils.extractOverridesFromPnpmLockSrc(lockSrc);
        // Track existing overrides in the root package.json to revert to later.
        revertOverrides = {
          [PNPM$6]: oldPnpmSection ? {
            ...oldPnpmSection,
            [OVERRIDES$1]: require$$7.hasKeys(oldOverrides) ? {
              ...oldOverrides,
              [overrideKey]: undefined
            } :
            // Properties with undefined values are deleted when saved as JSON.
            undefined
          } :
          // Properties with undefined values are deleted when saved as JSON.
          undefined
        };
        // Update overrides in the root package.json so that when `pnpm install`
        // generates pnpm-lock.yaml it updates transitive dependencies too.
        editablePkgJson.update({
          [PNPM$6]: {
            ...oldPnpmSection,
            [OVERRIDES$1]: {
              ...oldOverrides,
              [overrideKey]: utils.applyRange(oldOverrides?.[overrideKey] ?? oldVersion, newVersion, options.rangeStyle)
            }
          }
        });
      } else {
        revertOverrides = undefined;
        revertOverridesSrc = '';
      }
      revertData = {
        // If "pnpm" or "pnpm.overrides" fields are undefined they will be
        // deleted when saved.
        ...revertOverrides,
        // Track existing dependencies in the root package.json to revert to later.
        ...(editablePkgJson.content.dependencies && {
          dependencies: {
            ...editablePkgJson.content.dependencies
          }
        }),
        ...(editablePkgJson.content.optionalDependencies && {
          optionalDependencies: {
            ...editablePkgJson.content.optionalDependencies
          }
        }),
        ...(editablePkgJson.content.peerDependencies && {
          peerDependencies: {
            ...editablePkgJson.content.peerDependencies
          }
        })
      };
    },
    async afterInstall(editablePkgJson) {
      if (revertOverrides) {
        // Revert overrides metadata in package.json now that pnpm-lock.yaml
        // has been updated.
        editablePkgJson.update(revertOverrides);
        await editablePkgJson.save({
          ignoreWhitespace: true
        });
      }
      lockSrc = (await utils.readLockfile(pkgEnvDetails.lockPath)) ?? '';
      // Remove "overrides" block from pnpm-lock.yaml lockfile when processing
      // the root workspace.
      if (editablePkgJson.filename === pkgEnvDetails.editablePkgJson.filename) {
        const updatedOverridesContent = utils.extractOverridesFromPnpmLockSrc(lockSrc);
        if (updatedOverridesContent) {
          // Remove "overrides" block from pnpm-lock.yaml lockfile.
          lockSrc = lockSrc.replace(updatedOverridesContent, revertOverridesSrc);
          // Save pnpm-lock.yaml lockfile.
          await fs$1.promises.writeFile(pkgEnvDetails.lockPath, lockSrc, 'utf8');
        }
      }
    },
    async revertInstall(editablePkgJson) {
      if (revertData) {
        // Revert package.json.
        editablePkgJson.update(revertData);
        await editablePkgJson.save({
          ignoreWhitespace: true
        });
        // Revert pnpm-lock.yaml lockfile to be on the safe side.
        await fs$1.promises.writeFile(pkgEnvDetails.lockPath, lockSrc, 'utf8');
      }
    }
  }, fixConfig);
}

async function handleFix({
  autoMerge,
  cwd,
  ghsas,
  limit,
  minSatisfying,
  orgSlug,
  outputKind,
  prCheck,
  purls,
  rangeStyle,
  spinner,
  test,
  testScript,
  unknownFlags
}) {
  if (ghsas.length) {
    await outputFixResult(await coanaFix({
      cwd,
      ghsas,
      orgSlug,
      spinner,
      unknownFlags
    }), outputKind);
    return;
  }
  const pkgEnvCResult = await utils.detectAndValidatePackageEnvironment(cwd, {
    cmdName: CMD_NAME$1,
    logger: logger.logger
  });
  if (!pkgEnvCResult.ok) {
    await outputFixResult(pkgEnvCResult, outputKind);
    return;
  }
  const {
    data: pkgEnvDetails
  } = pkgEnvCResult;
  if (!pkgEnvDetails) {
    await outputFixResult({
      ok: false,
      message: 'No package found.',
      cause: `No valid package environment found for project path: ${cwd}`
    }, outputKind);
    return;
  }
  require$$6.debugDir('inspect', {
    pkgEnvDetails
  });

  // Lazily access constants.
  const {
    NPM,
    PNPM
  } = constants;
  const {
    agent,
    agentVersion
  } = pkgEnvDetails;
  if (agent !== NPM && agent !== PNPM) {
    await outputFixResult({
      ok: false,
      message: 'Not supported.',
      cause: `${agent} v${agentVersion} is not supported by this command.`
    }, outputKind);
    return;
  }
  logger.logger.info(`Fixing packages for ${agent} v${agentVersion}.\n`);
  const fixer = agent === NPM ? npmFix : pnpmFix;
  await outputFixResult(await fixer(pkgEnvDetails, {
    autoMerge,
    cwd,
    ghsas,
    limit,
    minSatisfying,
    orgSlug,
    prCheck,
    purls,
    rangeStyle,
    spinner,
    test,
    testScript,
    unknownFlags
  }), outputKind);
}

const {
  DRY_RUN_NOT_SAVING
} = constants;
const DEFAULT_LIMIT = 10;
const config$H = {
  commandName: 'fix',
  description: 'Update dependencies with "fixable" Socket alerts',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    autoMerge: {
      type: 'boolean',
      default: false,
      description: `Enable auto-merge for pull requests that Socket opens.\nSee ${vendor.terminalLinkExports('GitHub documentation', 'https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges/managing-auto-merge-for-pull-requests-in-your-repository')} for managing auto-merge for pull requests in your repository.`
    },
    autopilot: {
      type: 'boolean',
      default: false,
      description: `Shorthand for --autoMerge --test`
    },
    ghsa: {
      type: 'string',
      default: [],
      description: `Provide a list of ${vendor.terminalLinkExports('GHSA IDs', 'https://docs.github.com/en/code-security/security-advisories/working-with-global-security-advisories-from-the-github-advisory-database/about-the-github-advisory-database#about-ghsa-ids')} to compute fixes for, as either a comma separated value or as multiple flags.\nUse '--ghsa all' to lookup all GHSA IDs and compute fixes for them.`,
      isMultiple: true,
      hidden: true
    },
    limit: {
      type: 'number',
      default: DEFAULT_LIMIT,
      description: `The number of fixes to attempt at a time (default ${DEFAULT_LIMIT})`
    },
    maxSatisfying: {
      type: 'boolean',
      default: true,
      description: 'Use the maximum satisfying version for dependency updates',
      hidden: true
    },
    minSatisfying: {
      type: 'boolean',
      default: false,
      description: 'Constrain dependency updates to the minimum satisfying version'
    },
    prCheck: {
      type: 'boolean',
      default: true,
      description: 'Check for an existing PR before attempting a fix',
      hidden: true
    },
    purl: {
      type: 'string',
      default: [],
      description: `Provide a list of ${vendor.terminalLinkExports('PURLs', 'https://github.com/package-url/purl-spec?tab=readme-ov-file#purl')} to compute fixes for, as either a comma separated value or as\nmultiple flags, instead of querying the Socket API`,
      isMultiple: true,
      shortFlag: 'p'
    },
    rangeStyle: {
      type: 'string',
      default: 'preserve',
      description: `
Define how dependency version ranges are updated in package.json (default 'preserve').
Available styles:
  * caret - Use ^ range for compatible updates (e.g. ^1.2.3)
  * gt - Use > to allow any newer version (e.g. >1.2.3)
  * gte - Use >= to allow any newer version (e.g. >=1.2.3)
  * lt - Use < to allow only lower versions (e.g. <1.2.3)
  * lte - Use <= to allow only lower versions (e.g. <=1.2.3)
  * pin - Use the exact version (e.g. 1.2.3)
  * preserve - Retain the existing version range style as-is
  * tilde - Use ~ range for patch/minor updates (e.g. ~1.2.3)
      `.trim()
    },
    test: {
      type: 'boolean',
      default: false,
      description: 'Verify the fix by running unit tests'
    },
    testScript: {
      type: 'string',
      default: 'test',
      description: "The test script to run for fix attempts (default 'test')"
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} ./proj/tree --autoMerge
  `
};
const cmdFix = {
  description: config$H.description,
  hidden: config$H.hidden,
  run: run$H
};
async function run$H(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    allowUnknownFlags: false,
    argv,
    config: config$H,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  let rangeStyle = cli.flags['rangeStyle'];
  if (!rangeStyle) {
    rangeStyle = 'preserve';
  }
  const rawPurls = utils.cmdFlagValueToArray(cli.flags['purl']);
  const purls = [];
  for (const purl of rawPurls) {
    let version;
    try {
      version = vendor.packageurlJsExports$1.PackageURL.fromString(purl)?.version;
    } catch {}
    if (version) {
      purls.push(purl);
    } else {
      logger.logger.warn(`--purl ${purl} is missing a version and will be ignored.`);
    }
  }
  if (rawPurls.length !== purls.length && !purls.length) {
    process.exitCode = 1;
    logger.logger.fail('No valid --purl values provided.');
    return;
  }
  const outputKind = utils.getOutputKind(cli.flags['json'], cli.flags['markdown']);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: utils.RangeStyles.includes(rangeStyle),
    message: `Expecting range style of ${arrays.joinOr(utils.RangeStyles)}`,
    fail: 'invalid'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_NOT_SAVING);
    return;
  }
  const orgSlugCResult = await utils.getDefaultOrgSlug();
  if (!orgSlugCResult.ok) {
    process.exitCode = orgSlugCResult.code ?? 1;
    logger.logger.fail('Unable to resolve a Socket account organization.\nEnsure a Socket API token is specified for the organization using the SOCKET_CLI_API_TOKEN environment variable.');
    return;
  }
  const orgSlug = orgSlugCResult.data;
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  let autoMerge = Boolean(cli.flags['autoMerge']);
  let test = Boolean(cli.flags['test']);
  if (cli.flags['autopilot']) {
    autoMerge = true;
    test = true;
  }

  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  // We patched in this feature with `npx custompatch meow` at
  // socket-cli/patches/meow#13.2.0.patch.
  const unknownFlags = cli.unknownFlags ?? [];
  const ghsas = utils.cmdFlagValueToArray(cli.flags['ghsa']);
  const limit = Number(cli.flags['limit']) || DEFAULT_LIMIT;
  const maxSatisfying = Boolean(cli.flags['maxSatisfying']);
  const minSatisfying = Boolean(cli.flags['minSatisfying']) || !maxSatisfying;
  const prCheck = Boolean(cli.flags['prCheck']);
  const testScript = String(cli.flags['testScript'] || 'test');
  await handleFix({
    autoMerge,
    cwd,
    ghsas,
    limit,
    minSatisfying,
    prCheck,
    orgSlug,
    outputKind,
    purls,
    rangeStyle,
    spinner,
    test,
    testScript,
    unknownFlags
  });
}

async function outputInstallCompletion(result) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('');
  logger.logger.log(`Installation of tab completion for "${result.data.targetName}" finished!`);
  logger.logger.log('');
  result.data.actions.forEach(action => {
    logger.logger.log(`  - ${action}`);
  });
  logger.logger.log('');
  logger.logger.log('Socket tab completion works automatically in new terminals.');
  logger.logger.log('');
  logger.logger.log('Due to a bash limitation, tab completion cannot be enabled in the');
  logger.logger.log('current shell (bash instance) through NodeJS. You must either:');
  logger.logger.log('');
  logger.logger.log('1. Reload your .bashrc script (best):');
  logger.logger.log('');
  logger.logger.log(`   source ~/.bashrc`);
  logger.logger.log('');
  logger.logger.log('2. Run these commands to load the completion script:');
  logger.logger.log('');
  logger.logger.log(`   source ${result.data.targetPath}`);
  logger.logger.log(`   ${result.data.completionCommand}`);
  logger.logger.log('');
  logger.logger.log('3. Or restart bash somehow (restart terminal or run `bash`)');
  logger.logger.log('');
}

async function setupTabCompletion(targetName) {
  const result = utils.getBashrcDetails(targetName);
  if (!result.ok) {
    return result;
  }
  const {
    completionCommand,
    sourcingCommand,
    targetPath,
    toAddToBashrc
  } = result.data;

  // Target dir is something like ~/.local/share/socket/settings/completion (linux)
  const targetDir = path.dirname(targetPath);
  require$$6.debugFn('notice', 'target: path + dir', targetPath, targetDir);
  if (!fs$1.existsSync(targetDir)) {
    require$$6.debugFn('notice', 'create: target dir');
    fs$1.mkdirSync(targetDir, {
      recursive: true
    });
  }
  updateInstalledTabCompletionScript(targetPath);
  let bashrcUpdated = false;

  // Add to ~/.bashrc if not already there
  // Lazily access constants.homePath
  const bashrcPath = constants.homePath ? path.join(constants.homePath, '.bashrc') : '';
  const foundBashrc = Boolean(bashrcPath && fs$1.existsSync(bashrcPath));
  if (foundBashrc) {
    const content = fs$1.readFileSync(bashrcPath, 'utf8');
    if (!content.includes(sourcingCommand)) {
      fs$1.appendFileSync(bashrcPath, toAddToBashrc);
      bashrcUpdated = true;
    }
  }
  return {
    ok: true,
    data: {
      actions: [`Installed the tab completion script in ${targetPath}`, bashrcUpdated ? 'Added tab completion loader to ~/.bashrc' : foundBashrc ? 'Tab completion already found in ~/.bashrc' : 'No ~/.bashrc found so tab completion was not completely installed'],
      bashrcPath,
      bashrcUpdated,
      completionCommand,
      foundBashrc,
      sourcingCommand,
      targetName,
      targetPath
    }
  };
}
function getTabCompletionScriptRaw() {
  const sourceDir = path.dirname(require$$0.fileURLToPath(require('node:url').pathToFileURL(__filename).href));
  const sourcePath = path.join(sourceDir, 'socket-completion.bash');
  if (!fs$1.existsSync(sourcePath)) {
    return {
      ok: false,
      message: 'Source not found.',
      cause: `Unable to find the source tab completion bash script that Socket should ship. Expected to find it in \`${sourcePath}\` but it was not there.`
    };
  }
  return {
    ok: true,
    data: fs$1.readFileSync(sourcePath, 'utf8')
  };
}
function updateInstalledTabCompletionScript(targetPath) {
  const content = getTabCompletionScriptRaw();
  if (!content.ok) {
    return content;
  }

  // When installing set the current package.json version.
  // Later, we can call _socket_completion_version to get the installed version.
  fs$1.writeFileSync(targetPath, content.data.replaceAll('%SOCKET_VERSION_TOKEN%',
  // Lazily access constants.ENV.INLINED_SOCKET_CLI_VERSION_HASH.
  constants.ENV.INLINED_SOCKET_CLI_VERSION_HASH), 'utf8');
  return {
    ok: true,
    data: undefined
  };
}

async function handleInstallCompletion(targetName) {
  const result = await setupTabCompletion(targetName);
  await outputInstallCompletion(result);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$E
} = constants;
const config$G = {
  commandName: 'completion',
  description: 'Install bash completion for Socket CLI',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [NAME=socket]

    Installs bash completion for the Socket CLI. This will:
    1. Source the completion script in your current shell
    2. Add the source command to your ~/.bashrc if it's not already there

    This command will only setup tab completion, nothing else.

    Afterwards you should be able to type \`socket \` and then press tab to
    have bash auto-complete/suggest the sub/command or flags.

    Currently only supports bash.

    The optional name argument allows you to enable tab completion on a command
    name other than "socket". Mostly for debugging but also useful if you use a
    different alias for socket on your system.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples

      $ ${command}
      $ ${command} sd
      $ ${command} ./sd
  `
};
const cmdInstallCompletion = {
  description: config$G.description,
  hidden: config$G.hidden,
  run: run$G
};
async function run$G(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$G,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$E);
    return;
  }
  const targetName = cli.input[0] || 'socket';
  await handleInstallCompletion(String(targetName));
}

const description$6 = 'Install Socket CLI tab completion';
const cmdInstall = {
  description: description$6,
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      completion: cmdInstallCompletion
    }, {
      argv,
      description: description$6,
      importMeta,
      name: `${parentName} install`
    });
  }
};

async function outputCmdJson(cwd) {
  logger.logger.info('Target cwd:', constants.ENV.VITEST ? '<redacted>' : utils.tildify(cwd));
  const sockJsonPath = path.join(cwd, 'socket.json');
  const tildeSockJsonPath = constants.ENV.VITEST ? '<redacted>' : utils.tildify(sockJsonPath);
  if (!fs$1.existsSync(sockJsonPath)) {
    logger.logger.fail(`Not found: ${tildeSockJsonPath}`);
    process.exitCode = 1;
    return;
  }
  if (!fs$2.safeStatsSync(sockJsonPath)?.isFile()) {
    logger.logger.fail(`This is not a regular file (maybe a directory?): ${tildeSockJsonPath}`);
    process.exitCode = 1;
    return;
  }
  logger.logger.success(`This is the contents of ${tildeSockJsonPath}:`);
  logger.logger.error('');
  const data = fs$2.safeReadFileSync(sockJsonPath);
  logger.logger.log(data);
}

async function handleCmdJson(cwd) {
  await outputCmdJson(cwd);
}

const config$F = {
  commandName: 'json',
  description: 'Display the `socket.json` that would be applied for target folder',
  hidden: true,
  flags: {
    ...flags.commonFlags
  },
  help: command => `
    Usage
      $ ${command} [options] [CWD=.]

    Display the \`socket.json\` file that would apply when running relevant commands
    in the target directory.

    Examples
      $ ${command}
  `
};
const cmdJson = {
  description: config$F.description,
  hidden: config$F.hidden,
  run: run$F
};
async function run$F(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$F,
    importMeta,
    parentName
  });
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  await handleCmdJson(cwd);
}

function applyLogin(apiToken, enforcedOrgs, apiBaseUrl, apiProxy) {
  utils.updateConfigValue('enforcedOrgs', enforcedOrgs);
  utils.updateConfigValue('apiToken', apiToken);
  utils.updateConfigValue('apiBaseUrl', apiBaseUrl);
  utils.updateConfigValue('apiProxy', apiProxy);
}

const {
  SOCKET_PUBLIC_API_TOKEN
} = constants;
async function attemptLogin(apiBaseUrl, apiProxy) {
  apiBaseUrl ??= utils.getConfigValueOrUndef('apiBaseUrl') ?? undefined;
  apiProxy ??= utils.getConfigValueOrUndef('apiProxy') ?? undefined;
  const apiTokenInput = await prompts.password({
    message: `Enter your ${vendor.terminalLinkExports('Socket.dev API token', 'https://docs.socket.dev/docs/api-keys')} (leave blank to use a limited public token)`
  });
  if (apiTokenInput === undefined) {
    logger.logger.fail('Canceled by user');
    return {
      ok: false,
      message: 'Canceled',
      cause: 'Canceled by user'
    };
  }
  const apiToken = apiTokenInput || SOCKET_PUBLIC_API_TOKEN;
  const sockSdkCResult = await utils.setupSdk({
    apiBaseUrl,
    apiProxy,
    apiToken
  });
  if (!sockSdkCResult.ok) {
    process.exitCode = 1;
    logger.logger.fail(utils.failMsgWithBadge(sockSdkCResult.message, sockSdkCResult.cause));
    return;
  }
  const sockSdk = sockSdkCResult.data;
  const orgsCResult = await utils.handleApiCall(sockSdk.getOrganizations(), {
    desc: 'token verification'
  });
  if (!orgsCResult.ok) {
    process.exitCode = 1;
    logger.logger.fail(utils.failMsgWithBadge(orgsCResult.message, orgsCResult.cause));
    return;
  }
  const {
    organizations
  } = orgsCResult.data;
  const orgSlugs = Object.values(organizations).map(obj => obj.slug);
  logger.logger.success(`API token verified: ${orgSlugs}`);
  const enforcedChoices = Object.values(organizations).filter(org => org?.plan === 'enterprise').map(org => ({
    name: org.name ?? 'undefined',
    value: org.id
  }));
  let enforcedOrgs = [];
  if (enforcedChoices.length > 1) {
    const id = await prompts.select({
      message: "Which organization's policies should Socket enforce system-wide?",
      choices: enforcedChoices.concat({
        name: 'None',
        value: '',
        description: 'Pick "None" if this is a personal device'
      })
    });
    if (id === undefined) {
      logger.logger.fail('Canceled by user');
      return {
        ok: false,
        message: 'Canceled',
        cause: 'Canceled by user'
      };
    }
    if (id) {
      enforcedOrgs = [id];
    }
  } else if (enforcedChoices.length) {
    const shouldEnforce = await prompts.confirm({
      message: `Should Socket enforce ${enforcedChoices[0]?.name}'s security policies system-wide?`,
      default: true
    });
    if (shouldEnforce === undefined) {
      logger.logger.fail('Canceled by user');
      return {
        ok: false,
        message: 'Canceled',
        cause: 'Canceled by user'
      };
    }
    if (shouldEnforce) {
      const existing = enforcedChoices[0];
      if (existing) {
        enforcedOrgs = [existing.value];
      }
    }
  }
  const wantToComplete = await prompts.select({
    message: 'Would you like to install bash tab completion?',
    choices: [{
      name: 'Yes',
      value: true,
      description: 'Sets up tab completion for "socket" in your bash env. If you\'re unsure, this is probably what you want.'
    }, {
      name: 'No',
      value: false,
      description: 'Will skip tab completion setup. Does not change how Socket works.'
    }]
  });
  if (wantToComplete === undefined) {
    logger.logger.fail('Canceled by user');
    return {
      ok: false,
      message: 'Canceled',
      cause: 'Canceled by user'
    };
  }
  if (wantToComplete) {
    logger.logger.log('');
    logger.logger.log('Setting up tab completion...');
    const setupCResult = await setupTabCompletion('socket');
    if (setupCResult.ok) {
      logger.logger.success('Tab completion will be enabled after restarting your terminal');
    } else {
      logger.logger.fail('Failed to install tab completion script. Try `socket install completion` later.');
    }
  }
  utils.updateConfigValue('defaultOrg', orgSlugs[0]);
  const previousPersistedToken = utils.getConfigValueOrUndef('apiToken');
  try {
    applyLogin(apiToken, enforcedOrgs, apiBaseUrl, apiProxy);
    logger.logger.success(`API credentials ${previousPersistedToken === apiToken ? 'refreshed' : previousPersistedToken ? 'updated' : 'set'}`);
    if (utils.isReadOnlyConfig()) {
      logger.logger.log('');
      logger.logger.warn('Note: config is in read-only mode, at least one key was overridden through flag/env, so the login was not persisted!');
    }
  } catch {
    process.exitCode = 1;
    logger.logger.fail(`API login failed`);
  }
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$D
} = constants;
const config$E = {
  commandName: 'login',
  description: 'Setup Socket CLI with an API token and defaults',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    apiBaseUrl: {
      type: 'string',
      description: 'API server to connect to for login'
    },
    apiProxy: {
      type: 'string',
      description: 'Proxy to use when making connection to API server'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options]

    API Token Requirements
      - Quota: 1 unit

    Logs into the Socket API by prompting for an API token

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} --api-proxy=http://localhost:1234
  `
};
const cmdLogin = {
  description: config$E.description,
  hidden: config$E.hidden,
  run: run$E
};
async function run$E(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$E,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$D);
    return;
  }
  if (!vendor.isInteractiveExports()) {
    throw new utils.InputError('Cannot prompt for credentials in a non-interactive shell');
  }
  const apiBaseUrl = cli.flags['apiBaseUrl'];
  const apiProxy = cli.flags['apiProxy'];
  await attemptLogin(apiBaseUrl, apiProxy);
}

function applyLogout() {
  utils.updateConfigValue('apiToken', null);
  utils.updateConfigValue('apiBaseUrl', null);
  utils.updateConfigValue('apiProxy', null);
  utils.updateConfigValue('enforcedOrgs', null);
}

function attemptLogout() {
  try {
    applyLogout();
    logger.logger.success('Successfully logged out');
    if (utils.isReadOnlyConfig()) {
      logger.logger.log('');
      logger.logger.warn('Note: config is in read-only mode, at least one key was overridden through flag/env, so the logout was not persisted!');
    }
  } catch {
    logger.logger.fail('Failed to complete logout steps');
  }
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$C
} = constants;
const config$D = {
  commandName: 'logout',
  description: 'Socket API logout',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    Logs out of the Socket API and clears all Socket credentials from disk

    Examples
      $ ${command}
  `
};
const cmdLogout = {
  description: config$D.description,
  hidden: config$D.hidden,
  run: run$D
};
async function run$D(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$D,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$C);
    return;
  }
  attemptLogout();
}

const {
  NPM: NPM$5,
  NPX,
  PACKAGE_LOCK_JSON,
  PNPM: PNPM$5,
  YARN,
  YARN_LOCK
} = constants;
const nodejsPlatformTypes = new Set(['javascript', 'js', 'nodejs', NPM$5, PNPM$5, 'ts', 'tsx', 'typescript']);
function argvToArray(argv) {
  if (argv['help']) {
    return ['--help'];
  }
  const result = [];
  for (const {
    0: key,
    1: value
  } of Object.entries(argv)) {
    if (key === '_' || key === '--') {
      continue;
    }
    if (key === 'babel' || key === 'install-deps' || key === 'validate') {
      // cdxgen documents no-babel, no-install-deps, and no-validate flags so
      // use them when relevant.
      result.push(`--${value ? key : `no-${key}`}`);
    } else if (value === true) {
      result.push(`--${key}`);
    } else if (typeof value === 'string') {
      result.push(`--${key}`, String(value));
    } else if (Array.isArray(value)) {
      result.push(`--${key}`, ...value.map(String));
    }
  }
  const pathArgs = argv['_'];
  if (Array.isArray(pathArgs)) {
    result.push(...pathArgs);
  }
  const argsAfterDoubleHyphen = argv['--'];
  if (Array.isArray(argsAfterDoubleHyphen)) {
    result.push('--', ...argsAfterDoubleHyphen);
  }
  return result;
}
async function runCdxgen(yargvWithYes) {
  let cleanupPackageLock = false;
  const {
    yes,
    ...yargv
  } = {
    __proto__: null,
    ...yargvWithYes
  };
  const yesArgs = yes ? ['--yes'] : [];
  if (yargv.type !== YARN && nodejsPlatformTypes.has(yargv.type) && fs$1.existsSync(`./${YARN_LOCK}`)) {
    if (fs$1.existsSync(`./${PACKAGE_LOCK_JSON}`)) {
      yargv.type = NPM$5;
    } else {
      // Use synp to create a package-lock.json from the yarn.lock,
      // based on the node_modules folder, for a more accurate SBOM.
      try {
        await shadowNpmBin(NPX, [...yesArgs,
        // Lazily access constants.ENV.INLINED_SOCKET_CLI_SYNP_VERSION.
        `synp@${constants.ENV.INLINED_SOCKET_CLI_SYNP_VERSION}`, '--source-file', `./${YARN_LOCK}`]);
        yargv.type = NPM$5;
        cleanupPackageLock = true;
      } catch {}
    }
  }
  await shadowNpmBin(NPX, [...yesArgs,
  // Lazily access constants.ENV.INLINED_SOCKET_CLI_CYCLONEDX_CDXGEN_VERSION.
  `@cyclonedx/cdxgen@${constants.ENV.INLINED_SOCKET_CLI_CYCLONEDX_CDXGEN_VERSION}`, ...argvToArray(yargv)]);
  if (cleanupPackageLock) {
    try {
      await fs$1.promises.rm(`./${PACKAGE_LOCK_JSON}`);
    } catch {}
  }
  const fullOutputPath = path.join(process.cwd(), yargv.output);
  if (fs$1.existsSync(fullOutputPath)) {
    logger.logger.log(vendor.yoctocolorsCjsExports.cyanBright(`${yargv.output} created!`));
  }
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$B
} = constants;

// TODO: Convert yargs to meow.
const toLower = arg => arg.toLowerCase();
const arrayToLower = arg => arg.map(toLower);

// npx @cyclonedx/cdxgen@11.2.7 --help
//
// Options:
//   -o, --output                 Output file. Default bom.json                                       [default: "bom.json"]
//   -t, --type                   Project type. Please refer to https://cyclonedx.github.io/cdxgen/#/PROJECT_TYPES for supp
//                                orted languages/platforms.                                                        [array]
//       --exclude-type           Project types to exclude. Please refer to https://cyclonedx.github.io/cdxgen/#/PROJECT_TY
//                                PES for supported languages/platforms.
//   -r, --recurse                Recurse mode suitable for mono-repos. Defaults to true. Pass --no-recurse to disable.
//                                                                                                [boolean] [default: true]
//   -p, --print                  Print the SBOM as a table with tree.                                            [boolean]
//   -c, --resolve-class          Resolve class names for packages. jars only for now.                            [boolean]
//       --deep                   Perform deep searches for components. Useful while scanning C/C++ apps, live OS and oci i
//                                mages.                                                                          [boolean]
//       --server-url             Dependency track url. Eg: https://deptrack.cyclonedx.io
//       --skip-dt-tls-check      Skip TLS certificate check when calling Dependency-Track.      [boolean] [default: false]
//       --api-key                Dependency track api key
//       --project-group          Dependency track project group
//       --project-name           Dependency track project name. Default use the directory name
//       --project-version        Dependency track project version                                   [string] [default: ""]
//       --project-id             Dependency track project id. Either provide the id or the project name and version togeth
//                                er                                                                               [string]
//       --parent-project-id      Dependency track parent project id                                               [string]
//       --required-only          Include only the packages with required scope on the SBOM. Would set compositions.aggrega
//                                te to incomplete unless --no-auto-compositions is passed.                       [boolean]
//       --fail-on-error          Fail if any dependency extractor fails.                                         [boolean]
//       --no-babel               Do not use babel to perform usage analysis for JavaScript/TypeScript projects.  [boolean]
//       --generate-key-and-sign  Generate an RSA public/private key pair and then sign the generated SBOM using JSON Web S
//                                ignatures.                                                                      [boolean]
//       --server                 Run cdxgen as a server                                                          [boolean]
//       --server-host            Listen address                                                     [default: "127.0.0.1"]
//       --server-port            Listen port                                                             [default: "9090"]
//       --install-deps           Install dependencies automatically for some projects. Defaults to true but disabled for c
//                                ontainers and oci scans. Use --no-install-deps to disable this feature.
//                                                                                                [boolean] [default: true]
//       --validate               Validate the generated SBOM using json schema. Defaults to true. Pass --no-validate to di
//                                sable.                                                          [boolean] [default: true]
//       --evidence               Generate SBOM with evidence for supported languages.           [boolean] [default: false]
//       --spec-version           CycloneDX Specification version to use. Defaults to 1.6
//                                                                         [number] [choices: 1.4, 1.5, 1.6, 1.7] [default: 1.6]
//       --filter                 Filter components containing this word in purl or component.properties.value. Multiple va
//                                lues allowed.                                                                     [array]
//       --only                   Include components only containing this word in purl. Useful to generate BOM with first p
//                                arty components alone. Multiple values allowed.                                   [array]
//       --author                 The person(s) who created the BOM. Set this value if you're intending the modify the BOM
//                                and claim authorship.                               [array] [default: "OWASP Foundation"]
//       --profile                BOM profile to use for generation. Default generic.
//   [choices: "appsec", "research", "operational", "threat-modeling", "license-compliance", "generic", "machine-learning",
//                                                        "ml", "deep-learning", "ml-deep", "ml-tiny"] [default: "generic"]
//       --exclude                Additional glob pattern(s) to ignore                                              [array]
//       --export-proto           Serialize and export BOM as protobuf binary.  [boolean] [default: false]
//       --proto-bin-file         Path for the serialized protobuf binary.  [default: "bom.cdx"]
//       --include-formulation    Generate formulation section with git metadata and build tools. Defaults to false.
//                                                                                               [boolean] [default: false]
//       --include-crypto         Include crypto libraries as components.                        [boolean] [default: false]
//       --standard               The list of standards which may consist of regulations, industry or organizational-specif
//                                ic standards, maturity models, best practices, or any other requirements which can be eva
//                                luated against or attested to.
//   [array] [choices: "asvs-5.0", "asvs-4.0.3", "bsimm-v13", "masvs-2.0.0", "nist_ssdf-1.1", "pcissc-secure-slc-1.1", "scv
//                                                                                          s-1.0.0", "ssaf-DRAFT-2023-11"]
//       --json-pretty            Pretty-print the generated BOM json.                           [boolean] [default: false]
//       --min-confidence         Minimum confidence needed for the identity of a component from 0 - 1, where 1 is 100% con
//                                fidence.                                                            [number] [default: 0]
//       --technique              Analysis technique to use
//   [array] [choices: "auto", "source-code-analysis", "binary-analysis", "manifest-analysis", "hash-comparison", "instrume
//                                                                                                    ntation", "filename"]
//       --auto-compositions      Automatically set compositions when the BOM was filtered. Defaults to true
//                                                                                                [boolean] [default: true]
//   -h, --help                   Show help                                                                       [boolean]
//   -v, --version                Show version number                                                             [boolean]

// isSecureMode defined at:
// https://github.com/CycloneDX/cdxgen/blob/v11.2.7/lib/helpers/utils.js#L66
// const isSecureMode =
//   ['true', '1'].includes(process.env?.CDXGEN_SECURE_MODE) ||
//   process.env?.NODE_OPTIONS?.includes('--permission')

// Yargs CDXGEN configuration defined at:
// https://github.com/CycloneDX/cdxgen/blob/v11.2.7/bin/cdxgen.js#L64
const yargsConfig = {
  configuration: {
    'camel-case-expansion': false,
    'greedy-arrays': false,
    'parse-numbers': false,
    'populate--': true,
    'short-option-groups': false,
    'strip-aliased': true,
    'unknown-options-as-args': true
  },
  coerce: {
    'exclude-type': arrayToLower,
    'feature-flags': arrayToLower,
    filter: arrayToLower,
    only: arrayToLower,
    profile: toLower,
    standard: arrayToLower,
    technique: arrayToLower,
    type: arrayToLower
  },
  default: {
    //author: ['OWASP Foundation'],
    //'auto-compositions': true,
    //babel: true,
    //banner: false, // hidden
    //'deps-slices-file': 'deps.slices.json', // hidden
    //evidence: false,
    //'exclude-type': [],
    //'export-proto': false,
    //'fail-on-error': isSecureMode,
    //'feature-flags': [], // hidden
    //'include-crypto': false,
    //'include-formulation': false,
    //'install-deps': !isSecureMode
    //lifecycle: 'build', // hidden
    //'min-confidence': '0',
    //output: 'bom.json',
    //profile: 'generic',
    //'project-version': '',
    //'proto-bin-file': 'bom.cdx',
    //recurse: true,
    //'skip-dt-tls-check': false,
    //'semantics-slices-file': 'semantics.slices.json',
    //'server-host': '127.0.0.1',
    //'server-port': '9090',
    //'spec-version': '1.6',
    type: ['js']
    //validate: true,
  },
  alias: {
    help: ['h'],
    output: ['o'],
    print: ['p'],
    recurse: ['r'],
    'resolve-class': ['c'],
    type: ['t'],
    version: ['v'],
    yes: ['y']
  },
  array: [{
    key: 'author',
    type: 'string'
  }, {
    key: 'exclude',
    type: 'string'
  }, {
    key: 'exclude-type',
    type: 'string'
  }, {
    key: 'feature-flags',
    type: 'string'
  },
  // hidden
  {
    key: 'filter',
    type: 'string'
  }, {
    key: 'only',
    type: 'string'
  }, {
    key: 'standard',
    type: 'string'
  }, {
    key: 'technique',
    type: 'string'
  }, {
    key: 'type',
    type: 'string'
  }],
  boolean: ['auto-compositions', 'babel', 'banner',
  // hidden
  'deep', 'evidence', 'export-proto', 'fail-on-error', 'generate-key-and-sign', 'help', 'include-crypto', 'include-formulation', 'install-deps', 'json-pretty', 'print', 'recurse', 'required-only', 'resolve-class', 'skip-dt-tls-check', 'server', 'validate', 'version',
  // The --yes flag and -y alias map to the corresponding flag and alias of npx.
  // https://docs.npmjs.com/cli/v7/commands/npx#compatibility-with-older-npx-versions
  'yes'],
  string: ['api-key', 'data-flow-slices-file',
  // hidden
  'deps-slices-file',
  // hidden
  'evinse-output',
  // hidden
  'lifecycle', 'min-confidence',
  // number
  'openapi-spec-file',
  // hidden
  'output', 'parent-project-id', 'profile', 'project-group', 'project-name', 'project-version', 'project-id', 'proto-bin-file', 'reachables-slices-file',
  // hidden
  'semantics-slices-file',
  // hidden
  'server-host', 'server-port', 'server-url', 'spec-version',
  // number
  'usages-slices-file' // hidden
  ]
};
const config$C = {
  commandName: 'cdxgen',
  description: 'Create an SBOM with CycloneDX generator (cdxgen)',
  hidden: false,
  // Stub out flags and help.
  // TODO: Convert yargs to meow.
  flags: {},
  help: () => ''
};
const cmdManifestCdxgen = {
  description: config$C.description,
  hidden: config$C.hidden,
  run: run$C
};
async function run$C(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    // Don't let meow take over --help.
    argv: argv.filter(a => !utils.isHelpFlag(a)),
    config: config$C,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];

  // TODO: Convert yargs to meow.
  const yargv = {
    ...vendor.yargsParser(argv, yargsConfig)
  };
  const pathArgs = [];
  const unknowns = [];
  for (const a of yargv._) {
    if (path$1.isPath(a)) {
      pathArgs.push(a);
    } else {
      unknowns.push(a);
    }
  }
  yargv._ = pathArgs;
  const {
    length: unknownsCount
  } = unknowns;
  if (unknownsCount) {
    // Use exit status of 2 to indicate incorrect usage, generally invalid
    // options or missing arguments.
    // https://www.gnu.org/software/bash/manual/html_node/Exit-Status.html
    process.exitCode = 2;
    logger.logger.fail(`Unknown ${words.pluralize('argument', unknownsCount)}: ${unknowns.join(', ')}`);
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$B);
    return;
  }

  // Change defaults when not passing the --help flag.
  if (!yargv.help) {
    // Make 'lifecycle' default to 'pre-build', which also sets 'install-deps' to `false`,
    // to avoid arbitrary code execution on the cdxgen scan.
    // https://github.com/CycloneDX/cdxgen/issues/1328
    if (yargv.lifecycle === undefined) {
      yargv.lifecycle = 'pre-build';
      yargv['install-deps'] = false;
      logger.logger.info(`Setting cdxgen --lifecycle to "${yargv.lifecycle}" to avoid arbitrary code execution on this scan.\n  Pass "--lifecycle build" to generate a BOM consisting of information obtained during the build process.\n  See cdxgen ${vendor.terminalLinkExports('BOM lifecycles documentation', 'https://cyclonedx.github.io/cdxgen/#/ADVANCED?id=bom-lifecycles')} for more details.\n`);
    }
    if (yargv.output === undefined) {
      yargv.output = 'socket-cdx.json';
    }
  }
  await runCdxgen(yargv);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$A
} = constants;
const config$B = {
  commandName: 'auto',
  description: 'Auto-detect build and attempt to generate manifest file',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    verbose: {
      type: 'boolean',
      default: false,
      description: 'Enable debug output (only for auto itself; sub-steps need to have it pre-configured), may help when running into errors'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Tries to figure out what language your target repo uses. If it finds a
    supported case then it will try to generate the manifest file for that
    language with the default or detected settings.

    Note: you can exclude languages from being auto-generated if you don't want
          them to. Run \`socket manifest setup\` in the same dir to disable it.

    Examples

      $ ${command}
      $ ${command} ./project/foo
  `
};
const cmdManifestAuto = {
  description: config$B.description,
  hidden: config$B.hidden,
  run: run$B
};
async function run$B(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$B,
    importMeta,
    parentName
  });
  // TODO: Implement json/md further.
  const {
    json,
    markdown,
    verbose: verboseFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const verbose = !!verboseFlag;
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  const outputKind = utils.getOutputKind(json, markdown);
  if (verbose) {
    logger.logger.group('- ', parentName, config$B.commandName, ':');
    logger.logger.group('- flags:', cli.flags);
    logger.logger.groupEnd();
    logger.logger.log('- input:', cli.input);
    logger.logger.log('- cwd:', cwd);
    logger.logger.groupEnd();
  }
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  const detected = await detectManifestActions(sockJson, cwd);
  require$$6.debugDir('inspect', {
    detected
  });
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$A);
    return;
  }
  if (!detected.count) {
    logger.logger.fail('Was unable to discover any targets for which we can generate manifest files...');
    logger.logger.log('');
    logger.logger.log('- Make sure this script would work with your target build (see `socket manifest --help` for your target).');
    logger.logger.log('- Make sure to run it from the correct dir (use --cwd to target another dir)');
    logger.logger.log('- Make sure the necessary build tools are available (`PATH`)');
    process.exitCode = 1;
    return;
  }
  await generateAutoManifest({
    detected,
    cwd,
    outputKind,
    verbose
  });
  logger.logger.success(`Finished. Should have attempted to generate manifest files for ${detected.count} targets.`);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$z
} = constants;
const config$A = {
  commandName: 'conda',
  description: '[beta] Convert a Conda environment.yml file to a python requirements.txt',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    file: {
      type: 'string',
      description: 'Input file name (by default for Conda this is "environment.yml"), relative to cwd'
    },
    stdin: {
      type: 'boolean',
      description: 'Read the input from stdin (supersedes --file)'
    },
    out: {
      type: 'string',
      description: 'Output path (relative to cwd)'
    },
    stdout: {
      type: 'boolean',
      description: 'Print resulting requirements.txt to stdout (supersedes --out)'
    },
    verbose: {
      type: 'boolean',
      description: 'Print debug messages'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Warning: While we don't support Conda necessarily, this tool extracts the pip
             block from an environment.yml and outputs it as a requirements.txt
             which you can scan as if it were a pypi package.

    USE AT YOUR OWN RISK

    Note: FILE can be a dash (-) to indicate stdin. This way you can pipe the
          contents of a file to have it processed.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples

      $ ${command}
      $ ${command} ./project/foo --file environment.yaml
  `
};
const cmdManifestConda = {
  description: config$A.description,
  hidden: config$A.hidden,
  run: run$A
};
async function run$A(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$A,
    importMeta,
    parentName
  });
  const {
    json = false,
    markdown = false
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  let {
    file: filename,
    out,
    stdin,
    stdout,
    verbose
  } = cli.flags;

  // Set defaults for any flag/arg that is not given. Check socket.json first.
  if (stdin === undefined && sockJson.defaults?.manifest?.conda?.stdin !== undefined) {
    stdin = sockJson.defaults?.manifest?.conda?.stdin;
    logger.logger.info('Using default --stdin from socket.json:', stdin);
  }
  if (stdin) {
    filename = '-';
  } else if (!filename) {
    if (sockJson.defaults?.manifest?.conda?.infile) {
      filename = sockJson.defaults?.manifest?.conda?.infile;
      logger.logger.info('Using default --file from socket.json:', filename);
    } else {
      filename = 'environment.yml';
    }
  }
  if (stdout === undefined && sockJson.defaults?.manifest?.conda?.stdout !== undefined) {
    stdout = sockJson.defaults?.manifest?.conda?.stdout;
    logger.logger.info('Using default --stdout from socket.json:', stdout);
  }
  if (stdout) {
    out = '-';
  } else if (!out) {
    if (sockJson.defaults?.manifest?.conda?.outfile) {
      out = sockJson.defaults?.manifest?.conda?.outfile;
      logger.logger.info('Using default --out from socket.json:', out);
    } else {
      out = 'requirements.txt';
    }
  }
  if (verbose === undefined && sockJson.defaults?.manifest?.conda?.verbose !== undefined) {
    verbose = sockJson.defaults?.manifest?.conda?.verbose;
    logger.logger.info('Using default --verbose from socket.json:', verbose);
  } else if (verbose === undefined) {
    verbose = false;
  }
  if (verbose) {
    logger.logger.group('- ', parentName, config$A.commandName, ':');
    logger.logger.group('- flags:', cli.flags);
    logger.logger.groupEnd();
    logger.logger.log('- target:', cwd);
    logger.logger.log('- output:', out);
    logger.logger.groupEnd();
  }
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: cli.input.length <= 1,
    message: 'Can only accept one DIR (make sure to escape spaces!)',
    fail: `received ${cli.input.length}`
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  });
  if (!wasValidInput) {
    return;
  }
  logger.logger.warn('Warning: This will approximate your Conda dependencies using PyPI. We do not yet officially support Conda. Use at your own risk.');
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$z);
    return;
  }
  await handleManifestConda({
    cwd,
    filename: String(filename),
    out: String(out || ''),
    outputKind,
    verbose: Boolean(verbose)
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$y
} = constants;
const config$z = {
  commandName: 'gradle',
  description: '[beta] Use Gradle to generate a manifest file (`pom.xml`) for a Gradle/Java/Kotlin/etc project',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    bin: {
      type: 'string',
      description: 'Location of gradlew binary to use, default: CWD/gradlew'
    },
    gradleOpts: {
      type: 'string',
      description: 'Additional options to pass on to ./gradlew, see `./gradlew --help`'
    },
    verbose: {
      type: 'boolean',
      description: 'Print debug messages'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Uses gradle, preferably through your local project \`gradlew\`, to generate a
    \`pom.xml\` file for each task. If you have no \`gradlew\` you can try the
    global \`gradle\` binary but that may not work (hard to predict).

    The \`pom.xml\` is a manifest file similar to \`package.json\` for npm or
    or requirements.txt for PyPi), but specifically for Maven, which is Java's
    dependency repository. Languages like Kotlin and Scala piggy back on it too.

    There are some caveats with the gradle to \`pom.xml\` conversion:

    - each task will generate its own xml file and by default it generates one xml
      for every task. (This may be a good thing!)

    - it's possible certain features don't translate well into the xml. If you
      think something is missing that could be supported please reach out.

    - it works with your \`gradlew\` from your repo and local settings and config

    Support is beta. Please report issues or give us feedback on what's missing.

    Examples

      $ ${command} .
      $ ${command} --bin=../gradlew .
  `
};
const cmdManifestGradle = {
  description: config$z.description,
  hidden: config$z.hidden,
  run: run$z
};
async function run$z(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$z,
    importMeta,
    parentName
  });
  const {
    json = false,
    markdown = false
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];

  // TODO: Implement json/md further.
  const outputKind = utils.getOutputKind(json, markdown);
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  require$$6.debugFn('inspect', 'override: socket.json gradle', sockJson?.defaults?.manifest?.gradle);
  let {
    bin,
    gradleOpts,
    verbose
  } = cli.flags;

  // Set defaults for any flag/arg that is not given. Check socket.json first.
  if (!bin) {
    if (sockJson.defaults?.manifest?.gradle?.bin) {
      bin = sockJson.defaults?.manifest?.gradle?.bin;
      logger.logger.info('Using default --bin from socket.json:', bin);
    } else {
      bin = path.join(cwd, 'gradlew');
    }
  }
  if (!gradleOpts) {
    if (sockJson.defaults?.manifest?.gradle?.gradleOpts) {
      gradleOpts = sockJson.defaults?.manifest?.gradle?.gradleOpts;
      logger.logger.info('Using default --gradleOpts from socket.json:', gradleOpts);
    } else {
      gradleOpts = '';
    }
  }
  if (verbose === undefined) {
    if (sockJson.defaults?.manifest?.gradle?.verbose !== undefined) {
      verbose = sockJson.defaults?.manifest?.gradle?.verbose;
      logger.logger.info('Using default --verbose from socket.json:', verbose);
    } else {
      verbose = false;
    }
  }
  if (verbose) {
    logger.logger.group('- ', parentName, config$z.commandName, ':');
    logger.logger.group('- flags:', cli.flags);
    logger.logger.groupEnd();
    logger.logger.log('- input:', cli.input);
    logger.logger.groupEnd();
  }

  // TODO: We're not sure it's feasible to parse source file from stdin. We could
  //       try, store contents in a file in some folder, target that folder... what
  //       would the file name be?

  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: cli.input.length <= 1,
    message: 'Can only accept one DIR (make sure to escape spaces!)',
    fail: 'received ' + cli.input.length
  });
  if (!wasValidInput) {
    return;
  }
  if (verbose) {
    logger.logger.group();
    logger.logger.info('- cwd:', cwd);
    logger.logger.info('- gradle bin:', bin);
    logger.logger.groupEnd();
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$y);
    return;
  }
  await convertGradleToMaven({
    bin: String(bin),
    cwd,
    gradleOpts: String(gradleOpts || '').split(' ').map(s => s.trim()).filter(Boolean),
    verbose: Boolean(verbose)
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$x
} = constants;

// TODO: We may want to dedupe some pieces for all gradle languages. I think it
//       makes sense to have separate commands for them and I think it makes
//       sense for the help panels to note the requested language, rather than
//       `socket manifest kotlin` to print help screens with `gradle` as the
//       command. Room for improvement.
const config$y = {
  commandName: 'kotlin',
  description: '[beta] Use Gradle to generate a manifest file (`pom.xml`) for a Kotlin project',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    bin: {
      type: 'string',
      description: 'Location of gradlew binary to use, default: CWD/gradlew'
    },
    gradleOpts: {
      type: 'string',
      description: 'Additional options to pass on to ./gradlew, see `./gradlew --help`'
    },
    verbose: {
      type: 'boolean',
      description: 'Print debug messages'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Uses gradle, preferably through your local project \`gradlew\`, to generate a
    \`pom.xml\` file for each task. If you have no \`gradlew\` you can try the
    global \`gradle\` binary but that may not work (hard to predict).

    The \`pom.xml\` is a manifest file similar to \`package.json\` for npm or
    or requirements.txt for PyPi), but specifically for Maven, which is Java's
    dependency repository. Languages like Kotlin and Scala piggy back on it too.

    There are some caveats with the gradle to \`pom.xml\` conversion:

    - each task will generate its own xml file and by default it generates one xml
      for every task. (This may be a good thing!)

    - it's possible certain features don't translate well into the xml. If you
      think something is missing that could be supported please reach out.

    - it works with your \`gradlew\` from your repo and local settings and config

    Support is beta. Please report issues or give us feedback on what's missing.

    Examples

      $ ${command} .
      $ ${command} --bin=../gradlew .
  `
};
const cmdManifestKotlin = {
  description: config$y.description,
  hidden: config$y.hidden,
  run: run$y
};
async function run$y(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$y,
    importMeta,
    parentName
  });
  const {
    json = false,
    markdown = false
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];

  // TODO: Implement json/md further.
  const outputKind = utils.getOutputKind(json, markdown);
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  require$$6.debugFn('inspect', 'override: socket.json gradle', sockJson?.defaults?.manifest?.gradle);
  let {
    bin,
    gradleOpts,
    verbose
  } = cli.flags;

  // Set defaults for any flag/arg that is not given. Check socket.json first.
  if (!bin) {
    if (sockJson.defaults?.manifest?.gradle?.bin) {
      bin = sockJson.defaults?.manifest?.gradle?.bin;
      logger.logger.info('Using default --bin from socket.json:', bin);
    } else {
      bin = path.join(cwd, 'gradlew');
    }
  }
  if (!gradleOpts) {
    if (sockJson.defaults?.manifest?.gradle?.gradleOpts) {
      gradleOpts = sockJson.defaults?.manifest?.gradle?.gradleOpts;
      logger.logger.info('Using default --gradleOpts from socket.json:', gradleOpts);
    } else {
      gradleOpts = '';
    }
  }
  if (verbose === undefined) {
    if (sockJson.defaults?.manifest?.gradle?.verbose !== undefined) {
      verbose = sockJson.defaults?.manifest?.gradle?.verbose;
      logger.logger.info('Using default --verbose from socket.json:', verbose);
    } else {
      verbose = false;
    }
  }
  if (verbose) {
    logger.logger.group('- ', parentName, config$y.commandName, ':');
    logger.logger.group('- flags:', cli.flags);
    logger.logger.groupEnd();
    logger.logger.log('- input:', cli.input);
    logger.logger.groupEnd();
  }

  // TODO: We're not sure it's feasible to parse source file from stdin. We could
  //       try, store contents in a file in some folder, target that folder... what
  //       would the file name be?

  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: cli.input.length <= 1,
    message: 'Can only accept one DIR (make sure to escape spaces!)',
    fail: 'received ' + cli.input.length
  });
  if (!wasValidInput) {
    return;
  }
  if (verbose) {
    logger.logger.group();
    logger.logger.info('- cwd:', cwd);
    logger.logger.info('- gradle bin:', bin);
    logger.logger.groupEnd();
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$x);
    return;
  }
  await convertGradleToMaven({
    bin: String(bin),
    cwd,
    gradleOpts: String(gradleOpts || '').split(' ').map(s => s.trim()).filter(Boolean),
    verbose: Boolean(verbose)
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$w
} = constants;
const config$x = {
  commandName: 'scala',
  description: "[beta] Generate a manifest file (`pom.xml`) from Scala's `build.sbt` file",
  hidden: false,
  flags: {
    ...flags.commonFlags,
    bin: {
      type: 'string',
      description: 'Location of sbt binary to use'
    },
    out: {
      type: 'string',
      description: 'Path of output file; where to store the resulting manifest, see also --stdout'
    },
    stdout: {
      type: 'boolean',
      description: 'Print resulting pom.xml to stdout (supersedes --out)'
    },
    sbtOpts: {
      type: 'string',
      description: 'Additional options to pass on to sbt, as per `sbt --help`'
    },
    verbose: {
      type: 'boolean',
      description: 'Print debug messages'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Uses \`sbt makePom\` to generate a \`pom.xml\` from your \`build.sbt\` file.
    This xml file is the dependency manifest (like a package.json
    for Node.js or requirements.txt for PyPi), but specifically for Scala.

    There are some caveats with \`build.sbt\` to \`pom.xml\` conversion:

    - the xml is exported as socket.pom.xml as to not confuse existing build tools
      but it will first hit your /target/sbt<version> folder (as a different name)

    - the pom.xml format (standard by Scala) does not support certain sbt features
      - \`excludeAll()\`, \`dependencyOverrides\`, \`force()\`, \`relativePath\`
      - For details: https://www.scala-sbt.org/1.x/docs/Library-Management.html

    - it uses your sbt settings and local configuration verbatim

    - it can only export one target per run, so if you have multiple targets like
      development and production, you must run them separately.

    You can specify --bin to override the path to the \`sbt\` binary to invoke.

    Support is beta. Please report issues or give us feedback on what's missing.

    This is only for SBT. If your Scala setup uses gradle, please see the help
    sections for \`socket manifest gradle\` or \`socket cdxgen\`.

    Examples

      $ ${command}
      $ ${command} ./proj --bin=/usr/bin/sbt --file=boot.sbt
  `
};
const cmdManifestScala = {
  description: config$x.description,
  hidden: config$x.hidden,
  run: run$x
};
async function run$x(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$x,
    importMeta,
    parentName
  });
  const {
    json = false,
    markdown = false
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);

  // TODO: Implement json/md further.
  const outputKind = utils.getOutputKind(json, markdown);
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  require$$6.debugFn('inspect', 'override: socket.json sbt', sockJson?.defaults?.manifest?.sbt);
  let {
    bin,
    out,
    sbtOpts,
    stdout,
    verbose
  } = cli.flags;

  // Set defaults for any flag/arg that is not given. Check socket.json first.
  if (!bin) {
    if (sockJson.defaults?.manifest?.sbt?.bin) {
      bin = sockJson.defaults?.manifest?.sbt?.bin;
      logger.logger.info('Using default --bin from socket.json:', bin);
    } else {
      bin = 'sbt';
    }
  }
  if (stdout === undefined && sockJson.defaults?.manifest?.sbt?.stdout !== undefined) {
    stdout = sockJson.defaults?.manifest?.sbt?.stdout;
    logger.logger.info('Using default --stdout from socket.json:', stdout);
  }
  if (stdout) {
    out = '-';
  } else if (!out) {
    if (sockJson.defaults?.manifest?.sbt?.outfile) {
      out = sockJson.defaults?.manifest?.sbt?.outfile;
      logger.logger.info('Using default --out from socket.json:', out);
    } else {
      out = './socket.pom.xml';
    }
  }
  if (!sbtOpts) {
    if (sockJson.defaults?.manifest?.sbt?.sbtOpts) {
      sbtOpts = sockJson.defaults?.manifest?.sbt?.sbtOpts;
      logger.logger.info('Using default --sbtOpts from socket.json:', sbtOpts);
    } else {
      sbtOpts = '';
    }
  }
  if (verbose === undefined && sockJson.defaults?.manifest?.sbt?.verbose !== undefined) {
    verbose = sockJson.defaults?.manifest?.sbt?.verbose;
    logger.logger.info('Using default --verbose from socket.json:', verbose);
  } else if (verbose === undefined) {
    verbose = false;
  }
  if (verbose) {
    logger.logger.group('- ', parentName, config$x.commandName, ':');
    logger.logger.group('- flags:', cli.flags);
    logger.logger.groupEnd();
    logger.logger.log('- input:', cli.input);
    logger.logger.groupEnd();
  }

  // TODO: We're not sure it's feasible to parse source file from stdin. We could
  //       try, store contents in a file in some folder, target that folder... what
  //       would the file name be?

  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: cli.input.length <= 1,
    message: 'Can only accept one DIR (make sure to escape spaces!)',
    fail: 'received ' + cli.input.length
  });
  if (!wasValidInput) {
    return;
  }
  if (verbose) {
    logger.logger.group();
    logger.logger.log('- target:', cwd);
    logger.logger.log('- sbt bin:', bin);
    logger.logger.log('- out:', out);
    logger.logger.groupEnd();
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$w);
    return;
  }
  await convertSbtToMaven({
    bin: String(bin),
    cwd: cwd,
    out: String(out),
    sbtOpts: String(sbtOpts).split(' ').map(s => s.trim()).filter(Boolean),
    verbose: Boolean(verbose)
  });
}

async function outputManifestSetup(result) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.success('Setup complete');
}

async function setupManifestConfig(cwd, defaultOnReadError = false) {
  const detected = await detectManifestActions(null, cwd);
  require$$6.debugDir('inspect', {
    detected
  });

  // - repeat
  //   - give the user an option to configure one of the supported targets
  //   - run through an interactive prompt for selected target
  //   - each target will have its own specific options
  //   - record them to the socket.yml (or socket-cli.yml ? or just socket.json ?)

  const jsonPath = path.join(cwd, `socket.json`);
  if (fs$1.existsSync(jsonPath)) {
    logger.logger.info(`Found socket.json at ${jsonPath}`);
  } else {
    logger.logger.info(`No socket.json found at ${cwd}, will generate a new one`);
  }
  logger.logger.log('');
  logger.logger.log('Note: This tool will set up flag and argument defaults for certain');
  logger.logger.log('      CLI commands. You can still override them by explicitly');
  logger.logger.log('      setting the flag. It is meant to be a convenience tool.');
  logger.logger.log('');
  logger.logger.log('This command will generate a socket.json file in the target cwd.');
  logger.logger.log('You can choose to add this file to your repo (handy for collaboration)');
  logger.logger.log('or to add it to the ignored files, or neither. This file is only');
  logger.logger.log('used in CLI workflows.');
  logger.logger.log('');
  const choices = [{
    name: 'Conda'.padEnd(30, ' '),
    value: 'conda',
    description: 'Generate requirements.txt from a Conda environment.yml'
  }, {
    name: 'Gradle'.padEnd(30, ' '),
    value: 'gradle',
    description: 'Generate pom.xml files through gradle'
  }, {
    name: 'Kotlin (gradle)'.padEnd(30, ' '),
    value: 'gradle',
    description: 'Generate pom.xml files (for Kotlin) through gradle'
  }, {
    name: 'Scala (gradle)'.padEnd(30, ' '),
    value: 'gradle',
    description: 'Generate pom.xml files (for Scala) through gradle'
  }, {
    name: 'Scala (sbt)'.padEnd(30, ' '),
    value: 'sbt',
    description: 'Generate pom.xml files through sbt'
  }];
  choices.forEach(obj => {
    if (detected[obj.value]) {
      obj.name += ' [detected]';
    }
  });

  // Surface detected language first, then by alphabet
  choices.sort((a, b) => {
    if (detected[a.value] && !detected[b.value]) {
      return -1;
    }
    if (!detected[a.value] && detected[b.value]) {
      return 1;
    }
    return a.value < b.value ? -1 : a.value > b.value ? 1 : 0;
  });

  // Make exit the last entry...
  choices.push({
    name: 'None, exit configurator',
    value: '',
    description: 'Exit setup'
  });

  // TODO: Use detected to list those first.
  const targetEco = await prompts.select({
    message: 'Select ecosystem manifest generator to configure',
    choices
  });
  const sockJsonCResult = utils.readSocketJsonSync(cwd, defaultOnReadError);
  if (!sockJsonCResult.ok) {
    return sockJsonCResult;
  }
  const sockJson = sockJsonCResult.data;
  if (!sockJson.defaults) {
    sockJson.defaults = {};
  }
  if (!sockJson.defaults.manifest) {
    sockJson.defaults.manifest = {};
  }
  let result;
  switch (targetEco) {
    case 'conda':
      {
        if (!sockJson.defaults.manifest.conda) {
          sockJson.defaults.manifest.conda = {};
        }
        result = await setupConda(sockJson.defaults.manifest.conda);
        break;
      }
    case 'gradle':
      {
        if (!sockJson.defaults.manifest.gradle) {
          sockJson.defaults.manifest.gradle = {};
        }
        result = await setupGradle(sockJson.defaults.manifest.gradle);
        break;
      }
    case 'sbt':
      {
        if (!sockJson.defaults.manifest.sbt) {
          sockJson.defaults.manifest.sbt = {};
        }
        result = await setupSbt(sockJson.defaults.manifest.sbt);
        break;
      }
    default:
      {
        result = canceledByUser$1();
      }
  }
  if (!result.ok || result.data.canceled) {
    return result;
  }
  logger.logger.log('');
  logger.logger.log('Setup complete. Writing socket.json');
  logger.logger.log('');
  if (await prompts.select({
    message: `Do you want to write the new config to ${jsonPath} ?`,
    choices: [{
      name: 'yes',
      value: true,
      description: 'Update config'
    }, {
      name: 'no',
      value: false,
      description: 'Do not update the config'
    }]
  })) {
    return await utils.writeSocketJson(cwd, sockJson);
  }
  return canceledByUser$1();
}
async function setupConda(config) {
  const on = await askForEnabled(!config.disabled);
  if (on === undefined) {
    return canceledByUser$1();
  } else if (on) {
    delete config.disabled;
  } else {
    config.disabled = true;
  }
  const infile = await askForInputFile(config.infile || 'environment.yml');
  if (infile === undefined) {
    return canceledByUser$1();
  } else if (infile === '-') {
    config.stdin = true;
  } else {
    delete config.stdin;
    if (infile) {
      config.infile = infile;
    } else {
      delete config.infile;
    }
  }
  const stdout = await askForStdout(config.stdout);
  if (stdout === undefined) {
    return canceledByUser$1();
  } else if (stdout === 'yes') {
    config.stdout = true;
  } else if (stdout === 'no') {
    config.stdout = false;
  } else {
    delete config.stdout;
  }
  if (!config.stdout) {
    const out = await askForOutputFile(config.outfile || 'requirements.txt');
    if (out === undefined) {
      return canceledByUser$1();
    } else if (out === '-') {
      config.stdout = true;
    } else {
      delete config.stdout;
      if (out) {
        config.outfile = out;
      } else {
        delete config.outfile;
      }
    }
  }
  const verbose = await askForVerboseFlag(config.verbose);
  if (verbose === undefined) {
    return canceledByUser$1();
  } else if (verbose === 'yes' || verbose === 'no') {
    config.verbose = verbose === 'yes';
  } else {
    delete config.verbose;
  }
  return notCanceled$1();
}
async function setupGradle(config) {
  const bin = await askForBin(config.bin || './gradlew');
  if (bin === undefined) {
    return canceledByUser$1();
  } else if (bin) {
    config.bin = bin;
  } else {
    delete config.bin;
  }
  const opts = await prompts.input({
    message: '(--gradleOpts) Enter gradle options to pass through',
    default: config.gradleOpts || '',
    required: false
    // validate: async string => bool
  });
  if (opts === undefined) {
    return canceledByUser$1();
  } else if (opts) {
    config.gradleOpts = opts;
  } else {
    delete config.gradleOpts;
  }
  const verbose = await askForVerboseFlag(config.verbose);
  if (verbose === undefined) {
    return canceledByUser$1();
  } else if (verbose === 'yes' || verbose === 'no') {
    config.verbose = verbose === 'yes';
  } else {
    delete config.verbose;
  }
  return notCanceled$1();
}
async function setupSbt(config) {
  const bin = await askForBin(config.bin || 'sbt');
  if (bin === undefined) {
    return canceledByUser$1();
  } else if (bin) {
    config.bin = bin;
  } else {
    delete config.bin;
  }
  const opts = await prompts.input({
    message: '(--sbtOpts) Enter sbt options to pass through',
    default: config.sbtOpts || '',
    required: false
    // validate: async string => bool
  });
  if (opts === undefined) {
    return canceledByUser$1();
  } else if (opts) {
    config.sbtOpts = opts;
  } else {
    delete config.sbtOpts;
  }
  const stdout = await askForStdout(config.stdout);
  if (stdout === undefined) {
    return canceledByUser$1();
  } else if (stdout === 'yes') {
    config.stdout = true;
  } else if (stdout === 'no') {
    config.stdout = false;
  } else {
    delete config.stdout;
  }
  if (config.stdout !== true) {
    const out = await askForOutputFile(config.outfile || 'sbt.pom.xml');
    if (out === undefined) {
      return canceledByUser$1();
    } else if (out === '-') {
      config.stdout = true;
    } else {
      delete config.stdout;
      if (out) {
        config.outfile = out;
      } else {
        delete config.outfile;
      }
    }
  }
  const verbose = await askForVerboseFlag(config.verbose);
  if (verbose === undefined) {
    return canceledByUser$1();
  } else if (verbose === 'yes' || verbose === 'no') {
    config.verbose = verbose === 'yes';
  } else {
    delete config.verbose;
  }
  return notCanceled$1();
}
async function askForStdout(defaultValue) {
  return await prompts.select({
    message: '(--stdout) Print the resulting pom.xml to stdout?',
    choices: [{
      name: 'no',
      value: 'no',
      description: 'Write output to a file, not stdout'
    }, {
      name: 'yes',
      value: 'yes',
      description: 'Print in stdout (this will supersede --out)'
    }, {
      name: '(leave default)',
      value: '',
      description: 'Do not store a setting for this'
    }],
    default: defaultValue === true ? 'yes' : defaultValue === false ? 'no' : ''
  });
}
async function askForEnabled(defaultValue) {
  return await prompts.select({
    message: 'Do you want to enable or disable auto generating manifest files for this language in this dir?',
    choices: [{
      name: 'Enable',
      value: true,
      description: 'Generate manifest files for this language when detected'
    }, {
      name: 'Disable',
      value: false,
      description: 'Do not generate manifest files for this language when detected, unless explicitly asking for it'
    }, {
      name: 'Cancel',
      value: undefined,
      description: 'Exit configurator'
    }],
    default: defaultValue === true ? 'enable' : defaultValue === false ? 'disable' : ''
  });
}
async function askForInputFile(defaultName = '') {
  return await prompts.input({
    message: '(--file) What should be the default file name to read? Should be an absolute path or relative to the cwd. Use `-` to read from stdin instead.' + (defaultName ? ' (Backspace to leave default)' : ''),
    default: defaultName,
    required: false
    // validate: async string => bool
  });
}
async function askForOutputFile(defaultName = '') {
  return await prompts.input({
    message: '(--out) What should be the default output file? Should be absolute path or relative to cwd.' + (defaultName ? ' (Backspace to leave default)' : ''),
    default: defaultName,
    required: false
    // validate: async string => bool
  });
}
async function askForBin(defaultName = '') {
  return await prompts.input({
    message: '(--bin) What should be the command to execute? Usually your build binary.' + (defaultName ? ' (Backspace to leave default)' : ''),
    default: defaultName,
    required: false
    // validate: async string => bool
  });
}
async function askForVerboseFlag(current) {
  return await prompts.select({
    message: '(--verbose) Should this run in verbose mode by default?',
    choices: [{
      name: 'no',
      value: 'no',
      description: 'Do not run this manifest in verbose mode'
    }, {
      name: 'yes',
      value: 'yes',
      description: 'Run this manifest in verbose mode'
    }, {
      name: '(leave default)',
      value: '',
      description: 'Do not store a setting for this'
    }],
    default: current === true ? 'yes' : current === false ? 'no' : ''
  });
}
function canceledByUser$1() {
  logger.logger.log('');
  logger.logger.info('User canceled');
  logger.logger.log('');
  return {
    ok: true,
    data: {
      canceled: true
    }
  };
}
function notCanceled$1() {
  return {
    ok: true,
    data: {
      canceled: false
    }
  };
}

async function handleManifestSetup(cwd, defaultOnReadError) {
  const result = await setupManifestConfig(cwd, defaultOnReadError);
  await outputManifestSetup(result);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$v
} = constants;
const config$w = {
  commandName: 'setup',
  description: 'Start interactive configurator to customize default flag values for `socket manifest` in this dir',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    defaultOnReadError: {
      type: 'boolean',
      description: 'If reading the socket.json fails, just use a default config? Warning: This might override the existing json file!'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    This command will try to detect all supported ecosystems in given CWD. Then
    it starts a configurator where you can setup default values for certain flags
    when creating manifest files in that dir. These configuration details are
    then stored in a local \`socket.json\` file (which you may or may not commit
    to the repo). Next time you run \`socket manifest ...\` it will load this
    json file and any flags which are not explicitly set in the command but which
    have been registered in the json file will get the default value set to that
    value you stored rather than the hardcoded defaults.

    This helps with for example when your build binary is in a particular path
    or when your build tool needs specific opts and you don't want to specify
    them when running the command every time.

    You can also disable manifest generation for certain ecosystems.

    This generated configuration file will only be used locally by the CLI. You
    can commit it to the repo (useful for collaboration) or choose to add it to
    your .gitignore all the same. Only this CLI will use it.

    Examples
      $ ${command}
      $ ${command} ./proj
  `
};
const cmdManifestSetup = {
  description: config$w.description,
  hidden: config$w.hidden,
  run: run$w
};
async function run$w(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$w,
    importMeta,
    parentName
  });
  const {
    defaultOnReadError = false
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$v);
    return;
  }
  await handleManifestSetup(cwd, Boolean(defaultOnReadError));
}

const config$v = {
  commandName: 'manifest',
  description: 'Generate a dependency manifest for certain ecosystems',
  hidden: false,
  flags: {
    ...flags.commonFlags
  }};
const cmdManifest = {
  description: config$v.description,
  hidden: config$v.hidden,
  run: run$v
};
async function run$v(argv, importMeta, {
  parentName
}) {
  await utils.meowWithSubcommands({
    auto: cmdManifestAuto,
    cdxgen: cmdManifestCdxgen,
    conda: cmdManifestConda,
    gradle: cmdManifestGradle,
    kotlin: cmdManifestKotlin,
    scala: cmdManifestScala,
    setup: cmdManifestSetup
  }, {
    argv,
    aliases: {
      yolo: {
        description: config$v.description,
        hidden: true,
        argv: ['auto']
      }
    },
    description: config$v.description,
    importMeta,
    flags: config$v.flags,
    name: `${parentName} ${config$v.commandName}`
  });
}

const require$3 = require$$5.createRequire(require('node:url').pathToFileURL(__filename).href);
const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$u
} = constants;
const config$u = {
  commandName: 'npm',
  description: 'Run npm with the Socket wrapper',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: command => `
    Usage
      $ ${command} ...

    This runs npm but checks packages through Socket before installing anything.
    See docs for more details.

    Note: Everything after "npm" is sent straight to the npm command.
          Only the \`--dryRun\` and \`--help\` flags are caught here.

    Use \`socket wrapper on\` to automatically enable this such that you don't
    have to write \`socket npm\` for that purpose.

    Examples
      $ ${command}
      $ ${command} install -g socket
  `
};
const cmdNpm = {
  description: config$u.description,
  hidden: config$u.hidden,
  run: run$u
};
async function run$u(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$u,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$u);
    return;
  }

  // Lazily access constants.shadowNpmBinPath.
  const shadowBin = /*@__PURE__*/require$3(constants.shadowNpmBinPath);
  await shadowBin('npm', argv);
}

const require$2 = require$$5.createRequire(require('node:url').pathToFileURL(__filename).href);
const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$t
} = constants;
const config$t = {
  commandName: 'npx',
  description: 'Run npx with the Socket wrapper',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: (command, _config) => `
    Usage
      $ ${command} ...

    This runs npx but checks packages through Socket before running them.
    See docs for more details.

    Note: Everything after "npx" is sent straight to the npx command.
          Only the \`--dryRun\` and \`--help\` flags are caught here.

    Use \`socket wrapper on\` to automatically enable this such that you don't
    have to write \`socket npx\` for that purpose.

    Examples
      $ ${command}
      $ ${command} prettier
  `
};
const cmdNpx = {
  description: config$t.description,
  hidden: config$t.hidden,
  run: run$t
};
async function run$t(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$t,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$t);
    return;
  }

  // Lazily access constants.shadowNpmBinPath.
  const shadowBin = /*@__PURE__*/require$2(constants.shadowNpmBinPath);
  await shadowBin('npx', argv);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$s
} = constants;
const config$s = {
  commandName: 'oops',
  description: 'Trigger an intentional error (for development)',
  hidden: true,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    throw: {
      type: 'boolean',
      default: false,
      description: 'Throw an explicit error even if --json or --markdown are set'
    }
  },
  help: (parentName, config) => `
    Usage
      $ ${parentName} ${config.commandName}

    Don't run me.
  `
};
const cmdOops = {
  description: config$s.description,
  hidden: config$s.hidden,
  run: run$s
};
async function run$s(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$s,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    throw: justThrow
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$s);
    return;
  }
  if (json && !justThrow) {
    process.exitCode = 1;
    logger.logger.log(utils.serializeResultJson({
      ok: false,
      message: 'Oops',
      cause: 'This error was intentionally left blank'
    }));
  }
  if (markdown && !justThrow) {
    process.exitCode = 1;
    logger.logger.fail(utils.failMsgWithBadge('Oops', 'This error was intentionally left blank'));
    return;
  }
  throw new Error('This error was intentionally left blank');
}

const {
  BUN: BUN$3,
  NPM: NPM$4,
  PNPM: PNPM$4,
  VLT: VLT$4,
  YARN_BERRY: YARN_BERRY$3,
  YARN_CLASSIC: YARN_CLASSIC$3
} = constants;
function matchLsCmdViewHumanStdout(stdout, name) {
  return stdout.includes(` ${name}@`);
}
function matchQueryCmdStdout(stdout, name) {
  return stdout.includes(`"${name}"`);
}
function lsStdoutIncludes(pkgEnvDetails, stdout, name) {
  switch (pkgEnvDetails.agent) {
    case BUN$3:
    case YARN_BERRY$3:
    case YARN_CLASSIC$3:
      return matchLsCmdViewHumanStdout(stdout, name);
    case PNPM$4:
    case VLT$4:
    case NPM$4:
    default:
      return matchQueryCmdStdout(stdout, name);
  }
}

function getDependencyEntries(pkgEnvDetails) {
  const {
    dependencies,
    devDependencies,
    optionalDependencies,
    peerDependencies
  } = pkgEnvDetails.editablePkgJson.content;
  return [['dependencies', dependencies ? {
    __proto__: null,
    ...dependencies
  } : undefined], ['devDependencies', devDependencies ? {
    __proto__: null,
    ...devDependencies
  } : undefined], ['peerDependencies', peerDependencies ? {
    __proto__: null,
    ...peerDependencies
  } : undefined], ['optionalDependencies', optionalDependencies ? {
    __proto__: null,
    ...optionalDependencies
  } : undefined]].filter(({
    1: o
  }) => o);
}

const {
  BUN: BUN$2,
  LOCK_EXT,
  NPM: NPM$3,
  PNPM: PNPM$3,
  VLT: VLT$3,
  YARN_BERRY: YARN_BERRY$2,
  YARN_CLASSIC: YARN_CLASSIC$2
} = constants;
function npmLockSrcIncludes(lockSrc, name) {
  // Detects the package name in the following cases:
  //   "name":
  return lockSrc.includes(`"${name}":`);
}
function bunLockSrcIncludes(lockSrc, name, lockName) {
  // This is a bit counterintuitive. When lockName ends with a .lockb
  // we treat it as a yarn.lock. When lockName ends with a .lock we
  // treat it as a package-lock.json. The bun.lock format is not identical
  // package-lock.json, however it close enough for npmLockIncludes to work.
  const lockfileScanner = lockName?.endsWith(LOCK_EXT) ? npmLockSrcIncludes : yarnLockSrcIncludes;
  return lockfileScanner(lockSrc, name);
}
function pnpmLockSrcIncludes(lockSrc, name) {
  const escapedName = regexps.escapeRegExp(name);
  return new RegExp(
  // Detects the package name.
  // v9.0 and v6.0 lockfile patterns:
  //   'name'
  //   name:
  //   name@
  // v6.0 lockfile patterns:
  //   /name@
  `(?<=^\\s*)(?:'${escapedName}'|/?${escapedName}(?=[:@]))`, 'm').test(lockSrc);
}
function vltLockSrcIncludes(lockSrc, name) {
  // Detects the package name in the following cases:
  //   "name"
  return lockSrc.includes(`"${name}"`);
}
function yarnLockSrcIncludes(lockSrc, name) {
  const escapedName = regexps.escapeRegExp(name);
  return new RegExp(
  // Detects the package name in the following cases:
  //   "name@
  //   , "name@
  //   name@
  //   , name@
  `(?<=(?:^\\s*|,\\s*)"?)${escapedName}(?=@)`, 'm').test(lockSrc);
}
function lockSrcIncludes(pkgEnvDetails, lockSrc, name, lockName) {
  switch (pkgEnvDetails.agent) {
    case BUN$2:
      return bunLockSrcIncludes(lockSrc, name, lockName);
    case PNPM$3:
      return pnpmLockSrcIncludes(lockSrc, name);
    case VLT$3:
      return vltLockSrcIncludes(lockSrc, name);
    case YARN_BERRY$2:
      return yarnLockSrcIncludes(lockSrc, name);
    case YARN_CLASSIC$2:
      return yarnLockSrcIncludes(lockSrc, name);
    case NPM$3:
    default:
      return npmLockSrcIncludes(lockSrc, name);
  }
}

const {
  BUN: BUN$1,
  NPM: NPM$2,
  PNPM: PNPM$2,
  VLT: VLT$2,
  YARN_BERRY: YARN_BERRY$1,
  YARN_CLASSIC: YARN_CLASSIC$1
} = constants;
function cleanupQueryStdout(stdout) {
  if (stdout === '') {
    return '';
  }
  let pkgs;
  try {
    pkgs = JSON.parse(stdout);
  } catch {}
  if (!Array.isArray(pkgs) || !pkgs.length) {
    return '';
  }
  const names = new Set();
  for (const {
    _id,
    name,
    pkgid
  } of pkgs) {
    // `npm query` results may not have a "name" property, in which case we
    // fallback to "_id" and then "pkgid".
    // `vlt ls --view json` results always have a "name" property.
    const fallback = _id ?? pkgid ?? '';
    const resolvedName = name ?? fallback.slice(0, fallback.indexOf('@', 1));
    // Add package names, except for those under the `@types` scope as those
    // are known to only be dev dependencies.
    if (resolvedName && !resolvedName.startsWith('@types/')) {
      names.add(resolvedName);
    }
  }
  return JSON.stringify(Array.from(names), null, 2);
}
function parsableToQueryStdout(stdout) {
  if (stdout === '') {
    return '';
  }
  // Convert the parsable stdout into a json array of unique names.
  // The matchAll regexp looks for a forward (posix) or backward (win32) slash
  // and matches one or more non-slashes until the newline.
  const names = new Set(stdout.matchAll(/(?<=[/\\])[^/\\]+(?=\n)/g));
  return JSON.stringify(Array.from(names), null, 2);
}
async function npmQuery(npmExecPath, cwd) {
  let stdout = '';
  try {
    stdout = (await spawn.spawn(npmExecPath, ['query', ':not(.dev)'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return cleanupQueryStdout(stdout);
}
async function lsBun(pkgEnvDetails, options) {
  const {
    cwd = process.cwd()
  } = {
    __proto__: null,
    ...options
  };
  try {
    // Bun does not support filtering by production packages yet.
    // https://github.com/oven-sh/bun/issues/8283
    return (await spawn.spawn(pkgEnvDetails.agentExecPath, ['pm', 'ls', '--all'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return '';
}
async function lsNpm(pkgEnvDetails, options) {
  const {
    cwd = process.cwd()
  } = {
    __proto__: null,
    ...options
  };
  return await npmQuery(pkgEnvDetails.agentExecPath, cwd);
}
async function lsPnpm(pkgEnvDetails, options) {
  const {
    cwd = process.cwd(),
    npmExecPath
  } = {
    __proto__: null,
    ...options
  };
  if (npmExecPath && npmExecPath !== NPM$2) {
    const result = await npmQuery(npmExecPath, cwd);
    if (result) {
      return result;
    }
  }
  let stdout = '';
  try {
    stdout = (await spawn.spawn(pkgEnvDetails.agentExecPath,
    // Pnpm uses the alternative spelling of parsable.
    // https://en.wiktionary.org/wiki/parsable
    ['ls', '--parseable', '--prod', '--depth', 'Infinity'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return parsableToQueryStdout(stdout);
}
async function lsVlt(pkgEnvDetails, options) {
  const {
    cwd = process.cwd()
  } = {
    __proto__: null,
    ...options
  };
  let stdout = '';
  try {
    // See https://docs.vlt.sh/cli/commands/list#options.
    stdout = (await spawn.spawn(pkgEnvDetails.agentExecPath, ['ls', '--view', 'human', ':not(.dev)'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return cleanupQueryStdout(stdout);
}
async function lsYarnBerry(pkgEnvDetails, options) {
  const {
    cwd = process.cwd()
  } = {
    __proto__: null,
    ...options
  };
  try {
    // Yarn Berry does not support filtering by production packages yet.
    // https://github.com/yarnpkg/berry/issues/5117
    return (await spawn.spawn(pkgEnvDetails.agentExecPath, ['info', '--recursive', '--name-only'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return '';
}
async function lsYarnClassic(pkgEnvDetails, options) {
  const {
    cwd = process.cwd()
  } = {
    __proto__: null,
    ...options
  };
  try {
    // However, Yarn Classic does support it.
    // https://github.com/yarnpkg/yarn/releases/tag/v1.0.0
    // > Fix: Excludes dev dependencies from the yarn list output when the
    //   environment is production
    return (await spawn.spawn(pkgEnvDetails.agentExecPath, ['list', '--prod'], {
      cwd,
      // Lazily access constants.WIN32.
      shell: constants.WIN32
    })).stdout;
  } catch {}
  return '';
}
async function listPackages(pkgEnvDetails, options) {
  switch (pkgEnvDetails.agent) {
    case BUN$1:
      return await lsBun(pkgEnvDetails, options);
    case PNPM$2:
      return await lsPnpm(pkgEnvDetails, options);
    case VLT$2:
      return await lsVlt(pkgEnvDetails, options);
    case YARN_BERRY$1:
      return await lsYarnBerry(pkgEnvDetails, options);
    case YARN_CLASSIC$1:
      return await lsYarnClassic(pkgEnvDetails, options);
    case NPM$2:
    default:
      return await lsNpm(pkgEnvDetails, options);
  }
}

const CMD_NAME = 'socket optimize';

const {
  BUN,
  NPM: NPM$1,
  OVERRIDES,
  PNPM: PNPM$1,
  RESOLUTIONS,
  VLT: VLT$1,
  YARN_BERRY,
  YARN_CLASSIC
} = constants;
const depFields = ['dependencies', 'devDependencies', 'peerDependencies', 'peerDependenciesMeta', 'optionalDependencies', 'bundleDependencies'];
function getEntryIndexes(entries, keys) {
  return keys.map(n => entries.findIndex(p => p[0] === n)).filter(n => n !== -1).sort((a, b) => a - b);
}
function getLowestEntryIndex(entries, keys) {
  return getEntryIndexes(entries, keys)?.[0] ?? -1;
}
function getHighestEntryIndex(entries, keys) {
  return getEntryIndexes(entries, keys).at(-1) ?? -1;
}
function updatePkgJsonField(editablePkgJson, field, value) {
  const oldValue = editablePkgJson.content[field];
  if (oldValue) {
    // The field already exists so we simply update the field value.
    if (field === PNPM$1) {
      const isPnpmObj = require$$7.isObject(oldValue);
      if (require$$7.hasKeys(value)) {
        editablePkgJson.update({
          [field]: {
            ...(isPnpmObj ? oldValue : {}),
            overrides: {
              ...(isPnpmObj ? oldValue[OVERRIDES] : {}),
              ...value
            }
          }
        });
      } else {
        // Properties with undefined values are deleted when saved as JSON.
        editablePkgJson.update(require$$7.hasKeys(oldValue) ? {
          [field]: {
            ...(isPnpmObj ? oldValue : {}),
            overrides: undefined
          }
        } : {
          [field]: undefined
        });
      }
    } else if (field === OVERRIDES || field === RESOLUTIONS) {
      // Properties with undefined values are deleted when saved as JSON.
      editablePkgJson.update({
        [field]: require$$7.hasKeys(value) ? value : undefined
      });
    } else {
      editablePkgJson.update({
        [field]: value
      });
    }
    return;
  }
  if ((field === OVERRIDES || field === PNPM$1 || field === RESOLUTIONS) && !require$$7.hasKeys(value)) {
    return;
  }
  // Since the field doesn't exist we want to insert it into the package.json
  // in a place that makes sense, e.g. close to the "dependencies" field. If
  // we can't find a place to insert the field we'll add it to the bottom.
  const entries = Object.entries(editablePkgJson.content);
  let insertIndex = -1;
  let isPlacingHigher = false;
  if (field === OVERRIDES) {
    insertIndex = getLowestEntryIndex(entries, [RESOLUTIONS]);
    if (insertIndex === -1) {
      isPlacingHigher = true;
      insertIndex = getHighestEntryIndex(entries, [...depFields, PNPM$1]);
    }
  } else if (field === RESOLUTIONS) {
    isPlacingHigher = true;
    insertIndex = getHighestEntryIndex(entries, [...depFields, OVERRIDES, PNPM$1]);
  } else if (field === PNPM$1) {
    insertIndex = getLowestEntryIndex(entries, [OVERRIDES, RESOLUTIONS]);
    if (insertIndex === -1) {
      isPlacingHigher = true;
      insertIndex = getHighestEntryIndex(entries, depFields);
    }
  }
  if (insertIndex === -1) {
    insertIndex = getLowestEntryIndex(entries, ['engines', 'files']);
  }
  if (insertIndex === -1) {
    isPlacingHigher = true;
    insertIndex = getHighestEntryIndex(entries, ['exports', 'imports', 'main']);
  }
  if (insertIndex === -1) {
    insertIndex = entries.length;
  } else if (isPlacingHigher) {
    insertIndex += 1;
  }
  entries.splice(insertIndex, 0, [field, field === PNPM$1 ? {
    [OVERRIDES]: value
  } : value]);
  editablePkgJson.fromJSON(`${JSON.stringify(Object.fromEntries(entries), null, 2)}\n`);
}
function updateOverridesField(editablePkgJson, overrides) {
  updatePkgJsonField(editablePkgJson, OVERRIDES, overrides);
}
function updateResolutionsField(editablePkgJson, overrides) {
  updatePkgJsonField(editablePkgJson, RESOLUTIONS, overrides);
}
function updatePnpmField(editablePkgJson, overrides) {
  updatePkgJsonField(editablePkgJson, PNPM$1, overrides);
}
function updateManifest(agent, editablePkgJson, overrides) {
  switch (agent) {
    case BUN:
      updateResolutionsField(editablePkgJson, overrides);
      return;
    case PNPM$1:
      updatePnpmField(editablePkgJson, overrides);
      return;
    case VLT$1:
      updateOverridesField(editablePkgJson, overrides);
      return;
    case YARN_BERRY:
      updateResolutionsField(editablePkgJson, overrides);
      return;
    case YARN_CLASSIC:
      updateResolutionsField(editablePkgJson, overrides);
      return;
    case NPM$1:
    default:
      updateOverridesField(editablePkgJson, overrides);
      return;
  }
}

const {
  NPM,
  PNPM
} = constants;
const manifestNpmOverrides = registry.getManifestData(NPM);
async function addOverrides(pkgEnvDetails, pkgPath, options) {
  const {
    agent,
    lockName,
    lockSrc,
    npmExecPath,
    pkgPath: rootPath
  } = pkgEnvDetails;
  const {
    logger,
    pin,
    prod,
    spinner,
    state = {
      added: new Set(),
      addedInWorkspaces: new Set(),
      updated: new Set(),
      updatedInWorkspaces: new Set(),
      warnedPnpmWorkspaceRequiresNpm: false
    }
  } = {
    __proto__: null,
    ...options
  };
  const workspacePkgJsonPaths = await utils.globWorkspace(agent, pkgPath);
  const isWorkspace = workspacePkgJsonPaths.length > 0;
  const isWorkspaceRoot = pkgPath === rootPath;
  const isLockScanned = isWorkspaceRoot && !prod;
  const workspace = isWorkspaceRoot ? 'root' : path.relative(rootPath, pkgPath);
  if (isWorkspace && agent === PNPM &&
  // npmExecPath will === the agent name IF it CANNOT be resolved.
  npmExecPath === NPM && !state.warnedPnpmWorkspaceRequiresNpm) {
    state.warnedPnpmWorkspaceRequiresNpm = true;
    spinner?.stop();
    logger?.warn(utils.cmdPrefixMessage(CMD_NAME, `${agent} workspace support requires \`npm ls\`, falling back to \`${agent} list\``));
    spinner?.start();
  }
  const overridesDataObjects = [];
  if (isWorkspace || pkgEnvDetails.editablePkgJson.content['private']) {
    overridesDataObjects.push(getOverridesData(pkgEnvDetails));
  } else {
    overridesDataObjects.push(getOverridesDataNpm(pkgEnvDetails), getOverridesDataYarnClassic(pkgEnvDetails));
  }
  const depAliasMap = new Map();
  const depEntries = getDependencyEntries(pkgEnvDetails);
  const manifestEntries = manifestNpmOverrides.filter(({
    1: data
  }) => vendor.semverExports.satisfies(
  // Roughly check Node range as semver.coerce will strip leading
  // v's, carets (^), comparators (<,<=,>,>=,=), and tildes (~).
  vendor.semverExports.coerce(data.engines.node), pkgEnvDetails.pkgRequirements.node));
  const addingText = `Adding overrides to ${workspace}...`;
  let loggedAddingText = false;

  // Chunk package names to process them in parallel 3 at a time.
  await require$$8.pEach(manifestEntries, async ({
    1: data
  }) => {
    const {
      name: sockRegPkgName,
      package: origPkgName,
      version
    } = data;
    const major = utils.getMajor(version);
    const sockOverridePrefix = `${NPM}:${sockRegPkgName}@`;
    const sockOverrideSpec = `${sockOverridePrefix}${pin ? version : `^${major}`}`;
    for (const {
      1: depObj
    } of depEntries) {
      const sockSpec = require$$7.hasOwn(depObj, sockRegPkgName) ? depObj[sockRegPkgName] : undefined;
      if (sockSpec) {
        depAliasMap.set(sockRegPkgName, sockSpec);
      }
      const origSpec = require$$7.hasOwn(depObj, origPkgName) ? depObj[origPkgName] : undefined;
      if (origSpec) {
        let thisSpec = origSpec;
        // Add package aliases for direct dependencies to avoid npm EOVERRIDE
        // errors...
        // https://docs.npmjs.com/cli/v8/using-npm/package-spec#aliases
        if (
        // ...if the spec doesn't start with a valid Socket override.
        !(thisSpec.startsWith(sockOverridePrefix) &&
        // Check the validity of the spec by passing it through npa and
        // seeing if it will coerce to a version.
        vendor.semverExports.coerce(utils.npa(thisSpec).subSpec.rawSpec)?.version)) {
          thisSpec = sockOverrideSpec;
          depObj[origPkgName] = thisSpec;
          state.added.add(sockRegPkgName);
          if (!isWorkspaceRoot) {
            state.addedInWorkspaces.add(workspace);
          }
          if (!loggedAddingText) {
            spinner?.setText(addingText);
            loggedAddingText = true;
          }
        }
        depAliasMap.set(origPkgName, thisSpec);
      }
    }
    if (isWorkspaceRoot) {
      // The lockSrcIncludes and lsStdoutIncludes functions overlap in their
      // first two parameters. lockSrcIncludes accepts an optional third parameter
      // which lsStdoutIncludes will ignore.
      const thingScanner = isLockScanned ? lockSrcIncludes : lsStdoutIncludes;
      const thingToScan = isLockScanned ? lockSrc : await listPackages(pkgEnvDetails, {
        cwd: pkgPath,
        npmExecPath
      });
      // Chunk package names to process them in parallel 3 at a time.
      await require$$8.pEach(overridesDataObjects, async ({
        overrides,
        type
      }) => {
        const overrideExists = require$$7.hasOwn(overrides, origPkgName);
        if (overrideExists || thingScanner(pkgEnvDetails, thingToScan, origPkgName, lockName)) {
          const oldSpec = overrideExists ? overrides[origPkgName] : undefined;
          const origDepAlias = depAliasMap.get(origPkgName);
          const sockRegDepAlias = depAliasMap.get(sockRegPkgName);
          const depAlias = sockRegDepAlias ?? origDepAlias;
          let newSpec = sockOverrideSpec;
          if (type === NPM && depAlias) {
            // With npm one may not set an override for a package that one directly
            // depends on unless both the dependency and the override itself share
            // the exact same spec. To make this limitation easier to deal with,
            // overrides may also be defined as a reference to a spec for a direct
            // dependency by prefixing the name of the package to match the version
            // of with a $.
            // https://docs.npmjs.com/cli/v8/configuring-npm/package-json#overrides
            newSpec = `$${sockRegDepAlias ? sockRegPkgName : origPkgName}`;
          } else if (typeof oldSpec === 'string') {
            const thisSpec = oldSpec.startsWith('$') ? depAlias || newSpec : oldSpec || newSpec;
            if (thisSpec.startsWith(sockOverridePrefix)) {
              if (pin && utils.getMajor(
              // Check the validity of the spec by passing it through npa
              // and seeing if it will coerce to a version. semver.coerce
              // will strip leading v's, carets (^), comparators (<,<=,>,>=,=),
              // and tildes (~). If not coerced to a valid version then
              // default to the manifest entry version.
              vendor.semverExports.coerce(utils.npa(thisSpec).subSpec.rawSpec)?.version ?? version) !== major) {
                const otherVersion = (await packages.fetchPackageManifest(thisSpec))?.version;
                if (otherVersion && otherVersion !== version) {
                  newSpec = `${sockOverridePrefix}${pin ? otherVersion : `^${utils.getMajor(otherVersion)}`}`;
                }
              }
            } else {
              newSpec = oldSpec;
            }
          }
          if (newSpec !== oldSpec) {
            overrides[origPkgName] = newSpec;
            const addedOrUpdated = overrideExists ? 'updated' : 'added';
            state[addedOrUpdated].add(sockRegPkgName);
            if (!loggedAddingText) {
              spinner?.setText(addingText);
              loggedAddingText = true;
            }
          }
        }
      }, {
        concurrency: 3
      });
    }
  }, {
    concurrency: 3
  });
  if (isWorkspace) {
    // Chunk package names to process them in parallel 3 at a time.
    await require$$8.pEach(workspacePkgJsonPaths, async workspacePkgJsonPath => {
      const otherState = await addOverrides(pkgEnvDetails, path.dirname(workspacePkgJsonPath), {
        logger,
        pin,
        prod,
        spinner
      });
      for (const key of ['added', 'addedInWorkspaces', 'updated', 'updatedInWorkspaces']) {
        for (const value of otherState[key]) {
          state[key].add(value);
        }
      }
    }, {
      concurrency: 3
    });
  }
  if (state.added.size > 0 || state.updated.size > 0) {
    pkgEnvDetails.editablePkgJson.update(Object.fromEntries(depEntries));
    if (isWorkspaceRoot) {
      for (const {
        overrides,
        type
      } of overridesDataObjects) {
        updateManifest(type, pkgEnvDetails.editablePkgJson, require$$7.toSortedObject(overrides));
      }
    }
    await pkgEnvDetails.editablePkgJson.save();
  }
  return state;
}

const {
  NPM_BUGGY_OVERRIDES_PATCHED_VERSION
} = constants;
async function updateLockfile(pkgEnvDetails, options) {
  const {
    cmdName = '',
    logger,
    spinner
  } = {
    __proto__: null,
    ...options
  };
  const wasSpinning = !!spinner?.isSpinning;
  spinner?.start(`Updating ${pkgEnvDetails.lockName}...`);
  try {
    await utils.runAgentInstall(pkgEnvDetails, {
      spinner
    });
    if (pkgEnvDetails.features.npmBuggyOverrides) {
      spinner?.stop();
      logger?.log(`💡 Re-run ${cmdName ? `${cmdName} ` : ''}whenever ${pkgEnvDetails.lockName} changes.\n   This can be skipped for ${pkgEnvDetails.agent} >=${NPM_BUGGY_OVERRIDES_PATCHED_VERSION}.`);
    }
  } catch (e) {
    spinner?.stop();
    require$$6.debugFn('error', 'fail: update');
    require$$6.debugDir('inspect', {
      error: e
    });
    if (wasSpinning) {
      spinner.start();
    }
    return {
      ok: false,
      message: 'Update failed',
      cause: utils.cmdPrefixMessage(cmdName, `${pkgEnvDetails.agent} install failed to update ${pkgEnvDetails.lockName}`)
    };
  }
  spinner?.stop();
  if (wasSpinning) {
    spinner.start();
  }
  return {
    ok: true,
    data: undefined
  };
}

async function applyOptimization(pkgEnvDetails, {
  pin,
  prod
}) {
  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  spinner.start();
  const state = await addOverrides(pkgEnvDetails, pkgEnvDetails.pkgPath, {
    logger: logger.logger,
    pin,
    prod,
    spinner
  });
  const addedCount = state.added.size;
  const updatedCount = state.updated.size;
  const pkgJsonChanged = addedCount > 0 || updatedCount > 0;
  if (pkgJsonChanged || pkgEnvDetails.features.npmBuggyOverrides) {
    const result = await updateLockfile(pkgEnvDetails, {
      cmdName: CMD_NAME,
      logger: logger.logger,
      spinner
    });
    if (!result.ok) {
      spinner.stop();
      return result;
    }
  }
  spinner.stop();
  return {
    ok: true,
    data: {
      addedCount,
      addedInWorkspaces: state.addedInWorkspaces.size,
      pkgJsonChanged,
      updatedCount,
      updatedInWorkspaces: state.updatedInWorkspaces.size
    }
  };
}

async function outputOptimizeResult(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const data = result.data;
  if (data.updatedCount > 0) {
    logger.logger?.log(`${createActionMessage('Updated', data.updatedCount, data.updatedInWorkspaces)}${data.addedCount ? '.' : '🚀'}`);
  }
  if (data.addedCount > 0) {
    logger.logger?.log(`${createActionMessage('Added', data.addedCount, data.addedInWorkspaces)} 🚀`);
  }
  if (!data.pkgJsonChanged) {
    logger.logger?.log('Scan complete. No Socket.dev optimized overrides applied.');
  }
  logger.logger.log('');
  logger.logger.success('Finished!');
  logger.logger.log('');
}
function createActionMessage(verb, overrideCount, workspaceCount) {
  return `${verb} ${overrideCount} Socket.dev optimized ${words.pluralize('override', overrideCount)}${workspaceCount ? ` in ${workspaceCount} ${words.pluralize('workspace', workspaceCount)}` : ''}`;
}

const {
  VLT
} = constants;
async function handleOptimize({
  cwd,
  outputKind,
  pin,
  prod
}) {
  const pkgEnvCResult = await utils.detectAndValidatePackageEnvironment(cwd, {
    cmdName: CMD_NAME,
    logger: logger.logger,
    prod
  });
  if (!pkgEnvCResult.ok) {
    await outputOptimizeResult(pkgEnvCResult, outputKind);
    return;
  }
  const pkgEnvDetails = pkgEnvCResult.data;
  if (!pkgEnvDetails) {
    await outputOptimizeResult({
      ok: false,
      message: 'No package found.',
      cause: `No valid package environment found for project path: ${cwd}`
    }, outputKind);
    return;
  }
  const {
    agent,
    agentVersion
  } = pkgEnvDetails;
  if (agent === VLT) {
    await outputOptimizeResult({
      ok: false,
      message: 'Unsupported',
      cause: utils.cmdPrefixMessage(CMD_NAME, `${agent} v${agentVersion} does not support overrides.`)
    }, outputKind);
    return;
  }
  logger.logger.info(`Optimizing packages for ${agent} v${agentVersion}.\n`);
  await outputOptimizeResult(await applyOptimization(pkgEnvDetails, {
    pin,
    prod
  }), outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$r
} = constants;
const config$r = {
  commandName: 'optimize',
  description: 'Optimize dependencies with @socketregistry overrides',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    pin: {
      type: 'boolean',
      default: false,
      description: 'Pin overrides to their latest version'
    },
    prod: {
      type: 'boolean',
      default: false,
      description: 'Only add overrides for production dependencies'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} ./proj/tree --pin
  `
};
const cmdOptimize = {
  description: config$r.description,
  hidden: config$r.hidden,
  run: run$r
};
async function run$r(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$r,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$r);
    return;
  }
  const {
    json,
    markdown,
    pin,
    prod
  } = cli.flags;
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  const outputKind = utils.getOutputKind(json, markdown);
  await handleOptimize({
    cwd,
    pin: Boolean(pin),
    outputKind,
    prod: Boolean(prod)
  });
}

async function fetchDependencies(config, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  const {
    limit,
    offset
  } = {
    __proto__: null,
    ...config
  };
  return await utils.handleApiCall(sockSdk.searchDependencies({
    limit,
    offset
  }), {
    desc: 'organization dependencies'
  });
}

// @ts-ignore
async function outputDependencies(result, {
  limit,
  offset,
  outputKind
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  outputMarkdown(result.data, {
    limit,
    offset
  });
}
function outputMarkdown(result, {
  limit,
  offset
}) {
  logger.logger.log('# Organization dependencies');
  logger.logger.log('');
  logger.logger.log('Request details:');
  logger.logger.log('- Offset:', offset);
  logger.logger.log('- Limit:', limit);
  logger.logger.log('- Is there more data after this?', result.end ? 'no' : 'yes');
  logger.logger.log('');
  const options = {
    columns: [{
      field: 'type',
      name: vendor.yoctocolorsCjsExports.cyan('Ecosystem')
    }, {
      field: 'namespace',
      name: vendor.yoctocolorsCjsExports.cyan('Namespace')
    }, {
      field: 'name',
      name: vendor.yoctocolorsCjsExports.cyan('Name')
    }, {
      field: 'version',
      name: vendor.yoctocolorsCjsExports.cyan('Version')
    }, {
      field: 'repository',
      name: vendor.yoctocolorsCjsExports.cyan('Repository')
    }, {
      field: 'branch',
      name: vendor.yoctocolorsCjsExports.cyan('Branch')
    }, {
      field: 'direct',
      name: vendor.yoctocolorsCjsExports.cyan('Direct')
    }]
  };
  logger.logger.log(vendor.srcExports(options, result.rows));
}

async function handleDependencies({
  limit,
  offset,
  outputKind
}) {
  const result = await fetchDependencies({
    limit,
    offset
  });
  await outputDependencies(result, {
    limit,
    offset,
    outputKind
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$q
} = constants;
const config$q = {
  commandName: 'dependencies',
  description: 'Search for any dependency that is being used in your organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    limit: {
      type: 'number',
      default: 50,
      description: 'Maximum number of dependencies returned'
    },
    offset: {
      type: 'number',
      default: 0,
      description: 'Page number'
    },
    ...flags.outputFlags
  },
  help: (command, config) => `
    Usage
      ${command} [options]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: none (does need token with access to target org)

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      ${command}
      ${command} --limit 20 --offset 10
  `
};
const cmdOrganizationDependencies = {
  description: config$q.description,
  hidden: config$q.hidden,
  run: run$q
};
async function run$q(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$q,
    importMeta,
    parentName
  });
  const {
    json,
    limit,
    markdown,
    offset
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$q);
    return;
  }
  await handleDependencies({
    limit: Number(limit || 0) || 0,
    offset: Number(offset || 0) || 0,
    outputKind
  });
}

async function fetchLicensePolicy(orgSlug, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgLicensePolicy(orgSlug), {
    desc: 'organization license policy'
  });
}

async function outputLicensePolicy(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.info('Use --json to get the full result');
  logger.logger.log('# License policy');
  logger.logger.log('');
  logger.logger.log('This is the license policy for your organization:');
  logger.logger.log('');
  const rules = result.data['license_policy'];
  const entries = rules ? Object.entries(rules) : [];
  const mapped = entries.map(({
    0: key,
    1: value
  }) => [key, value?.['allowed'] ? ' yes' : ' no']);
  mapped.sort(([a], [b]) => a < b ? -1 : a > b ? 1 : 0);
  logger.logger.log(utils.mdTableOfPairs(mapped, ['License Name', 'Allowed']));
  logger.logger.log('');
}

async function handleLicensePolicy(orgSlug, outputKind) {
  const data = await fetchLicensePolicy(orgSlug);
  await outputLicensePolicy(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$p
} = constants;
const config$p = {
  commandName: 'license',
  description: 'Retrieve the license policy of an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: license-policy:read

    Options
      ${utils.getFlagListOutput(config$p.flags)}

    Your API token will need the \`license-policy:read\` permission otherwise
    the request will fail with an authentication error.

    Examples
      $ ${command}
      $ ${command} --json
  `
};
const cmdOrganizationPolicyLicense = {
  description: config$p.description,
  hidden: config$p.hidden,
  run: run$p
};
async function run$p(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$p,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$p);
    return;
  }
  await handleLicensePolicy(orgSlug, outputKind);
}

async function fetchSecurityPolicy(orgSlug, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgSecurityPolicy(orgSlug), {
    desc: 'organization security policy'
  });
}

async function outputSecurityPolicy(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('# Security policy');
  logger.logger.log('');
  logger.logger.log(`The default security policy setting is: "${result.data.securityPolicyDefault}"`);
  logger.logger.log('');
  logger.logger.log('These are the security policies per setting for your organization:');
  logger.logger.log('');
  const rules = result.data.securityPolicyRules;
  const entries = rules ? Object.entries(rules) : [];
  const mapped = entries.map(({
    0: key,
    1: value
  }) => [key, value.action]);
  mapped.sort(([a], [b]) => a < b ? -1 : a > b ? 1 : 0);
  logger.logger.log(utils.mdTableOfPairs(mapped, ['name', 'action']));
  logger.logger.log('');
}

async function handleSecurityPolicy(orgSlug, outputKind) {
  const data = await fetchSecurityPolicy(orgSlug);
  await outputSecurityPolicy(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$o
} = constants;
const config$o = {
  commandName: 'security',
  description: 'Retrieve the security policy of an organization',
  hidden: true,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: security-policy:read

    Options
      ${utils.getFlagListOutput(config$o.flags)}

    Your API token will need the \`security-policy:read\` permission otherwise
    the request will fail with an authentication error.

    Examples
      $ ${command}
      $ ${command} --json
  `
};
const cmdOrganizationPolicySecurity = {
  description: config$o.description,
  hidden: config$o.hidden,
  run: run$o
};
async function run$o(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$o,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$o);
    return;
  }
  await handleSecurityPolicy(orgSlug, outputKind);
}

async function outputOrganizationList(result, outputKind = 'text') {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const organizations = Object.values(result.data.organizations);
  const visibleTokenPrefix = utils.getVisibleTokenPrefix();
  switch (outputKind) {
    case 'markdown':
      {
        // | Syntax      | Description |
        // | ----------- | ----------- |
        // | Header      | Title       |
        // | Paragraph   | Text        |
        let mw1 = 4;
        let mw2 = 2;
        let mw3 = 4;
        for (const o of organizations) {
          mw1 = Math.max(mw1, o.name?.length ?? 0);
          mw2 = Math.max(mw2, o.id.length);
          mw3 = Math.max(mw3, o.plan.length);
        }
        logger.logger.log('# Organizations\n');
        logger.logger.log(`List of organizations associated with your API token, starting with: ${vendor.yoctocolorsCjsExports.italic(visibleTokenPrefix)}\n`);
        logger.logger.log(`| Name${' '.repeat(mw1 - 4)} | ID${' '.repeat(mw2 - 2)} | Plan${' '.repeat(mw3 - 4)} |`);
        logger.logger.log(`| ${'-'.repeat(mw1)} | ${'-'.repeat(mw2)} | ${'-'.repeat(mw3)} |`);
        for (const o of organizations) {
          logger.logger.log(`| ${(o.name || '').padEnd(mw1, ' ')} | ${(o.id || '').padEnd(mw2, ' ')} | ${(o.plan || '').padEnd(mw3, ' ')} |`);
        }
        logger.logger.log(`| ${'-'.repeat(mw1)} | ${'-'.repeat(mw2)} | ${'-'.repeat(mw3)} |`);
        return;
      }
    default:
      {
        logger.logger.log(`List of organizations associated with your API token, starting with: ${vendor.yoctocolorsCjsExports.italic(visibleTokenPrefix)}\n`);
        // Just dump
        for (const o of organizations) {
          logger.logger.log(`- Name: ${vendor.yoctocolorsCjsExports.bold(o.name ?? 'undefined')}, ID: ${vendor.yoctocolorsCjsExports.bold(o.id)}, Plan: ${vendor.yoctocolorsCjsExports.bold(o.plan)}`);
        }
      }
  }
}

async function handleOrganizationList(outputKind = 'text') {
  const data = await utils.fetchOrganization();
  await outputOrganizationList(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$n
} = constants;
const config$n = {
  commandName: 'list',
  description: 'List organizations associated with the Socket API token',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: none (does need a token)

    Options
      ${utils.getFlagListOutput(config$n.flags)}

    Examples
      $ ${command}
      $ ${command} --json
  `
};
const cmdOrganizationList = {
  description: config$n.description,
  hidden: config$n.hidden,
  run: run$n
};
async function run$n(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$n,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$n);
    return;
  }
  await handleOrganizationList(outputKind);
}

const description$5 = 'Organization policy details';
const cmdOrganizationPolicy = {
  description: description$5,
  // Hidden because it was broken all this time (nobody could be using it)
  // and we're not sure if it's useful to anyone in its current state.
  // Until we do, we'll hide this to keep the help tidier.
  // And later, we may simply move this under `scan`, anyways.
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      security: cmdOrganizationPolicySecurity,
      license: cmdOrganizationPolicyLicense
    }, {
      argv,
      description: description$5,
      defaultSub: 'list',
      // Backwards compat
      importMeta,
      name: `${parentName} policy`
    });
  }
};

async function fetchQuota(options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getQuota(), {
    desc: 'token quota'
  });
}

async function outputQuota(result, outputKind = 'text') {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log('# Quota');
    logger.logger.log('');
    logger.logger.log(`Quota left on the current API token: ${result.data.quota}`);
    logger.logger.log('');
    return;
  }
  logger.logger.log(`Quota left on the current API token: ${result.data.quota}`);
  logger.logger.log('');
}

async function handleQuota(outputKind = 'text') {
  const data = await fetchQuota();
  await outputQuota(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$m
} = constants;
const config$m = {
  commandName: 'quota',
  description: 'List organizations associated with the Socket API token',
  hidden: true,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags
  },
  help: (command, _config) => `
    Usage
      $ ${command} [options]

    Options
      ${utils.getFlagListOutput(config$m.flags)}

    Examples
      $ ${command}
      $ ${command} --json
  `
};
const cmdOrganizationQuota = {
  description: config$m.description,
  hidden: config$m.hidden,
  run: run$m
};
async function run$m(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$m,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  const json = Boolean(cli.flags['json']);
  const markdown = Boolean(cli.flags['markdown']);
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$m);
    return;
  }
  await handleQuota(outputKind);
}

const description$4 = 'Manage Socket organization account details';
const cmdOrganization = {
  description: description$4,
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      dependencies: cmdOrganizationDependencies,
      list: cmdOrganizationList,
      quota: cmdOrganizationQuota,
      policy: cmdOrganizationPolicy
    }, {
      aliases: {
        deps: {
          description: cmdOrganizationDependencies.description,
          hidden: true,
          argv: ['dependencies']
        },
        license: {
          description: cmdOrganizationPolicyLicense.description,
          hidden: true,
          argv: ['policy', 'license']
        },
        security: {
          description: cmdOrganizationPolicySecurity.description,
          hidden: true,
          argv: ['policy', 'security']
        }
      },
      argv,
      description: description$4,
      importMeta,
      name: `${parentName} organization`
    });
  }
};

async function fetchPurlDeepScore(purl) {
  logger.logger.info(`Requesting deep score data for this purl: ${purl}`);
  return await utils.queryApiSafeJson(`purl/score/${encodeURIComponent(purl)}`, 'the deep package scores');
}

async function outputPurlsDeepScore(purl, result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    const md = createMarkdownReport(result.data);
    logger.logger.success(`Score report for "${result.data.purl}" ("${purl}"):\n`);
    logger.logger.log(md);
    return;
  }
  logger.logger.log(`Score report for "${purl}" (use --json for raw and --markdown for formatted reports):`);
  logger.logger.log(result.data);
  logger.logger.log('');
}
function createMarkdownReport(data) {
  const {
    self: {
      alerts: selfAlerts,
      capabilities: selfCaps,
      purl,
      score: selfScore
    },
    transitively: {
      alerts,
      capabilities,
      dependencyCount,
      func,
      lowest,
      score
    }
  } = data;
  const o = ['# Complete Package Score', ''];
  if (dependencyCount) {
    o.push(`This is a Socket report for the package *"${purl}"* and its *${dependencyCount}* direct/transitive dependencies.`);
  } else {
    o.push(`This is a Socket report for the package *"${purl}"*. It has *no dependencies*.`);
  }
  o.push('');
  if (dependencyCount) {
    o.push(`It will show you the shallow score for just the package itself and a deep score for all the transitives combined. Additionally you can see which capabilities were found and the top alerts as well as a package that was responsible for it.`);
  } else {
    o.push(`It will show you the shallow score for the package itself, which capabilities were found, and its top alerts.`);
    o.push('');
    o.push('Since it has no dependencies, the shallow score is also the deep score.');
  }
  o.push('');
  if (dependencyCount) {
    // This doesn't make much sense if there are no dependencies. Better to omit it.
    o.push('The report should give you a good insight into the status of this package.');
    o.push('');
    o.push('## Package itself');
    o.push('');
    o.push('Here are results for the package itself (excluding data from dependencies).');
  } else {
    o.push('## Report');
    o.push('');
    o.push('The report should give you a good insight into the status of this package.');
  }
  o.push('');
  o.push('### Shallow Score');
  o.push('');
  o.push('This score is just for the package itself:');
  o.push('');
  o.push(`- Overall: ${selfScore.overall}`);
  o.push(`- Maintenance: ${selfScore.maintenance}`);
  o.push(`- Quality: ${selfScore.quality}`);
  o.push(`- Supply Chain: ${selfScore.supplyChain}`);
  o.push(`- Vulnerability: ${selfScore.vulnerability}`);
  o.push(`- License: ${selfScore.license}`);
  o.push('');
  o.push('### Capabilities');
  o.push('');
  if (selfCaps.length) {
    o.push('These are the capabilities detected in the package itself:');
    o.push('');
    for (const cap of selfCaps) {
      o.push(`- ${cap}`);
    }
  } else {
    o.push('No capabilities were found in the package.');
  }
  o.push('');
  o.push('### Alerts for this package');
  o.push('');
  if (selfAlerts.length) {
    if (dependencyCount) {
      o.push('These are the alerts found for the package itself:');
    } else {
      o.push('These are the alerts found for this package:');
    }
    o.push('');
    o.push(utils.mdTable(selfAlerts, ['severity', 'name'], ['Severity', 'Alert Name']));
  } else {
    o.push('There are currently no alerts for this package.');
  }
  o.push('');
  if (dependencyCount) {
    o.push('## Transitive Package Results');
    o.push('');
    o.push('Here are results for the package and its direct/transitive dependencies.');
    o.push('');
    o.push('### Deep Score');
    o.push('');
    o.push('This score represents the package and and its direct/transitive dependencies:');
    o.push(`The function used to calculate the values in aggregate is: *"${func}"*`);
    o.push('');
    o.push(`- Overall: ${score.overall}`);
    o.push(`- Maintenance: ${score.maintenance}`);
    o.push(`- Quality: ${score.quality}`);
    o.push(`- Supply Chain: ${score.supplyChain}`);
    o.push(`- Vulnerability: ${score.vulnerability}`);
    o.push(`- License: ${score.license}`);
    o.push('');
    o.push('### Capabilities');
    o.push('');
    o.push('These are the packages with the lowest recorded score. If there is more than one with the lowest score, just one is shown here. This may help you figure out the source of low scores.');
    o.push('');
    o.push(`- Overall: ${lowest.overall}`);
    o.push(`- Maintenance: ${lowest.maintenance}`);
    o.push(`- Quality: ${lowest.quality}`);
    o.push(`- Supply Chain: ${lowest.supplyChain}`);
    o.push(`- Vulnerability: ${lowest.vulnerability}`);
    o.push(`- License: ${lowest.license}`);
    o.push('');
    o.push('### Capabilities');
    o.push('');
    if (capabilities.length) {
      o.push('These are the capabilities detected in at least one package:');
      o.push('');
      for (const cap of capabilities) {
        o.push(`- ${cap}`);
      }
    } else {
      o.push('This package had no capabilities and neither did any of its direct/transitive dependencies.');
    }
    o.push('');
    o.push('### Alerts');
    o.push('');
    if (alerts.length) {
      o.push('These are the alerts found:');
      o.push('');
      o.push(utils.mdTable(alerts, ['severity', 'name', 'example'], ['Severity', 'Alert Name', 'Example package reporting it']));
    } else {
      o.push('This package had no alerts and neither did any of its direct/transitive dependencies');
    }
    o.push('');
  }
  return o.join('\n');
}

async function handlePurlDeepScore(purl, outputKind) {
  const result = await fetchPurlDeepScore(purl);
  await outputPurlsDeepScore(purl, result, outputKind);
}

// Either an ecosystem was given or all args must be (namespaced) purls
// The `pkg:` part is optional here. We'll scan for `eco/name@version`.
// Not hardcoding the namespace since we don't know what the server accepts.
// The ecosystem is considered as the first package if it is not an a-z string.
function parsePackageSpecifiers(ecosystem, pkgs) {
  let valid = true;
  const purls = [];
  if (!ecosystem) {
    valid = false;
  } else if (/^[a-zA-Z]+$/.test(ecosystem)) {
    for (let i = 0; i < pkgs.length; ++i) {
      const pkg = pkgs[i] ?? '';
      if (!pkg) {
        valid = false;
        break;
      } else if (pkg.startsWith('pkg:')) {
        // keep
        purls.push(pkg);
      } else {
        purls.push('pkg:' + ecosystem + '/' + pkg);
      }
    }
    if (!purls.length) {
      valid = false;
    }
  } else {
    // Assume ecosystem is a purl, too.
    pkgs.unshift(ecosystem);
    for (let i = 0; i < pkgs.length; ++i) {
      const pkg = pkgs[i] ?? '';
      if (!/^(?:pkg:)?[a-zA-Z]+\/./.test(pkg)) {
        // At least one purl did not start with `pkg:eco/x` or `eco/x`.
        valid = false;
        break;
      } else if (pkg.startsWith('pkg:')) {
        purls.push(pkg);
      } else {
        purls.push('pkg:' + pkg);
      }
    }
    if (!purls.length) {
      valid = false;
    }
  }
  return {
    purls,
    valid
  };
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$l
} = constants;
const config$l = {
  commandName: 'score',
  description: 'Look up score for one package which reflects all of its transitive dependencies as well',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <<ECOSYSTEM> <NAME> | <PURL>>

    API Token Requirements
      - Quota: 100 units
      - Permissions: packages:list

    Options
      ${utils.getFlagListOutput(config.flags)}

    Show deep scoring details for one package. The score will reflect the package
    itself, any of its dependencies, and any of its transitive dependencies.

    When you want to know whether to trust a package, this is the command to run.

    See also the \`socket package shallow\` command, which returns the shallow
    score for any number of packages. That will not reflect the dependency scores.

    Only a few ecosystems are supported like npm, pypi, nuget, gem, golang, and maven.

    A "purl" is a standard package name formatting: \`pkg:eco/name@version\`
    This command will automatically prepend "pkg:" when not present.

    The version is optional but when given should be a direct match. The \`pkg:\`
    prefix is optional.

    Note: if a package cannot be found it may be too old or perhaps was removed
          before we had the opportunity to process it.

    Examples
      $ ${command} npm babel-cli
      $ ${command} npm eslint@1.0.0 --json
      $ ${command} pkg:golang/github.com/steelpoor/tlsproxy@v0.0.0-20250304082521-29051ed19c60
      $ ${command} nuget/needpluscommonlibrary@1.0.0 --markdown
  `
};
const cmdPackageScore = {
  description: config$l.description,
  hidden: config$l.hidden,
  run: run$l
};
async function run$l(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$l,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [ecosystem = '', purl] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const {
    purls,
    valid
  } = parsePackageSpecifiers(ecosystem, purl ? [purl] : []);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: valid,
    message: 'First parameter must be an ecosystem or the whole purl',
    fail: 'bad'
  }, {
    test: purls.length === 1,
    message: 'Expecting at least one package',
    fail: purls.length === 0 ? 'missing' : 'too many'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$l);
    return;
  }
  await handlePurlDeepScore(purls[0] || '', outputKind);
}

async function fetchPurlsShallowScore(purls, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  logger.logger.info(`Requesting shallow score data for ${purls.length} package urls (purl): ${purls.join(', ')}`);
  const batchPackageCResult = await utils.handleApiCall(sockSdk.batchPackageFetch({
    components: purls.map(purl => ({
      purl
    }))
  }, {
    alerts: 'true'
  }), {
    desc: 'looking up package'
  });
  if (!batchPackageCResult.ok) {
    return batchPackageCResult;
  }

  // TODO: Seems like there's a bug in the typing since we absolutely have to
  // return the .data here.
  return {
    ok: true,
    data: batchPackageCResult.data
  };
}

// This is a simplified view of an artifact. Potentially merged with other artifacts.

function outputPurlsShallowScore(purls, result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const {
    missing,
    rows
  } = preProcess(result.data, purls);
  if (outputKind === 'markdown') {
    const md = generateMarkdownReport(rows, missing);
    logger.logger.log(md);
    return;
  }
  const txt = generateTextReport(rows, missing);
  logger.logger.log(txt);
}
function formatReportCard(artifact, color) {
  const scoreResult = {
    'Supply Chain Risk': Math.floor((artifact.score?.supplyChain ?? 0) * 100),
    Maintenance: Math.floor((artifact.score?.maintenance ?? 0) * 100),
    Quality: Math.floor((artifact.score?.quality ?? 0) * 100),
    Vulnerabilities: Math.floor((artifact.score?.vulnerability ?? 0) * 100),
    License: Math.floor((artifact.score?.license ?? 0) * 100)
  };
  const alertString = getAlertString(artifact.alerts, !color);
  if (!artifact.ecosystem) {
    require$$6.debugFn('notice', 'miss: artifact ecosystem', artifact);
  }
  const purl = `pkg:${artifact.ecosystem}/${artifact.name}${artifact.version ? '@' + artifact.version : ''}`;
  return ['Package: ' + (color ? vendor.yoctocolorsCjsExports.bold(purl) : purl), '', ...Object.entries(scoreResult).map(score => `- ${score[0]}:`.padEnd(20, ' ') + `  ${formatScore(score[1], !color, true)}`), alertString].join('\n');
}
function formatScore(score, noColor = false, pad = false) {
  const padded = String(score).padStart(pad ? 3 : 0, ' ');
  if (noColor) {
    return padded;
  }
  if (score >= 80) {
    return vendor.yoctocolorsCjsExports.green(padded);
  }
  if (score >= 60) {
    return vendor.yoctocolorsCjsExports.yellow(padded);
  }
  return vendor.yoctocolorsCjsExports.red(padded);
}
function getAlertString(alerts, noColor = false) {
  if (!alerts.size) {
    return noColor ? `- Alerts: none!` : `- Alerts: ${vendor.yoctocolorsCjsExports.green('none')}!`;
  }
  const o = Array.from(alerts.values());
  const bad = o.filter(alert => alert.severity !== 'low' && alert.severity !== 'middle').sort((a, b) => a.type < b.type ? -1 : a.type > b.type ? 1 : 0);
  const mid = o.filter(alert => alert.severity === 'middle').sort((a, b) => a.type < b.type ? -1 : a.type > b.type ? 1 : 0);
  const low = o.filter(alert => alert.severity === 'low').sort((a, b) => a.type < b.type ? -1 : a.type > b.type ? 1 : 0);

  // We need to create the no-color string regardless because the actual string
  // contains a bunch of invisible ANSI chars which would screw up length checks.
  const colorless = `- Alerts (${bad.length}/${mid.length.toString()}/${low.length}):`;
  if (noColor) {
    return colorless + ' '.repeat(Math.max(0, 20 - colorless.length)) + '  ' + [bad.map(alert => `[${alert.severity}] ` + alert.type).join(', '), mid.map(alert => `[${alert.severity}] ` + alert.type).join(', '), low.map(alert => `[${alert.severity}] ` + alert.type).join(', ')].filter(Boolean).join(', ');
  }
  return `- Alerts (${vendor.yoctocolorsCjsExports.red(bad.length.toString())}/${vendor.yoctocolorsCjsExports.yellow(mid.length.toString())}/${low.length}):` + ' '.repeat(Math.max(0, 20 - colorless.length)) + '  ' + [bad.map(alert => vendor.yoctocolorsCjsExports.red(vendor.yoctocolorsCjsExports.dim(`[${alert.severity}] `) + alert.type)).join(', '), mid.map(alert => vendor.yoctocolorsCjsExports.yellow(vendor.yoctocolorsCjsExports.dim(`[${alert.severity}] `) + alert.type)).join(', '), low.map(alert => vendor.yoctocolorsCjsExports.dim(`[${alert.severity}] `) + alert.type).join(', ')].filter(Boolean).join(', ');
}
function preProcess(artifacts, requestedPurls) {
  // Dedupe results (for example, pypi will emit one package for each system release (win/mac/cpu) even if it's
  // the same package version with same results. The duplication is irrelevant and annoying to the user.

  // Make some effort to match the requested data with the response
  // Dedupe and merge results when only the .release value is different

  // API does not tell us which purls were not found.
  // Generate all purls to try so we can try to match search request.
  const purls = new Set();
  for (const data of artifacts) {
    purls.add(`pkg:${data.type}/${data.namespace ? `${data.namespace}/` : ''}${data.name}@${data.version}`);
    purls.add(`pkg:${data.type}/${data.name}@${data.version}`);
    purls.add(`pkg:${data.type}/${data.name}`);
    purls.add(`pkg:${data.type}/${data.namespace ? `${data.namespace}/` : ''}${data.name}`);
  }
  // Try to match the searched purls against this list
  const missing = requestedPurls.filter(purl => {
    if (purls.has(purl)) {
      return false;
    }
    if (purl.endsWith('@latest') && purls.has(purl.slice(0, -'@latest'.length))) {
      return false;
    }
    // Not found.
    return true;
  });

  // Create a unique set of rows which represents each artifact that is returned
  // while deduping when the artifact (main) meta data only differs due to the
  // .release field (observed with python, at least).
  // Merge the alerts for duped packages. Use lowest score between all of them.
  const rows = new Map();
  for (const artifact of artifacts) {
    const purl = `pkg:${artifact.type}/${artifact.namespace ? `${artifact.namespace}/` : ''}${artifact.name}${artifact.version ? `@${artifact.version}` : ''}`;
    if (rows.has(purl)) {
      const row = rows.get(purl);
      if (!row) {
        // Unreachable; Satisfy TS.
        continue;
      }
      if ((artifact.score?.supplyChain || 100) < row.score.supplyChain) {
        row.score.supplyChain = artifact.score?.supplyChain || 100;
      }
      if ((artifact.score?.maintenance || 100) < row.score.maintenance) {
        row.score.maintenance = artifact.score?.maintenance || 100;
      }
      if ((artifact.score?.quality || 100) < row.score.quality) {
        row.score.quality = artifact.score?.quality || 100;
      }
      if ((artifact.score?.vulnerability || 100) < row.score.vulnerability) {
        row.score.vulnerability = artifact.score?.vulnerability || 100;
      }
      if ((artifact.score?.license || 100) < row.score.license) {
        row.score.license = artifact.score?.license || 100;
      }
      artifact.alerts?.forEach(({
        severity,
        type
      }) => {
        row.alerts.set(`${type}:${severity}`, {
          type: type ?? 'unknown',
          severity: severity ?? 'none'
        });
      });
    } else {
      const alerts = new Map();
      artifact.alerts?.forEach(({
        severity,
        type
      }) => {
        alerts.set(`${type}:${severity}`, {
          type: type ?? 'unknown',
          severity: severity ?? 'none'
        });
      });
      rows.set(purl, {
        ecosystem: artifact.type,
        namespace: artifact.namespace || '',
        name: artifact.name,
        version: artifact.version || '',
        score: {
          supplyChain: artifact.score?.supplyChain || 100,
          maintenance: artifact.score?.maintenance || 100,
          quality: artifact.score?.quality || 100,
          vulnerability: artifact.score?.vulnerability || 100,
          license: artifact.score?.license || 100
        },
        alerts
      });
    }
  }
  return {
    rows,
    missing
  };
}
function generateMarkdownReport(artifacts, missing) {
  const blocks = [];
  const dupes = new Set();
  for (const artifact of artifacts.values()) {
    const block = `## ${formatReportCard(artifact, false)}`;
    if (dupes.has(block)) {
      // Omit duplicate blocks.
      continue;
    }
    dupes.add(block);
    blocks.push(block);
  }
  return `
# Shallow Package Report

This report contains the response for requesting data on some package url(s).

Please note: The listed scores are ONLY for the package itself. It does NOT
             reflect the scores of any dependencies, transitive or otherwise.

${missing.length ? `\n## Missing response\n\nAt least one package had no response or the purl was not canonical:\n\n${missing.map(purl => `- ${purl}\n`).join('')}` : ''}

${blocks.join('\n\n\n')}
    `.trim();
}
function generateTextReport(artifacts, missing) {
  const o = [];
  o.push(`\n${vendor.yoctocolorsCjsExports.bold('Shallow Package Score')}\n`);
  o.push('Please note: The listed scores are ONLY for the package itself. It does NOT\n' + '             reflect the scores of any dependencies, transitive or otherwise.');
  if (missing.length) {
    o.push(`\nAt least one package had no response or the purl was not canonical:\n${missing.map(purl => `\n- ${vendor.yoctocolorsCjsExports.bold(purl)}`).join('')}`);
  }
  const dupes = new Set();
  for (const artifact of artifacts.values()) {
    const block = formatReportCard(artifact, true);
    if (dupes.has(block)) {
      // Omit duplicate blocks.
      continue;
    }
    dupes.add(block);
    o.push('\n');
    o.push(block);
  }
  o.push('');
  return o.join('\n');
}

async function handlePurlsShallowScore({
  outputKind,
  purls
}) {
  const packageData = await fetchPurlsShallowScore(purls);
  outputPurlsShallowScore(purls, packageData, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$k
} = constants;
const config$k = {
  commandName: 'shallow',
  description: 'Look up info regarding one or more packages but not their transitives',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <<ECOSYSTEM> <PKGNAME> [<PKGNAME> ...] | <PURL> [<PURL> ...]>

    API Token Requirements
      - Quota: 100 units
      - Permissions: packages:list

    Options
      ${utils.getFlagListOutput(config.flags)}

    Show scoring details for one or more packages purely based on their own package.
    This means that any dependency scores are not reflected by the score. You can
    use the \`socket package score <pkg>\` command to get its full transitive score.

    Only a few ecosystems are supported like npm, pypi, nuget, gem, golang, and maven.

    A "purl" is a standard package name formatting: \`pkg:eco/name@version\`
    This command will automatically prepend "pkg:" when not present.

    If the first arg is an ecosystem, remaining args that are not a purl are
    assumed to be scoped to that ecosystem. The \`pkg:\` prefix is optional.

    Note: if a package cannot be found, it may be too old or perhaps was removed
          before we had the opportunity to process it.

    Examples
      $ ${command} npm webtorrent
      $ ${command} npm webtorrent@1.9.1
      $ ${command} npm/webtorrent@1.9.1
      $ ${command} pkg:npm/webtorrent@1.9.1
      $ ${command} maven webtorrent babel
      $ ${command} npm/webtorrent golang/babel
      $ ${command} npm npm/webtorrent@1.0.1 babel
  `
};
const cmdPackageShallow = {
  description: config$k.description,
  hidden: config$k.hidden,
  alias: {
    shallowScore: {
      description: config$k.description,
      hidden: true,
      argv: []
    }
  },
  run: run$k
};
async function run$k(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$k,
    importMeta,
    parentName
  });
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const [ecosystem = '', ...pkgs] = cli.input;
  const outputKind = utils.getOutputKind(json, markdown);
  const {
    purls,
    valid
  } = parsePackageSpecifiers(ecosystem, pkgs);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: valid,
    message: 'First parameter should be an ecosystem or all args must be purls',
    fail: 'bad'
  }, {
    test: purls.length > 0,
    message: 'Expecting at least one package',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$k);
    return;
  }
  await handlePurlsShallowScore({
    outputKind,
    purls
  });
}

const description$3 = 'Look up published package details';
const cmdPackage = {
  description: description$3,
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      score: cmdPackageScore,
      shallow: cmdPackageShallow
    }, {
      aliases: {
        deep: {
          description: description$3,
          hidden: true,
          argv: ['score']
        }
      },
      argv,
      description: description$3,
      importMeta,
      name: `${parentName} package`
    });
  }
};

async function runRawNpm(argv) {
  const spawnPromise = spawn.spawn(utils.getNpmBinPath(), argv, {
    // Lazily access constants.WIN32.
    shell: constants.WIN32,
    stdio: 'inherit'
  });
  // See https://nodejs.org/api/child_process.html#event-exit.
  spawnPromise.process.on('exit', (code, signalName) => {
    if (signalName) {
      process.kill(process.pid, signalName);
    } else if (code !== null) {
      // eslint-disable-next-line n/no-process-exit
      process.exit(code);
    }
  });
  await spawnPromise;
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$j
} = constants;
const config$j = {
  commandName: 'raw-npm',
  description: 'Run npm without the Socket wrapper',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: command => `
    Usage
      $ ${command} ...

    This does the opposite of \`socket npm\`: it will execute the real \`npm\`
    command without Socket. This can be useful when you have the wrapper on
    and want to install a certain package anyways. Use at your own risk.

    Note: Everything after "raw-npm" is sent straight to the npm command.
          Only the \`--dryRun\` and \`--help\` flags are caught here.

    Examples
      $ ${command} install -g socket
  `
};
const cmdRawNpm = {
  description: config$j.description,
  hidden: config$j.hidden,
  run: run$j
};
async function run$j(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$j,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$j);
    return;
  }
  await runRawNpm(argv);
}

async function runRawNpx(argv) {
  const spawnPromise = spawn.spawn(utils.getNpxBinPath(), argv, {
    // Lazily access constants.WIN32.
    shell: constants.WIN32,
    stdio: 'inherit'
  });
  // See https://nodejs.org/api/child_process.html#event-exit.
  spawnPromise.process.on('exit', (code, signalName) => {
    if (signalName) {
      process.kill(process.pid, signalName);
    } else if (code !== null) {
      // eslint-disable-next-line n/no-process-exit
      process.exit(code);
    }
  });
  await spawnPromise;
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$i
} = constants;
const config$i = {
  commandName: 'raw-npx',
  description: 'Run npx without the Socket wrapper',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: command => `
    Usage
      $ ${command} ...

    This does the opposite of \`socket npx\`: it will execute the real \`npx\`
    command without Socket. This can be useful when you have the wrapper on
    and want to run a certain package anyways. Use at your own risk.

    Note: Everything after "raw-npx" is sent straight to the npx command.
          Only the \`--dryRun\` and \`--help\` flags are caught here.

    Examples
      $ ${command} prettier
  `
};
const cmdRawNpx = {
  description: config$i.description,
  hidden: config$i.hidden,
  run: run$i
};
async function run$i(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$i,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$i);
    return;
  }
  await runRawNpx(argv);
}

async function fetchCreateRepo(config, options) {
  const {
    defaultBranch,
    description,
    homepage,
    orgSlug,
    repoName,
    visibility
  } = config;
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.createOrgRepo(orgSlug, {
    default_branch: defaultBranch,
    description,
    homepage,
    name: repoName,
    visibility
  }), {
    desc: 'to create a repository'
  });
}

function outputCreateRepo(result, requestedName, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const {
    slug
  } = result.data;
  logger.logger.success(`OK. Repository created successfully, slug: \`${slug}\`${slug !== requestedName ? ' (Warning: slug is not the same as name that was requested!)' : ''}`);
}

async function handleCreateRepo({
  defaultBranch,
  description,
  homepage,
  orgSlug,
  repoName,
  visibility
}, outputKind) {
  const data = await fetchCreateRepo({
    defaultBranch,
    description,
    homepage,
    orgSlug,
    repoName,
    visibility
  });
  outputCreateRepo(data, repoName, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$h
} = constants;
const config$h = {
  commandName: 'create',
  description: 'Create a repository in an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    defaultBranch: {
      type: 'string',
      default: 'main',
      description: 'Repository default branch. Defaults to "main"'
    },
    homepage: {
      type: 'string',
      default: '',
      description: 'Repository url'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    repoDescription: {
      type: 'string',
      default: '',
      description: 'Repository description'
    },
    visibility: {
      type: 'string',
      default: 'private',
      description: 'Repository visibility (Default Private)'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <REPO>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: repo:create

    The REPO name should be a "slug". Follows the same naming convention as GitHub.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} test-repo
      $ ${command} our-repo --homepage=socket.dev --default-branch=trunk
  `
};
const cmdRepositoryCreate = {
  description: config$h.description,
  hidden: config$h.hidden,
  run: run$h
};
async function run$h(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$h,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['repoName'];
  const [repoName = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    test: !!repoName,
    message: 'Repository name as first argument',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$h);
    return;
  }
  await handleCreateRepo({
    orgSlug,
    repoName: String(repoName),
    description: String(cli.flags['repoDescription'] || ''),
    homepage: String(cli.flags['homepage'] || ''),
    defaultBranch: String(cli.flags['defaultBranch'] || ''),
    visibility: String(cli.flags['visibility'] || 'private')
  }, outputKind);
}

async function fetchDeleteRepo(orgSlug, repoName, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.deleteOrgRepo(orgSlug, repoName), {
    desc: 'to delete a repository'
  });
}

async function outputDeleteRepo(result, repoName, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.success(`OK. Repository \`${repoName}\` deleted successfully`);
}

async function handleDeleteRepo(orgSlug, repoName, outputKind) {
  const data = await fetchDeleteRepo(orgSlug, repoName);
  await outputDeleteRepo(data, repoName, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$g
} = constants;
const config$g = {
  commandName: 'del',
  description: 'Delete a repository in an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <REPO>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: repo:delete

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} test-repo
  `
};
const cmdRepositoryDel = {
  description: config$g.description,
  hidden: config$g.hidden,
  run: run$g
};
async function run$g(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$g,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['repoName'];
  const [repoName = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    test: !!repoName,
    message: 'Repository name as first argument',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$g);
    return;
  }
  await handleDeleteRepo(orgSlug, repoName, outputKind);
}

async function fetchListAllRepos(orgSlug, options) {
  const {
    direction,
    sdkOptions,
    sort
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  const rows = [];
  let protection = 0;
  let nextPage = 0;
  while (nextPage >= 0) {
    if (++protection > 100) {
      return {
        ok: false,
        message: 'Infinite loop detected',
        cause: `Either there are over 100 pages of results or the fetch has run into an infinite loop. Breaking it off now. nextPage=${nextPage}`
      };
    }
    // eslint-disable-next-line no-await-in-loop
    const orgRepoListCResult = await utils.handleApiCall(sockSdk.getOrgRepoList(orgSlug, {
      sort,
      direction,
      per_page: String(100),
      // max
      page: String(nextPage)
    }), {
      desc: 'list of repositories'
    });
    if (!orgRepoListCResult.ok) {
      return orgRepoListCResult;
    }
    rows.push(...orgRepoListCResult.data.results);
    nextPage = orgRepoListCResult.data.nextPage ?? -1;
  }
  return {
    ok: true,
    data: {
      results: rows,
      nextPage: null
    }
  };
}

async function fetchListRepos(config, options) {
  const {
    direction,
    orgSlug,
    page,
    perPage,
    sort
  } = {
    __proto__: null,
    ...config
  };
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgRepoList(orgSlug, {
    sort,
    direction,
    per_page: String(perPage),
    page: String(page)
  }), {
    desc: 'list of repositories'
  });
}

// @ts-ignore
async function outputListRepos(result, outputKind, page, nextPage, sort, perPage, direction) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    if (result.ok) {
      logger.logger.log(utils.serializeResultJson({
        ok: true,
        data: {
          data: result.data,
          direction,
          nextPage: nextPage ?? 0,
          page,
          perPage,
          sort
        }
      }));
    } else {
      logger.logger.log(utils.serializeResultJson(result));
    }
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log(`Result page: ${page}, results per page: ${perPage === Infinity ? 'all' : perPage}, sorted by: ${sort}, direction: ${direction}`);
  const options = {
    columns: [{
      field: 'id',
      name: vendor.yoctocolorsCjsExports.magenta('ID')
    }, {
      field: 'name',
      name: vendor.yoctocolorsCjsExports.magenta('Name')
    }, {
      field: 'visibility',
      name: vendor.yoctocolorsCjsExports.magenta('Visibility')
    }, {
      field: 'default_branch',
      name: vendor.yoctocolorsCjsExports.magenta('Default branch')
    }, {
      field: 'archived',
      name: vendor.yoctocolorsCjsExports.magenta('Archived')
    }]
  };
  logger.logger.log(vendor.srcExports(options, result.data.results));
  if (nextPage) {
    logger.logger.info(`This is page ${page}. Server indicated there are more results available on page ${nextPage}...`);
    logger.logger.info(`(Hint: you can use \`socket repository list --page ${nextPage}\`)`);
  } else if (perPage === Infinity) {
    logger.logger.info(`This should be the entire list available on the server.`);
  } else {
    logger.logger.info(`This is page ${page}. Server indicated this is the last page with results.`);
  }
}

async function handleListRepos({
  all,
  direction,
  orgSlug,
  outputKind,
  page,
  perPage,
  sort
}) {
  if (all) {
    const data = await fetchListAllRepos(orgSlug, {
      direction,
      sort
    });
    await outputListRepos(data, outputKind, 0, 0, sort, Infinity, direction);
  } else {
    const data = await fetchListRepos({
      direction,
      orgSlug,
      page,
      perPage,
      sort
    });
    if (!data.ok) {
      await outputListRepos(data, outputKind, 0, 0, '', 0, direction);
    } else {
      // Note: nextPage defaults to 0, is null when there's no next page
      await outputListRepos(data, outputKind, page, data.data.nextPage, sort, perPage, direction);
    }
  }
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$f
} = constants;
const config$f = {
  commandName: 'list',
  description: 'List repositories in an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    all: {
      type: 'boolean',
      default: false,
      description: 'By default view shows the last n repos. This flag allows you to fetch the entire list. Will ignore --page and --perPage.'
    },
    direction: {
      type: 'string',
      default: 'desc',
      description: 'Direction option'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    perPage: {
      type: 'number',
      shortFlag: 'pp',
      default: 30,
      description: 'Number of results per page'
    },
    page: {
      type: 'number',
      shortFlag: 'p',
      default: 1,
      description: 'Page number'
    },
    sort: {
      type: 'string',
      shortFlag: 's',
      default: 'created_at',
      description: 'Sorting option'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: repo:list

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} --json
  `
};
const cmdRepositoryList = {
  description: config$f.description,
  hidden: config$f.hidden,
  run: run$f
};
async function run$f(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$f,
    importMeta,
    parentName
  });
  const {
    all,
    direction = 'desc',
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    nook: true,
    test: direction === 'asc' || direction === 'desc',
    message: 'The --direction value must be "asc" or "desc"',
    fail: 'unexpected value'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$f);
    return;
  }
  await handleListRepos({
    all: Boolean(all),
    direction: direction === 'asc' ? 'asc' : 'desc',
    orgSlug,
    outputKind,
    page: Number(cli.flags['page']) || 1,
    perPage: Number(cli.flags['perPage']) || 30,
    sort: String(cli.flags['sort'] || 'created_at')
  });
}

async function fetchUpdateRepo(config, options) {
  const {
    defaultBranch,
    description,
    homepage,
    orgSlug,
    repoName,
    visibility
  } = {
    __proto__: null,
    ...config
  };
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.updateOrgRepo(orgSlug, repoName, {
    default_branch: defaultBranch,
    description,
    homepage,
    name: repoName,
    orgSlug,
    visibility
  }), {
    desc: 'to update a repository'
  });
}

async function outputUpdateRepo(result, repoName, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.success(`Repository \`${repoName}\` updated successfully`);
}

async function handleUpdateRepo({
  defaultBranch,
  description,
  homepage,
  orgSlug,
  repoName,
  visibility
}, outputKind) {
  const data = await fetchUpdateRepo({
    defaultBranch,
    description,
    homepage,
    orgSlug,
    repoName,
    visibility
  });
  await outputUpdateRepo(data, repoName, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$e
} = constants;
const config$e = {
  commandName: 'update',
  description: 'Update a repository in an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    defaultBranch: {
      type: 'string',
      shortFlag: 'b',
      default: 'main',
      description: 'Repository default branch'
    },
    homepage: {
      type: 'string',
      shortFlag: 'h',
      default: '',
      description: 'Repository url'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    repoDescription: {
      type: 'string',
      shortFlag: 'd',
      default: '',
      description: 'Repository description'
    },
    visibility: {
      type: 'string',
      shortFlag: 'v',
      default: 'private',
      description: 'Repository visibility (Default Private)'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <REPO>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: repo:update

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} test-repo
      $ ${command} test-repo --homepage https://example.com
  `
};
const cmdRepositoryUpdate = {
  description: config$e.description,
  hidden: config$e.hidden,
  run: run$e
};
async function run$e(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$e,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['repoName'];
  const [repoName = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    test: !!repoName,
    message: 'Repository name as first argument',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$e);
    return;
  }
  await handleUpdateRepo({
    orgSlug,
    repoName: String(repoName),
    description: String(cli.flags['repoDescription'] || ''),
    homepage: String(cli.flags['homepage'] || ''),
    defaultBranch: String(cli.flags['defaultBranch'] || ''),
    visibility: String(cli.flags['visibility'] || 'private')
  }, outputKind);
}

async function fetchViewRepo(orgSlug, repoName, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgRepo(orgSlug, repoName), {
    desc: 'repository data'
  });
}

// @ts-ignore
async function outputViewRepo(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const options = {
    columns: [{
      field: 'id',
      name: vendor.yoctocolorsCjsExports.magenta('ID')
    }, {
      field: 'name',
      name: vendor.yoctocolorsCjsExports.magenta('Name')
    }, {
      field: 'visibility',
      name: vendor.yoctocolorsCjsExports.magenta('Visibility')
    }, {
      field: 'default_branch',
      name: vendor.yoctocolorsCjsExports.magenta('Default branch')
    }, {
      field: 'homepage',
      name: vendor.yoctocolorsCjsExports.magenta('Homepage')
    }, {
      field: 'archived',
      name: vendor.yoctocolorsCjsExports.magenta('Archived')
    }, {
      field: 'created_at',
      name: vendor.yoctocolorsCjsExports.magenta('Created at')
    }]
  };
  logger.logger.log(vendor.srcExports(options, [result.data]));
}

async function handleViewRepo(orgSlug, repoName, outputKind) {
  const data = await fetchViewRepo(orgSlug, repoName);
  await outputViewRepo(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$d
} = constants;
const config$d = {
  commandName: 'view',
  description: 'View repositories in an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <REPO>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: repo:list

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} test-repo
      $ ${command} test-repo --json
  `
};
const cmdRepositoryView = {
  description: config$d.description,
  hidden: config$d.hidden,
  run: run$d
};
async function run$d(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$d,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['repoName'];
  const [repoName = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    test: !!repoName,
    message: 'Repository name as first argument',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$d);
    return;
  }
  await handleViewRepo(orgSlug, String(repoName), outputKind);
}

const description$2 = 'Manage registered repositories';
const cmdRepository = {
  description: description$2,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      create: cmdRepositoryCreate,
      view: cmdRepositoryView,
      list: cmdRepositoryList,
      del: cmdRepositoryDel,
      update: cmdRepositoryUpdate
    }, {
      argv,
      description: description$2,
      importMeta,
      name: `${parentName} repository`
    });
  }
};

const reachabilityFlags = {
  reachAnalysisMemoryLimit: {
    type: 'number',
    default: 8192,
    description: 'The maximum memory in MB to use for the reachability analysis. The default is 8192MB.'
  },
  reachAnalysisTimeout: {
    type: 'number',
    default: 0,
    description: 'Set timeout for the reachability analysis. Split analysis runs may cause the total scan time to exceed this timeout significantly.'
  },
  reachDisableAnalytics: {
    type: 'boolean',
    default: false,
    description: 'Disable reachability analytics sharing with Socket. Also disables caching-based optimizations.'
  },
  reachEcosystems: {
    type: 'string',
    isMultiple: true,
    description: 'List of ecosystems to conduct reachability analysis on, as either a comma separated value or as multiple flags. Defaults to all ecosystems.'
  },
  reachExcludePaths: {
    type: 'string',
    isMultiple: true,
    description: 'List of paths to exclude from reachability analysis, as either a comma separated value or as multiple flags.'
  }
};

async function suggestTarget() {
  // We could prefill this with sub-dirs of the current
  // dir ... but is that going to be useful?
  const proceed = await prompts.select({
    message: 'No TARGET given. Do you want to use the current directory?',
    choices: [{
      name: 'Yes',
      value: true,
      description: 'Target the current directory'
    }, {
      name: 'No',
      value: false,
      description: 'Do not use the current directory (this will end in a no-op)'
    }]
  });
  return proceed ? ['.'] : [];
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$c,
  SOCKET_DEFAULT_BRANCH: SOCKET_DEFAULT_BRANCH$1,
  SOCKET_DEFAULT_REPOSITORY: SOCKET_DEFAULT_REPOSITORY$1
} = constants;
const generalFlags$1 = {
  ...flags.commonFlags,
  ...flags.outputFlags,
  autoManifest: {
    type: 'boolean',
    description: 'Run `socket manifest auto` before collecting manifest files. This is necessary for languages like Scala, Gradle, and Kotlin, See `socket manifest auto --help`.'
  },
  branch: {
    type: 'string',
    shortFlag: 'b',
    description: 'Branch name'
  },
  commitHash: {
    type: 'string',
    shortFlag: 'ch',
    default: '',
    description: 'Commit hash'
  },
  commitMessage: {
    type: 'string',
    shortFlag: 'm',
    default: '',
    description: 'Commit message'
  },
  committers: {
    type: 'string',
    shortFlag: 'c',
    default: '',
    description: 'Committers'
  },
  cwd: {
    type: 'string',
    description: 'working directory, defaults to process.cwd()'
  },
  defaultBranch: {
    type: 'boolean',
    default: false,
    description: 'Set the default branch of the repository to the branch of this full-scan. Should only need to be done once, for example for the "main" or "master" branch.'
  },
  interactive: {
    type: 'boolean',
    default: true,
    description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
  },
  pullRequest: {
    type: 'number',
    shortFlag: 'pr',
    description: 'Commit hash'
  },
  org: {
    type: 'string',
    description: 'Force override the organization slug, overrides the default org from config'
  },
  reach: {
    type: 'boolean',
    default: false,
    description: 'Run tier 1 full application reachability analysis'
  },
  readOnly: {
    type: 'boolean',
    default: false,
    description: 'Similar to --dry-run except it can read from remote, stops before it would create an actual report'
  },
  repo: {
    type: 'string',
    shortFlag: 'r',
    description: 'Repository name'
  },
  report: {
    type: 'boolean',
    description: 'Wait for the scan creation to complete, then basically run `socket scan report` on it'
  },
  setAsAlertsPage: {
    type: 'boolean',
    default: true,
    aliases: ['pendingHead'],
    description: 'When true and if this is the "default branch" then this Scan will be the one reflected on your alerts page. See help for details. Defaults to true.'
  },
  tmp: {
    type: 'boolean',
    shortFlag: 't',
    default: false,
    description: 'Set the visibility (true/false) of the scan in your dashboard.'
  }
};
const config$c = {
  commandName: 'create',
  description: 'Create a new Socket scan and report',
  hidden: false,
  flags: {
    ...generalFlags$1,
    ...reachabilityFlags
  },
  // TODO: Your project's "socket.yml" file's "projectIgnorePaths".
  help: command => `
    Usage
      $ ${command} [options] [TARGET...]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:create

    Options
      ${utils.getFlagListOutput(generalFlags$1)}

    Reachability Options (when --reach is used)
      ${utils.getFlagListOutput(reachabilityFlags)}

    Uploads the specified dependency manifest files for Go, Gradle, JavaScript,
    Kotlin, Python, and Scala. Files like "package.json" and "requirements.txt".
    If any folder is specified, the ones found in there recursively are uploaded.

    Details on TARGET:

    - Defaults to the current dir (cwd) if none given
    - Multiple targets can be specified
    - If a target is a file, only that file is checked
    - If it is a dir, the dir is scanned for any supported manifest files
    - Dirs MUST be within the current dir (cwd), you can use --cwd to change it
    - Supports globbing such as "**/package.json", "**/requirements.txt", etc.
    - Ignores any file specified in your project's ".gitignore"
    - Also a sensible set of default ignores from the "ignore-by-default" module

    The --repo and --branch flags tell Socket to associate this Scan with that
    repo/branch. The names will show up on your dashboard on the Socket website.

    Note: for a first run you probably want to set --defaultBranch to indicate
          the default branch name, like "main" or "master".

    The "alerts page" (https://socket.dev/dashboard/org/YOURORG/alerts) will show
    the results from the last scan designated as the "pending head" on the branch
    configured on Socket to be the "default branch". When creating a scan the
    --setAsAlertsPage flag will default to true to update this. You can prevent
    this by using --no-setAsAlertsPage. This flag is ignored for any branch that
    is not designated as the "default branch". It is disabled when using --tmp.

    You can use \`socket scan setup\` to configure certain repo flag defaults.

    Examples
      $ ${command}
      $ ${command} ./proj --json
      $ ${command} --repo=test-repo --branch=main ./package.json
  `
};
const cmdScanCreate = {
  description: config$c.description,
  hidden: config$c.hidden,
  run: run$c
};
async function run$c(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$c,
    importMeta,
    parentName
  });
  const {
    commitHash,
    commitMessage,
    committers,
    cwd: cwdOverride,
    defaultBranch,
    interactive = true,
    json,
    markdown,
    org: orgFlag,
    pullRequest,
    reach,
    reachAnalysisMemoryLimit,
    reachAnalysisTimeout,
    reachDisableAnalytics,
    readOnly,
    setAsAlertsPage: pendingHeadFlag,
    tmp
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];

  // Process comma-separated values for isMultiple flags.
  const reachEcosystemsRaw = utils.cmdFlagValueToArray(cli.flags['reachEcosystems']);
  const reachExcludePaths = utils.cmdFlagValueToArray(cli.flags['reachExcludePaths']);

  // Validate ecosystem values.
  const reachEcosystems = [];
  const validEcosystems = utils.getEcosystemChoicesForMeow();
  for (const ecosystem of reachEcosystemsRaw) {
    if (!validEcosystems.includes(ecosystem)) {
      throw new Error(`Invalid ecosystem: "${ecosystem}". Valid values are: ${arrays.joinAnd(validEcosystems)}`);
    }
    reachEcosystems.push(ecosystem);
  }
  let {
    autoManifest,
    branch: branchName,
    repo: repoName,
    report
  } = cli.flags;
  let [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const processCwd = process.cwd();
  const cwd = cwdOverride && cwdOverride !== processCwd ? path.resolve(processCwd, String(cwdOverride)) : processCwd;
  const sockJson = utils.readOrDefaultSocketJson(cwd);

  // Note: This needs meow booleanDefault=undefined.
  if (typeof autoManifest !== 'boolean') {
    if (sockJson.defaults?.scan?.create?.autoManifest !== undefined) {
      autoManifest = sockJson.defaults.scan.create.autoManifest;
      logger.logger.info('Using default --autoManifest from socket.json:', autoManifest);
    } else {
      autoManifest = false;
    }
  }
  if (!branchName) {
    if (sockJson.defaults?.scan?.create?.branch) {
      branchName = sockJson.defaults.scan.create.branch;
      logger.logger.info('Using default --branch from socket.json:', branchName);
    } else {
      branchName = (await utils.gitBranch(cwd)) || SOCKET_DEFAULT_BRANCH$1;
    }
  }
  if (!repoName) {
    if (sockJson.defaults?.scan?.create?.repo) {
      repoName = sockJson.defaults.scan.create.repo;
      logger.logger.info('Using default --repo from socket.json:', repoName);
    } else {
      repoName = (await utils.getRepoName(cwd)) || SOCKET_DEFAULT_REPOSITORY$1;
    }
  }
  if (typeof report !== 'boolean') {
    if (sockJson.defaults?.scan?.create?.report !== undefined) {
      report = sockJson.defaults.scan.create.report;
      logger.logger.info('Using default --report from socket.json:', report);
    } else {
      report = false;
    }
  }

  // If we updated any inputs then we should print the command line to repeat
  // the command without requiring user input, as a suggestion.
  let updatedInput = false;

  // Accept zero or more paths. Default to cwd() if none given.
  let targets = cli.input || [cwd];
  if (!targets.length && !dryRun && interactive) {
    targets = await suggestTarget();
    updatedInput = true;
  }

  // We're going to need an api token to suggest data because those suggestions
  // must come from data we already know. Don't error on missing api token yet.
  // If the api-token is not set, ignore it for the sake of suggestions.
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const pendingHead = tmp ? false : pendingHeadFlag;

  // If the current cwd is unknown and is used as a repo slug anyways, we will
  // first need to register the slug before we can use it.
  // Only do suggestions with an apiToken and when not in dryRun mode
  if (hasApiToken && !dryRun && interactive) {
    if (!orgSlug) {
      const suggestion = await utils.suggestOrgSlug();
      if (suggestion === undefined) {
        await outputCreateNewScan({
          ok: false,
          message: 'Canceled by user',
          cause: 'Org selector was canceled by user'
        }, {
          interactive: false,
          outputKind
        });
        return;
      }
      if (suggestion) {
        orgSlug = suggestion;
      }
      updatedInput = true;
    }
  }
  const detected = await detectManifestActions(sockJson, cwd);
  if (detected.count > 0 && !autoManifest) {
    logger.logger.info(`Detected ${detected.count} manifest targets we could try to generate. Please set the --autoManifest flag if you want to include languages covered by \`socket manifest auto\` in the Scan.`);
  }
  if (updatedInput && orgSlug && targets.length) {
    logger.logger.info('Note: You can invoke this command next time to skip the interactive questions:');
    logger.logger.error('```');
    logger.logger.error(`    socket scan create [other flags...] ${orgSlug} ${targets.join(' ')}`);
    logger.logger.error('```');
    logger.logger.error('');
    logger.logger.info('You can also run `socket scan setup` to persist these flag defaults to a socket.json file.');
    logger.logger.error('');
  }
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    test: !!targets.length,
    message: 'At least one TARGET (e.g. `.` or `./package.json`)',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    nook: true,
    test: !pendingHead || !!branchName,
    message: 'When --pendingHead is set, --branch is mandatory',
    fail: 'missing branch name'
  }, {
    nook: true,
    test: !defaultBranch || !!branchName,
    message: 'When --defaultBranch is set, --branch is mandatory',
    fail: 'missing branch name'
  }, {
    nook: true,
    test: reach || reachDisableAnalytics === reachabilityFlags['reachDisableAnalytics']?.default,
    message: 'The --reachDisableAnalytics flag requires --reach to be set',
    fail: 'missing --reach flag'
  }, {
    nook: true,
    test: reach || reachAnalysisMemoryLimit === reachabilityFlags['reachAnalysisMemoryLimit']?.default,
    message: 'The --reachAnalysisMemoryLimit flag requires --reach to be set',
    fail: 'missing --reach flag'
  }, {
    nook: true,
    test: reach || reachAnalysisTimeout === reachabilityFlags['reachAnalysisTimeout']?.default,
    message: 'The --reachAnalysisTimeout flag requires --reach to be set',
    fail: 'missing --reach flag'
  }, {
    nook: true,
    test: reach || !reachEcosystems.length,
    message: 'The --reachEcosystems flag requires --reach to be set',
    fail: 'missing --reach flag'
  }, {
    nook: true,
    test: reach || !reachExcludePaths.length,
    message: 'The --reachExcludePaths flag requires --reach to be set',
    fail: 'missing --reach flag'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$c);
    return;
  }
  await handleCreateNewScan({
    autoManifest: Boolean(autoManifest),
    branchName: branchName,
    commitHash: commitHash && String(commitHash) || '',
    commitMessage: commitMessage && String(commitMessage) || '',
    committers: committers && String(committers) || '',
    cwd,
    defaultBranch: Boolean(defaultBranch),
    interactive: Boolean(interactive),
    orgSlug,
    outputKind,
    pendingHead: Boolean(pendingHead),
    pullRequest: Number(pullRequest),
    reach: {
      runReachabilityAnalysis: Boolean(reach),
      reachDisableAnalytics: Boolean(reachDisableAnalytics),
      reachAnalysisTimeout: Number(reachAnalysisTimeout),
      reachAnalysisMemoryLimit: Number(reachAnalysisMemoryLimit),
      reachEcosystems,
      reachExcludePaths
    },
    readOnly: Boolean(readOnly),
    repoName,
    report,
    targets,
    tmp: Boolean(tmp)
  });
}

async function fetchDeleteOrgFullScan(orgSlug, scanId, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.deleteOrgFullScan(orgSlug, scanId), {
    desc: 'to delete a scan'
  });
}

async function outputDeleteScan(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.success('Scan deleted successfully');
}

async function handleDeleteScan(orgSlug, scanId, outputKind) {
  const data = await fetchDeleteOrgFullScan(orgSlug, scanId);
  await outputDeleteScan(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$b
} = constants;
const config$b = {
  commandName: 'del',
  description: 'Delete a scan',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <SCAN_ID>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:delete

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0 --json
  `
};
const cmdScanDel = {
  description: config$b.description,
  hidden: config$b.hidden,
  run: run$b
};
async function run$b(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$b,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const [scanId = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug, defaultOrgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: !!defaultOrgSlug,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    test: !!scanId,
    message: 'Scan ID to delete',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$b);
    return;
  }
  await handleDeleteScan(orgSlug, scanId, outputKind);
}

async function fetchDiffScan({
  id1,
  id2,
  orgSlug
}) {
  logger.logger.info('Scan ID 1:', id1);
  logger.logger.info('Scan ID 2:', id2);
  logger.logger.info('Note: this request may take some time if the scans are big');
  return await utils.queryApiSafeJson(`orgs/${orgSlug}/full-scans/diff?before=${encodeURIComponent(id1)}&after=${encodeURIComponent(id2)}`, 'a scan diff');
}

const {
  SOCKET_WEBSITE_URL: SOCKET_WEBSITE_URL$2
} = constants;
const SOCKET_SBOM_URL_PREFIX$1 = `${SOCKET_WEBSITE_URL$2}/dashboard/org/SocketDev/sbom/`;
async function outputDiffScan(result, {
  depth,
  file,
  outputKind
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(result));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const dashboardUrl = result.data.diff_report_url;
  const dashboardMessage = dashboardUrl ? `\n View this diff scan in the Socket dashboard: ${vendor.yoctocolorsCjsExports.cyan(dashboardUrl)}` : '';

  // When forcing json, or dumping to file, serialize to string such that it
  // won't get truncated. The only way to dump the full raw JSON to stdout is
  // to use `--json --file -` (the dash is a standard notation for stdout)
  if (outputKind === 'json' || file) {
    await handleJson(result, file, dashboardMessage);
    return;
  }
  if (outputKind === 'markdown') {
    await handleMarkdown(result.data);
    return;
  }

  // In this case neither the --json nor the --file flag was passed
  // Dump the JSON to CLI and let NodeJS deal with truncation

  logger.logger.log('Diff scan result:');
  logger.logger.log(require$$1.inspect(result.data, {
    showHidden: false,
    depth: depth > 0 ? depth : null,
    colors: true,
    maxArrayLength: null
  }));
  logger.logger.info(`\n 📝 To display the detailed report in the terminal, use the --json flag. For a friendlier report, use the --markdown flag.\n`);
  logger.logger.info(dashboardMessage);
}
async function handleJson(data, file, dashboardMessage) {
  const json = utils.serializeResultJson(data);
  if (file && file !== '-') {
    logger.logger.log(`Writing json to \`${file}\``);
    fs$1.writeFile(file, json, err => {
      if (err) {
        logger.logger.fail(`Writing to \`${file}\` failed...`);
        logger.logger.error(err);
      } else {
        logger.logger.success(`Data successfully written to \`${file}\``);
      }
      logger.logger.error(dashboardMessage);
    });
  } else {
    // only .log goes to stdout
    logger.logger.info(`\n Diff scan result: \n`);
    logger.logger.log(json);
    logger.logger.info(dashboardMessage);
  }
}
async function handleMarkdown(data) {
  logger.logger.log('# Scan diff result');
  logger.logger.log('');
  logger.logger.log('This Socket.dev report shows the changes between two scans:');
  logger.logger.log(`- [${data.before.id}](${SOCKET_SBOM_URL_PREFIX$1}${data.before.id})`);
  logger.logger.log(`- [${data.after.id}](${SOCKET_SBOM_URL_PREFIX$1}${data.after.id})`);
  logger.logger.log('');
  logger.logger.log(`You can [view this report in your dashboard](${data.diff_report_url})`);
  logger.logger.log('');
  logger.logger.log('## Changes');
  logger.logger.log('');
  logger.logger.log(`- directDependenciesChanged: ${data.directDependenciesChanged}`);
  logger.logger.log(`- Added packages: ${data.artifacts.added.length}`);
  if (data.artifacts.added.length > 0) {
    data.artifacts.added.slice(0, 10).forEach(artifact => {
      logger.logger.log(`  - ${artifact.type} ${artifact.name}@${artifact.version}`);
    });
    if (data.artifacts.added.length > 10) {
      logger.logger.log(`  ... and ${data.artifacts.added.length - 10} more`);
    }
  }
  logger.logger.log(`- Removed packages: ${data.artifacts.removed.length}`);
  if (data.artifacts.removed.length > 0) {
    data.artifacts.removed.slice(0, 10).forEach(artifact => {
      logger.logger.log(`  - ${artifact.type} ${artifact.name}@${artifact.version}`);
    });
    if (data.artifacts.removed.length > 10) {
      logger.logger.log(`  ... and ${data.artifacts.removed.length - 10} more`);
    }
  }
  logger.logger.log(`- Replaced packages: ${data.artifacts.replaced.length}`);
  if (data.artifacts.replaced.length > 0) {
    data.artifacts.replaced.slice(0, 10).forEach(artifact => {
      logger.logger.log(`  - ${artifact.type} ${artifact.name}@${artifact.version}`);
    });
    if (data.artifacts.replaced.length > 10) {
      logger.logger.log(`  ... and ${data.artifacts.replaced.length - 10} more`);
    }
  }
  logger.logger.log(`- Updated packages: ${data.artifacts.updated.length}`);
  if (data.artifacts.updated.length > 0) {
    data.artifacts.updated.slice(0, 10).forEach(artifact => {
      logger.logger.log(`  - ${artifact.type} ${artifact.name}@${artifact.version}`);
    });
    if (data.artifacts.updated.length > 10) {
      logger.logger.log(`  ... and ${data.artifacts.updated.length - 10} more`);
    }
  }
  logger.logger.log(`- Unchanged packages: ${data.artifacts.unchanged.length}`);
  if (data.artifacts.unchanged.length > 0) {
    data.artifacts.unchanged.slice(0, 10).forEach(artifact => {
      logger.logger.log(`  - ${artifact.type} ${artifact.name}@${artifact.version}`);
    });
    if (data.artifacts.unchanged.length > 10) {
      logger.logger.log(`  ... and ${data.artifacts.unchanged.length - 10} more`);
    }
  }
  logger.logger.log('');
  logger.logger.log(`## Scan ${data.before.id}`);
  logger.logger.log('');
  logger.logger.log('This Scan was considered to be the "base" / "from" / "before" Scan.');
  logger.logger.log('');
  for (const [key, value] of Object.entries(data.before)) {
    if (key === 'pull_request' && !value) {
      continue;
    }
    if (!['id', 'organization_id', 'repository_id'].includes(key)) {
      logger.logger.group(`- ${key === 'repository_slug' ? 'repo' : key === 'organization_slug' ? 'org' : key}: ${value}`);
      logger.logger.groupEnd();
    }
  }
  logger.logger.log('');
  logger.logger.log(`## Scan ${data.after.id}`);
  logger.logger.log('');
  logger.logger.log('This Scan was considered to be the "head" / "to" / "after" Scan.');
  logger.logger.log('');
  for (const [key, value] of Object.entries(data.after)) {
    if (key === 'pull_request' && !value) {
      continue;
    }
    if (!['id', 'organization_id', 'repository_id'].includes(key)) {
      logger.logger.group(`- ${key === 'repository_slug' ? 'repo' : key === 'organization_slug' ? 'org' : key}: ${value}`);
      logger.logger.groupEnd();
    }
  }
  logger.logger.log('');
}

async function handleDiffScan({
  depth,
  file,
  id1,
  id2,
  orgSlug,
  outputKind
}) {
  const data = await fetchDiffScan({
    id1,
    id2,
    orgSlug
  });
  await outputDiffScan(data, {
    depth,
    file,
    outputKind
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$a,
  SOCKET_WEBSITE_URL: SOCKET_WEBSITE_URL$1
} = constants;
const SOCKET_SBOM_URL_PREFIX = `${SOCKET_WEBSITE_URL$1}/dashboard/org/SocketDev/sbom/`;
const {
  length: SOCKET_SBOM_URL_PREFIX_LENGTH
} = SOCKET_SBOM_URL_PREFIX;
const config$a = {
  commandName: 'diff',
  description: 'See what changed between two Scans',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    depth: {
      type: 'number',
      default: 2,
      description: 'Max depth of JSON to display before truncating, use zero for no limit (without --json/--file)'
    },
    file: {
      type: 'string',
      shortFlag: 'f',
      default: '',
      description: 'Path to a local file where the output should be saved. Use `-` to force stdout.'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <SCAN_ID1> <SCAN_ID2>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:list

    This command displays the package changes between two scans. The full output
    can be pretty large depending on the size of your repo and time range. It is
    best stored to disk (with --json) to be further analyzed by other tools.

    Note: While it will work in any order, the first Scan ID is assumed to be the
          older ID, even if it is a newer Scan. This is only relevant for the
          added/removed list (similar to diffing two files with git).

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} aaa0aa0a-aaaa-0000-0a0a-0000000a00a0 aaa1aa1a-aaaa-1111-1a1a-1111111a11a1
      $ ${command} aaa0aa0a-aaaa-0000-0a0a-0000000a00a0 aaa1aa1a-aaaa-1111-1a1a-1111111a11a1 --json
  `
};
const cmdScanDiff = {
  description: config$a.description,
  hidden: config$a.hidden,
  run: run$a
};
async function run$a(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$a,
    importMeta,
    parentName
  });
  const {
    depth,
    file,
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  let [id1 = '', id2 = ''] = cli.input;
  // Support dropping in full socket urls to an sbom.
  if (id1.startsWith(SOCKET_SBOM_URL_PREFIX)) {
    id1 = id1.slice(SOCKET_SBOM_URL_PREFIX_LENGTH);
  }
  if (id2.startsWith(SOCKET_SBOM_URL_PREFIX)) {
    id2 = id2.slice(SOCKET_SBOM_URL_PREFIX_LENGTH);
  }
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: !!(id1 && id2),
    message: 'Specify two Scan IDs.\nA Scan ID looks like `aaa0aa0a-aaaa-0000-0a0a-0000000a00a0`.',
    fail: !id1 && !id2 ? 'missing both Scan IDs' : !id2 ? 'missing second Scan ID' : 'missing first Scan ID' // Not sure how this can happen but ok.
  }, {
    test: !!orgSlug,
    nook: true,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$a);
    return;
  }
  await handleDiffScan({
    id1: String(id1 || ''),
    id2: String(id2 || ''),
    depth: Number(depth),
    orgSlug,
    outputKind,
    file: String(file || '')
  });
}

async function createScanFromGithub({
  all,
  githubApiUrl,
  githubToken,
  interactive,
  orgGithub,
  orgSlug,
  outputKind,
  repos
}) {
  let targetRepos = repos.trim().split(',').map(r => r.trim()).filter(Boolean);
  if (all || targetRepos.length === 0) {
    // Fetch from Socket API
    const result = await fetchListAllRepos(orgSlug, {
      direction: 'asc',
      sort: 'name'
    });
    if (!result.ok) {
      return result;
    }
    targetRepos = result.data.results.map(obj => obj.slug || '');
  }
  targetRepos = targetRepos.map(s => s.trim()).filter(Boolean);
  logger.logger.info(`Have ${targetRepos.length} repo names to Scan!`);
  logger.logger.log('');
  if (!targetRepos.filter(Boolean).length) {
    return {
      ok: false,
      message: 'No repo found',
      cause: 'You did not set the --repos value and/or the server responded with zero repos when asked for some. Unable to proceed.'
    };
  }

  // Non-interactive or explicitly requested; just do it.
  if (interactive && targetRepos.length > 1 && !all && !repos) {
    const which = await selectFocus(targetRepos);
    if (!which.ok) {
      return which;
    }
    targetRepos = which.data;
  }

  // 10 is an arbitrary number. Maybe confirm whenever count>1 ?
  // Do not ask to confirm when the list was given explicit.
  if (interactive && (all || !repos) && targetRepos.length > 10) {
    const sure = await makeSure(targetRepos.length);
    if (!sure.ok) {
      return sure;
    }
  }
  let scansCreated = 0;
  for (const repoSlug of targetRepos) {
    // eslint-disable-next-line no-await-in-loop
    const scanCResult = await scanRepo(repoSlug, {
      githubApiUrl,
      githubToken,
      orgSlug,
      orgGithub,
      outputKind,
      repos
    });
    if (scanCResult.ok) {
      const {
        scanCreated
      } = scanCResult.data;
      if (scanCreated) {
        scansCreated += 1;
      }
    }
  }
  logger.logger.success(targetRepos.length, 'GitHub repos detected');
  logger.logger.success(scansCreated, 'with supported Manifest files');
  return {
    ok: true,
    data: undefined
  };
}
async function scanRepo(repoSlug, {
  githubApiUrl,
  githubToken,
  orgGithub,
  orgSlug,
  outputKind,
  repos
}) {
  logger.logger.info(`Requesting repo details from GitHub API for: \`${orgGithub}/${repoSlug}\`...`);
  logger.logger.group();
  const result = await scanOneRepo(repoSlug, {
    githubApiUrl,
    githubToken,
    orgSlug,
    orgGithub,
    outputKind});
  logger.logger.groupEnd();
  logger.logger.log('');
  return result;
}
async function scanOneRepo(repoSlug, {
  githubApiUrl,
  githubToken,
  orgGithub,
  orgSlug,
  outputKind
}) {
  const repoResult = await getRepoDetails({
    orgGithub,
    repoSlug,
    githubApiUrl,
    githubToken
  });
  if (!repoResult.ok) {
    return repoResult;
  }
  const {
    defaultBranch,
    repoApiUrl
  } = repoResult.data;
  logger.logger.info(`Default branch: \`${defaultBranch}\``);
  const treeResult = await getRepoBranchTree({
    defaultBranch,
    githubToken,
    orgGithub,
    repoSlug,
    repoApiUrl
  });
  if (!treeResult.ok) {
    return treeResult;
  }
  const files = treeResult.data;
  if (!files.length) {
    logger.logger.warn('No files were reported for the default branch. Moving on to next repo.');
    return {
      ok: true,
      data: {
        scanCreated: false
      }
    };
  }
  const tmpDir = fs$1.mkdtempSync(path.join(os.tmpdir(), repoSlug));
  require$$6.debugFn('notice', 'init: temp dir for scan root', tmpDir);
  const downloadResult = await testAndDownloadManifestFiles({
    files,
    tmpDir,
    repoSlug,
    defaultBranch,
    orgGithub,
    repoApiUrl,
    githubToken
  });
  if (!downloadResult.ok) {
    return downloadResult;
  }
  const commitResult = await getLastCommitDetails({
    orgGithub,
    repoSlug,
    defaultBranch,
    repoApiUrl,
    githubToken
  });
  if (!commitResult.ok) {
    return commitResult;
  }
  const {
    lastCommitMessage,
    lastCommitSha,
    lastCommitter
  } = commitResult.data;

  // Make request for full scan
  // I think we can just kick off the socket scan create command now...

  await handleCreateNewScan({
    autoManifest: false,
    branchName: defaultBranch,
    commitHash: lastCommitSha,
    commitMessage: lastCommitMessage || '',
    committers: lastCommitter || '',
    cwd: tmpDir,
    defaultBranch: true,
    interactive: false,
    orgSlug,
    outputKind,
    pendingHead: true,
    pullRequest: 0,
    reach: {
      runReachabilityAnalysis: false,
      reachDisableAnalytics: false,
      reachAnalysisTimeout: 0,
      reachAnalysisMemoryLimit: 0,
      reachEcosystems: [],
      reachExcludePaths: []
    },
    readOnly: false,
    repoName: repoSlug,
    report: false,
    targets: ['.'],
    tmp: false
  });
  return {
    ok: true,
    data: {
      scanCreated: true
    }
  };
}
async function testAndDownloadManifestFiles({
  defaultBranch,
  files,
  githubToken,
  orgGithub,
  repoApiUrl,
  repoSlug,
  tmpDir
}) {
  logger.logger.info(`File tree for ${defaultBranch} contains`, files.length, `entries. Searching for supported manifest files...`);
  logger.logger.group();
  let fileCount = 0;
  let firstFailureResult;
  for (const file of files) {
    // eslint-disable-next-line no-await-in-loop
    const result = await testAndDownloadManifestFile({
      file,
      tmpDir,
      defaultBranch,
      repoApiUrl,
      githubToken
    });
    if (result.ok) {
      if (result.data.isManifest) {
        fileCount += 1;
      }
    } else if (!firstFailureResult) {
      firstFailureResult = result;
    }
  }
  logger.logger.groupEnd();
  logger.logger.info('Found and downloaded', fileCount, 'manifest files');
  if (!fileCount) {
    if (firstFailureResult) {
      logger.logger.fail('While no supported manifest files were downloaded, at least one error encountered trying to do so. Showing the first error.');
      return firstFailureResult;
    }
    return {
      ok: false,
      message: 'No manifest files found',
      cause: `No supported manifest files were found in the latest commit on the branch ${defaultBranch} for repo ${orgGithub}/${repoSlug}. Skipping full scan.`
    };
  }
  return {
    ok: true,
    data: undefined
  };
}
async function testAndDownloadManifestFile({
  defaultBranch,
  file,
  githubToken,
  repoApiUrl,
  tmpDir
}) {
  require$$6.debugFn('notice', 'testing: file', file);
  const supportedFilesCResult = await fetchSupportedScanFileNames();
  const supportedFiles = supportedFilesCResult.ok ? supportedFilesCResult.data : undefined;
  if (!supportedFiles || !utils.isReportSupportedFile(file, supportedFiles)) {
    require$$6.debugFn('notice', '  - skip: not a known pattern');
    // Not an error.
    return {
      ok: true,
      data: {
        isManifest: false
      }
    };
  }
  require$$6.debugFn('notice', 'found: manifest file, going to attempt to download it;', file);
  const result = await downloadManifestFile({
    file,
    tmpDir,
    defaultBranch,
    repoApiUrl,
    githubToken
  });
  return result.ok ? {
    ok: true,
    data: {
      isManifest: true
    }
  } : result;
}
async function downloadManifestFile({
  defaultBranch,
  file,
  githubToken,
  repoApiUrl,
  tmpDir
}) {
  require$$6.debugFn('notice', 'request: download url from GitHub');
  const fileUrl = `${repoApiUrl}/contents/${file}?ref=${defaultBranch}`;
  require$$6.debugDir('inspect', {
    fileUrl
  });
  const downloadUrlResponse = await fetch(fileUrl, {
    method: 'GET',
    headers: {
      Authorization: `Bearer ${githubToken}`
    }
  });
  require$$6.debugFn('notice', 'complete: request');
  const downloadUrlText = await downloadUrlResponse.text();
  require$$6.debugFn('inspect', 'response: raw download url', downloadUrlText);
  let downloadUrl;
  try {
    downloadUrl = JSON.parse(downloadUrlText).download_url;
  } catch {
    logger.logger.fail(`GitHub response contained invalid JSON for download url for: ${file}`);
    return {
      ok: false,
      message: 'Invalid JSON response',
      cause: `Server responded with invalid JSON for download url ${downloadUrl}`
    };
  }
  const localPath = path.join(tmpDir, file);
  require$$6.debugFn('notice', 'download: manifest file started', downloadUrl, '->', localPath);

  // Now stream the file to that file...
  const result = await streamDownloadWithFetch(localPath, downloadUrl);
  if (!result.ok) {
    // Do we proceed? Bail? Hrm...
    logger.logger.fail(`Failed to download manifest file, skipping to next file. File: ${file}`);
    return result;
  }
  require$$6.debugFn('notice', 'download: manifest file completed');
  return {
    ok: true,
    data: undefined
  };
}

// Courtesy of gemini:
async function streamDownloadWithFetch(localPath, downloadUrl) {
  let response; // Declare response here to access it in catch if needed

  try {
    response = await fetch(downloadUrl);
    if (!response.ok) {
      const errorMsg = `Download failed due to bad server response: ${response.status} ${response.statusText} for ${downloadUrl}`;
      logger.logger.fail(errorMsg);
      return {
        ok: false,
        message: 'Download Failed',
        cause: errorMsg
      };
    }
    if (!response.body) {
      logger.logger.fail(`Download failed because the server response was empty, for ${downloadUrl}`);
      return {
        ok: false,
        message: 'Download Failed',
        cause: 'Response body is null or undefined.'
      };
    }

    // Make sure the dir exists. It may be nested and we need to construct that
    // before starting the download.
    const dir = path.dirname(localPath);
    if (!fs$1.existsSync(dir)) {
      fs$1.mkdirSync(dir, {
        recursive: true
      });
    }
    const fileStream = fs$1.createWriteStream(localPath);

    // Using stream.pipeline for better error handling and cleanup

    await promises.pipeline(response.body, fileStream);
    // 'pipeline' will automatically handle closing streams and propagating errors.
    // It resolves when the piping is fully complete and fileStream is closed.
    return {
      ok: true,
      data: localPath
    };
  } catch (error) {
    logger.logger.fail('An error was thrown while trying to download a manifest file... url:', downloadUrl);
    require$$6.debugDir('inspect', {
      error
    });

    // If an error occurs and fileStream was created, attempt to clean up.
    if (fs$1.existsSync(localPath)) {
      // Check if fileStream was even opened before trying to delete
      // This check might be too simplistic depending on when error occurs
      fs$1.unlink(localPath, unlinkErr => {
        if (unlinkErr) {
          logger.logger.fail(`Error deleting partial file ${localPath}: ${unlinkErr.message}`);
        }
      });
    }
    // Construct a more informative error message
    let detailedError = `Error during download of ${downloadUrl}: ${error.message}`;
    if (error.cause) {
      // Include cause if available (e.g., from network errors)
      detailedError += `\nCause: ${error.cause}`;
    }
    if (response && !response.ok) {
      // If error was due to bad HTTP status
      detailedError += ` (HTTP Status: ${response.status} ${response.statusText})`;
    }
    require$$6.debugFn('error', detailedError);
    return {
      ok: false,
      message: 'Download Failed',
      cause: detailedError
    };
  }
}
async function getLastCommitDetails({
  defaultBranch,
  githubToken,
  orgGithub,
  repoApiUrl,
  repoSlug
}) {
  logger.logger.info(`Requesting last commit for default branch ${defaultBranch} for ${orgGithub}/${repoSlug}...`);
  const commitApiUrl = `${repoApiUrl}/commits?sha=${defaultBranch}&per_page=1`;
  require$$6.debugFn('inspect', 'url: commit', commitApiUrl);
  const commitResponse = await fetch(commitApiUrl, {
    headers: {
      Authorization: `Bearer ${githubToken}`
    }
  });
  const commitText = await commitResponse.text();
  require$$6.debugFn('inspect', 'response: commit', commitText);
  let lastCommit;
  try {
    lastCommit = JSON.parse(commitText)?.[0];
  } catch {
    logger.logger.fail(`GitHub response contained invalid JSON for last commit`);
    logger.logger.error(commitText);
    return {
      ok: false,
      message: 'Invalid JSON response',
      cause: `Server responded with invalid JSON for last commit of repo ${repoSlug}`
    };
  }
  const lastCommitSha = lastCommit.sha;
  const lastCommitter = Array.from(new Set([lastCommit.commit.author.name, lastCommit.commit.committer.name]))[0];
  const lastCommitMessage = lastCommit.message;
  if (!lastCommitSha) {
    return {
      ok: false,
      message: 'Missing commit SHA',
      cause: 'Unable to get last commit for repo'
    };
  }
  if (!lastCommitter) {
    return {
      ok: false,
      message: 'Missing committer',
      cause: 'Last commit does not have information about who made the commit'
    };
  }
  return {
    ok: true,
    data: {
      lastCommitSha,
      lastCommitter,
      lastCommitMessage
    }
  };
}
async function selectFocus(repos) {
  const proceed = await prompts.select({
    message: 'Please select the repo to process:',
    choices: repos.map(slug => ({
      name: slug,
      value: slug,
      description: `Create scan for the ${slug} repo through GitHub`
    })).concat({
      name: '(Exit)',
      value: '',
      description: 'Cancel this action and exit'
    })
  });
  if (!proceed) {
    return {
      ok: false,
      message: 'Canceled by user',
      cause: 'User chose to cancel the action'
    };
  }
  return {
    ok: true,
    data: [proceed]
  };
}
async function makeSure(count) {
  if (!(await prompts.confirm({
    message: `Are you sure you want to run this for ${count} repos?`,
    default: false
  }))) {
    return {
      ok: false,
      message: 'User canceled',
      cause: 'Action canceled by user'
    };
  }
  return {
    ok: true,
    data: undefined
  };
}
async function getRepoDetails({
  githubApiUrl,
  githubToken,
  orgGithub,
  repoSlug
}) {
  const repoApiUrl = `${githubApiUrl}/repos/${orgGithub}/${repoSlug}`;
  require$$6.debugDir('inspect', {
    repoApiUrl
  });
  const repoDetailsResponse = await fetch(repoApiUrl, {
    method: 'GET',
    headers: {
      Authorization: `Bearer ${githubToken}`
    }
  });
  logger.logger.success(`Request completed.`);
  const repoDetailsText = await repoDetailsResponse.text();
  require$$6.debugFn('inspect', 'response: repo', repoDetailsText);
  let repoDetails;
  try {
    repoDetails = JSON.parse(repoDetailsText);
  } catch {
    logger.logger.fail(`GitHub response contained invalid JSON for repo ${repoSlug}`);
    logger.logger.error(repoDetailsText);
    return {
      ok: false,
      message: 'Invalid JSON response',
      cause: `Server responded with invalid JSON for repo ${repoSlug}`
    };
  }
  const defaultBranch = repoDetails.default_branch;
  if (!defaultBranch) {
    return {
      ok: false,
      message: 'Default Branch Not Found',
      cause: `Repo ${repoSlug} does not have a default branch set or it was not reported`
    };
  }
  return {
    ok: true,
    data: {
      defaultBranch,
      repoDetails,
      repoApiUrl
    }
  };
}
async function getRepoBranchTree({
  defaultBranch,
  githubToken,
  orgGithub,
  repoApiUrl,
  repoSlug
}) {
  logger.logger.info(`Requesting default branch file tree; branch \`${defaultBranch}\`, repo \`${orgGithub}/${repoSlug}\`...`);
  const treeApiUrl = `${repoApiUrl}/git/trees/${defaultBranch}?recursive=1`;
  require$$6.debugFn('inspect', 'url: tree', treeApiUrl);
  const treeResponse = await fetch(treeApiUrl, {
    method: 'GET',
    headers: {
      Authorization: `Bearer ${githubToken}`
    }
  });
  const treeText = await treeResponse.text();
  require$$6.debugFn('inspect', 'response: tree', treeText);
  let treeDetails;
  try {
    treeDetails = JSON.parse(treeText);
  } catch {
    logger.logger.fail(`GitHub response contained invalid JSON for default branch of repo ${repoSlug}`);
    logger.logger.error(treeText);
    return {
      ok: false,
      message: 'Invalid JSON response',
      cause: `Server responded with invalid JSON for repo ${repoSlug}`
    };
  }
  if (treeDetails.message) {
    if (treeDetails.message === 'Git Repository is empty.') {
      logger.logger.warn(`GitHub reports the default branch of repo ${repoSlug} to be empty. Moving on to next repo.`);
      return {
        ok: true,
        data: []
      };
    }
    logger.logger.fail('Negative response from GitHub:', treeDetails.message);
    return {
      ok: false,
      message: 'Unexpected error response',
      cause: `GitHub responded with an unexpected error while asking for details on the default branch: ${treeDetails.message}`
    };
  }
  if (!treeDetails.tree || !Array.isArray(treeDetails.tree)) {
    require$$6.debugDir('inspect', {
      treeDetails: {
        tree: treeDetails.tree
      }
    });
    return {
      ok: false,
      message: `Tree response for default branch ${defaultBranch} for ${orgGithub}/${repoSlug} was not a list`
    };
  }
  const files = treeDetails.tree.filter(obj => obj.type === 'blob').map(obj => obj.path);
  return {
    ok: true,
    data: files
  };
}

async function outputScanGithub(result, outputKind) {
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('');
  logger.logger.success('Finished!');
}

async function handleCreateGithubScan({
  all,
  githubApiUrl,
  githubToken,
  interactive,
  orgGithub,
  orgSlug,
  outputKind,
  repos
}) {
  const ghScanCResult = await createScanFromGithub({
    all: Boolean(all),
    githubApiUrl,
    githubToken,
    interactive: Boolean(interactive),
    orgSlug,
    orgGithub,
    outputKind,
    repos: String(repos || '')
  });
  await outputScanGithub(ghScanCResult, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$9
} = constants;
const config$9 = {
  commandName: 'github',
  description: 'Create a scan for given GitHub repo',
  hidden: true,
  // wip
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    all: {
      type: 'boolean',
      description: 'Apply for all known repositories reported by the Socket API. Supersedes `repos`.'
    },
    githubToken: {
      type: 'string',
      description: 'Required GitHub token for authentication.\nMay set environment variable GITHUB_TOKEN or SOCKET_CLI_GITHUB_TOKEN instead.'
    },
    githubApiUrl: {
      type: 'string',
      description: 'Base URL of the GitHub API (default: https://api.github.com)'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    orgGithub: {
      type: 'string',
      description: 'Alternate GitHub Org if the name is different than the Socket Org'
    },
    repos: {
      type: 'string',
      description: 'List of repos to target in a comma-separated format (e.g., repo1,repo2). If not specified, the script will pull the list from Socket and ask you to pick one. Use --all to use them all.'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:create

    This is similar to the \`socket scan create\` command except it pulls the files
    from GitHub. See the help for that command for more details.

    A GitHub Personal Access Token (PAT) will at least need read access to the repo
    ("contents", read-only) for this command to work.

    Note: This command cannot run the \`socket manifest auto\` things because that
    requires local access to the repo while this command runs entirely through the
    GitHub for file access.

    You can use \`socket scan setup\` to configure certain repo flag defaults.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} ./proj
  `
};
const cmdScanGithub = {
  description: config$9.description,
  hidden: config$9.hidden,
  run: run$9
};
async function run$9(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$9,
    importMeta,
    parentName
  });
  const {
    // Lazily access constants.ENV.SOCKET_CLI_GITHUB_TOKEN.
    githubToken = constants.ENV.SOCKET_CLI_GITHUB_TOKEN,
    interactive = true,
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  let {
    all,
    githubApiUrl,
    orgGithub,
    repos
  } = cli.flags;
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  let [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const sockJson = utils.readOrDefaultSocketJson(cwd);
  if (all === undefined) {
    if (sockJson.defaults?.scan?.github?.all !== undefined) {
      all = sockJson.defaults?.scan?.github?.all;
    } else {
      all = false;
    }
  }
  if (!githubApiUrl) {
    if (sockJson.defaults?.scan?.github?.githubApiUrl !== undefined) {
      githubApiUrl = sockJson.defaults.scan.github.githubApiUrl;
    } else {
      githubApiUrl = 'https://api.github.com';
    }
  }
  if (!orgGithub) {
    if (sockJson.defaults?.scan?.github?.orgGithub !== undefined) {
      orgGithub = sockJson.defaults.scan.github.orgGithub;
    } else {
      // Default to Socket org slug. Often that's fine. Vanity and all that.
      orgGithub = orgSlug;
    }
  }
  if (!all && !repos) {
    if (sockJson.defaults?.scan?.github?.repos !== undefined) {
      repos = sockJson.defaults.scan.github.repos;
    } else {
      repos = '';
    }
  }

  // We will also be needing that GitHub token.
  const hasGithubApiToken = !!githubToken;

  // We're going to need an api token to suggest data because those suggestions
  // must come from data we already know. Don't error on missing api token yet.
  // If the api-token is not set, ignore it for the sake of suggestions.
  const hasSocketApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);

  // If the current cwd is unknown and is used as a repo slug anyways, we will
  // first need to register the slug before we can use it.
  // Only do suggestions with an apiToken and when not in dryRun mode
  if (hasSocketApiToken && !dryRun && interactive) {
    if (!orgSlug) {
      const suggestion = await utils.suggestOrgSlug();
      if (suggestion === undefined) {
        await outputScanGithub({
          ok: false,
          message: 'Canceled by user',
          cause: 'Org selector was canceled by user'
        }, outputKind);
        return;
      }
      if (suggestion) {
        orgSlug = suggestion;
      }
    }
  }
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasSocketApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    test: hasGithubApiToken,
    message: 'This command requires a GitHub API token for access',
    fail: 'missing'
  });
  if (!wasValidInput) {
    return;
  }

  // Note exiting earlier to skirt a hidden auth requirement
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$9);
    return;
  }
  await handleCreateGithubScan({
    all: Boolean(all),
    githubApiUrl,
    githubToken,
    interactive: Boolean(interactive),
    orgSlug,
    orgGithub,
    outputKind,
    repos
  });
}

async function fetchOrgFullScanList(config, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  const {
    branch,
    direction,
    from_time,
    orgSlug,
    page,
    perPage,
    repo,
    sort
  } = {
    __proto__: null,
    ...config
  };
  return await utils.handleApiCall(sockSdk.getOrgFullScanList(orgSlug, {
    ...(branch ? {
      branch
    } : {}),
    ...(repo ? {
      repo
    } : {}),
    sort,
    direction,
    from: from_time,
    page: String(page),
    per_page: String(perPage)
  }), {
    desc: 'list of scans'
  });
}

// @ts-ignore
async function outputListScans(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  const options = {
    columns: [{
      field: 'id',
      name: vendor.yoctocolorsCjsExports.magenta('ID')
    }, {
      field: 'report_url',
      name: vendor.yoctocolorsCjsExports.magenta('Scan URL')
    }, {
      field: 'repo',
      name: vendor.yoctocolorsCjsExports.magenta('Repo')
    }, {
      field: 'branch',
      name: vendor.yoctocolorsCjsExports.magenta('Branch')
    }, {
      field: 'created_at',
      name: vendor.yoctocolorsCjsExports.magenta('Created at')
    }]
  };
  const formattedResults = result.data.results.map(d => {
    return {
      id: d.id,
      report_url: vendor.yoctocolorsCjsExports.underline(`${d.html_report_url}`),
      created_at: d.created_at ? new Date(d.created_at).toLocaleDateString('en-us', {
        year: 'numeric',
        month: 'numeric',
        day: 'numeric'
      }) : '',
      repo: d.repo,
      branch: d.branch
    };
  });
  logger.logger.log(vendor.srcExports(options, formattedResults));
}

async function handleListScans({
  branch,
  direction,
  from_time,
  orgSlug,
  outputKind,
  page,
  perPage,
  repo,
  sort
}) {
  const data = await fetchOrgFullScanList({
    branch,
    direction,
    from_time,
    orgSlug,
    page,
    perPage,
    repo,
    sort
  });
  await outputListScans(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$8
} = constants;
const config$8 = {
  commandName: 'list',
  description: 'List the scans for an organization',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    branch: {
      type: 'string',
      description: 'Filter to show only scans with this branch name'
    },
    direction: {
      type: 'string',
      shortFlag: 'd',
      default: 'desc',
      description: 'Direction option (`desc` or `asc`) - Default is `desc`'
    },
    fromTime: {
      type: 'string',
      shortFlag: 'f',
      default: '',
      description: 'From time - as a unix timestamp'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    page: {
      type: 'number',
      shortFlag: 'p',
      default: 1,
      description: 'Page number - Default is 1'
    },
    perPage: {
      type: 'number',
      shortFlag: 'pp',
      default: 30,
      description: 'Results per page - Default is 30'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    sort: {
      type: 'string',
      shortFlag: 's',
      default: 'created_at',
      description: 'Sorting option (`name` or `created_at`) - default is `created_at`'
    },
    untilTime: {
      type: 'string',
      shortFlag: 'u',
      default: '',
      description: 'Until time - as a unix timestamp'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [REPO [BRANCH]]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:list

    Optionally filter by REPO. If you specify a repo, you can also specify a
    branch to filter by. (Note: If you don't specify a repo then you must use
    \`--branch\` to filter by branch across all repos).

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command}
      $ ${command} webtools badbranch --markdown
  `
};
const cmdScanList = {
  description: config$8.description,
  hidden: config$8.hidden,
  run: run$8
};
async function run$8(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$8,
    importMeta,
    parentName
  });
  const {
    branch: branchFlag,
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const noLegacy = !cli.flags['repo'];
  const [repo = '', branchArg = ''] = cli.input;
  const branch = String(branchFlag || branchArg || '');
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: noLegacy,
    message: 'Legacy flags are no longer supported. See v1 migration guide.',
    fail: `received legacy flags`
  }, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'dot is an invalid org, most likely you forgot the org name here?'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    nook: true,
    test: !branchFlag || !branchArg,
    message: 'You should not set --branch and also give a second arg for branch name',
    fail: 'received flag and second arg'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$8);
    return;
  }
  await handleListScans({
    branch: branch ? String(branch) : '',
    direction: String(cli.flags['direction'] || ''),
    from_time: String(cli.flags['fromTime'] || ''),
    orgSlug,
    outputKind,
    page: Number(cli.flags['page'] || 1),
    perPage: Number(cli.flags['perPage'] || 30),
    repo: repo ? String(repo) : '',
    sort: String(cli.flags['sort'] || '')
  });
}

async function fetchScanMetadata(orgSlug, scanId, options) {
  const {
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  return await utils.handleApiCall(sockSdk.getOrgFullScanMetadata(orgSlug, scanId), {
    desc: 'meta data for a full scan'
  });
}

async function outputScanMetadata(result, scanId, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'markdown') {
    logger.logger.log('# Scan meta data\n');
  }
  logger.logger.log(`Scan ID: ${scanId}\n`);
  for (const [key, value] of Object.entries(result.data)) {
    if (['id', 'updated_at', 'organization_id', 'repository_id', 'commit_hash', 'html_report_url'].includes(key)) {
      continue;
    }
    logger.logger.log(`- ${key}:`, value);
  }
  if (outputKind === 'markdown') {
    logger.logger.log(`\nYou can view this report at: [${result.data.html_report_url}](${result.data.html_report_url})\n`);
  } else {
    logger.logger.log(`\nYou can view this report at: ${result.data.html_report_url}]\n`);
  }
}

async function handleOrgScanMetadata(orgSlug, scanId, outputKind) {
  const data = await fetchScanMetadata(orgSlug, scanId);
  await outputScanMetadata(data, scanId, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$7
} = constants;
const config$7 = {
  commandName: 'metadata',
  description: "Get a scan's metadata",
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <SCAN_ID>

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:list

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0 --json
  `
};
const cmdScanMetadata = {
  description: config$7.description,
  hidden: config$7.hidden,
  run: run$7
};
async function run$7(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$7,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const [scanId = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: orgSlug === '.' ? 'dot is an invalid org, most likely you forgot the org name here?' : 'missing'
  }, {
    test: !!scanId,
    message: 'Scan ID to inspect as argument',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$7);
    return;
  }
  await handleOrgScanMetadata(orgSlug, scanId, outputKind);
}

const {
  DOT_SOCKET_DOT_FACTS_JSON
} = constants;
async function outputScanReach(result, {
  cwd,
  outputKind
}) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('');
  logger.logger.success('Reachability analysis completed successfully!');
  logger.logger.info(`Reachability report has been written to: ${path.join(cwd, DOT_SOCKET_DOT_FACTS_JSON)}`);
}

async function handleScanReach({
  cwd,
  interactive: _interactive,
  orgSlug,
  outputKind,
  reachabilityOptions,
  targets
}) {
  // Get supported file names
  const supportedFilesCResult = await fetchSupportedScanFileNames();
  if (!supportedFilesCResult.ok) {
    await outputScanReach(supportedFilesCResult, {
      cwd,
      outputKind
    });
    return;
  }

  // Lazily access constants.spinner.
  const {
    spinner
  } = constants;
  spinner.start('Searching for local manifest files to include in reachability analysis...');
  const supportedFiles = supportedFilesCResult.data;
  const packagePaths = await utils.getPackageFilesForScan(targets, supportedFiles, {
    cwd
  });
  spinner.stop();
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: packagePaths.length > 0,
    fail: 'found no eligible files to analyze',
    message: 'TARGET (file/dir) must contain matching / supported file types for reachability analysis'
  });
  if (!wasValidInput) {
    return;
  }
  logger.logger.success(`Found ${packagePaths.length} local ${words.pluralize('file', packagePaths.length)}`);
  spinner.start('Running reachability analysis...');
  const result = await performReachabilityAnalysis({
    cwd,
    orgSlug,
    packagePaths,
    reachabilityOptions,
    spinner,
    uploadManifests: true
  });
  spinner.stop();
  await outputScanReach(result, {
    cwd,
    outputKind
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$6
} = constants;
const generalFlags = {
  ...flags.commonFlags,
  ...flags.outputFlags,
  cwd: {
    type: 'string',
    description: 'working directory, defaults to process.cwd()'
  },
  org: {
    type: 'string',
    description: 'Force override the organization slug, overrides the default org from config'
  }
};
const config$6 = {
  commandName: 'reach',
  description: 'Compute tier 1 reachability',
  hidden: true,
  flags: {
    ...generalFlags,
    ...reachabilityFlags
  },
  help: command => `
    Usage
      $ ${command} [options] [CWD=.]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:create

    Options
      ${utils.getFlagListOutput(generalFlags)}

    Reachability Options
      ${utils.getFlagListOutput(reachabilityFlags)}

    Runs the Socket reachability analysis without creating a scan in Socket.
    The output is written to .socket.facts.json in the current working directory.

    Note: Manifest files are uploaded to Socket's backend services because the
    reachability analysis requires creating a Software Bill of Materials (SBOM)
    from these files before the analysis can run.

    Examples
      $ ${command}
      $ ${command} ./proj
      $ ${command} ./proj --reach-ecosystems npm,pypi
  `
};
const cmdScanReach = {
  description: config$6.description,
  hidden: config$6.hidden,
  run: run$6
};
async function run$6(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$6,
    importMeta,
    parentName
  });
  const {
    cwd: cwdOverride,
    interactive = true,
    json,
    markdown,
    org: orgFlag,
    reachAnalysisMemoryLimit,
    reachAnalysisTimeout,
    reachDisableAnalytics
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];

  // Process comma-separated values for isMultiple flags.
  const reachEcosystemsRaw = utils.cmdFlagValueToArray(cli.flags['reachEcosystems']);
  const reachExcludePaths = utils.cmdFlagValueToArray(cli.flags['reachExcludePaths']);

  // Validate ecosystem values.
  const reachEcosystems = [];
  const validEcosystems = utils.getEcosystemChoicesForMeow();
  for (const ecosystem of reachEcosystemsRaw) {
    if (!validEcosystems.includes(ecosystem)) {
      throw new Error(`Invalid ecosystem: "${ecosystem}". Valid values are: ${arrays.joinAnd(validEcosystems)}`);
    }
    reachEcosystems.push(ecosystem);
  }
  const processCwd = process.cwd();
  const cwd = cwdOverride && cwdOverride !== processCwd ? path.resolve(processCwd, String(cwdOverride)) : processCwd;

  // Accept zero or more paths. Default to cwd() if none given.
  let targets = cli.input || [cwd];

  // Use suggestTarget if no targets specified and in interactive mode
  if (!targets.length && !dryRun && interactive) {
    targets = await suggestTarget();
  }
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const hasApiToken = utils.hasDefaultToken();
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires an API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$6);
    return;
  }
  await handleScanReach({
    cwd,
    orgSlug,
    outputKind,
    targets,
    interactive,
    reachabilityOptions: {
      reachDisableAnalytics: Boolean(reachDisableAnalytics),
      reachAnalysisTimeout: Number(reachAnalysisTimeout),
      reachAnalysisMemoryLimit: Number(reachAnalysisMemoryLimit),
      reachEcosystems,
      reachExcludePaths
    }
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$5
} = constants;
const config$5 = {
  commandName: 'report',
  description: 'Check whether a scan result passes the organizational policies (security, license)',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    fold: {
      type: 'string',
      default: 'none',
      description: 'Fold reported alerts to some degree'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    reportLevel: {
      type: 'string',
      default: 'warn',
      description: 'Which policy level alerts should be reported'
    },
    short: {
      type: 'boolean',
      default: false,
      description: 'Report only the healthy status'
    },
    license: {
      type: 'boolean',
      default: false,
      description: 'Also report the license policy status. Default: false'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <SCAN_ID> [OUTPUT_PATH]

    API Token Requirements
      - Quota: 2 units
      - Permissions: full-scans:list security-policy:read

    Options
      ${utils.getFlagListOutput(config.flags)}

    When no output path is given the contents is sent to stdout.

    By default the result is a nested object that looks like this:
      \`{
        [ecosystem]: {
          [pkgName]: {
            [version]: {
              [file]: {
                [line:col]: alert
      }}}}\`
    So one alert for each occurrence in every file, version, etc, a huge response.

    You can --fold these up to given level: 'pkg', 'version', 'file', and 'none'.
    For example: \`socket scan report --fold=version\` will dedupe alerts to only
    show one alert of a particular kind, no matter how often it was foud in a
    file or in how many files it was found. At most one per version that has it.

    By default only the warn and error policy level alerts are reported. You can
    override this and request more ('defer' < 'ignore' < 'monitor' < 'warn' < 'error')

    Short responses look like this:
      --json:     \`{healthy:bool}\`
      --markdown: \`healthy = bool\`
      neither:    \`OK/ERR\`

    Examples
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0 --json --fold=version
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0 --license --markdown --short
  `
};
const cmdScanReport = {
  description: config$5.description,
  hidden: config$5.hidden,
  run: run$5
};
async function run$5(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$5,
    importMeta,
    parentName
  });
  const {
    fold = 'none',
    json,
    license,
    markdown,
    org: orgFlag,
    reportLevel = 'warn'
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const [scanId = '', file = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'dot is an invalid org, most likely you forgot the org name here?'
  }, {
    test: !!scanId,
    message: 'Scan ID to report on',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$5);
    return;
  }
  await handleScanReport({
    orgSlug,
    scanId,
    includeLicensePolicy: !!license,
    outputKind,
    filePath: file,
    fold: fold,
    short: !!cli.flags['short'],
    reportLevel: reportLevel
  });
}

async function outputScanConfigResult(result) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log('');
  logger.logger.log('Finished');
  logger.logger.log('');
}

const {
  SOCKET_DEFAULT_BRANCH,
  SOCKET_DEFAULT_REPOSITORY
} = constants;
async function setupScanConfig(cwd, defaultOnReadError = false) {
  const jsonPath = path.join(cwd, `socket.json`);
  if (fs$1.existsSync(jsonPath)) {
    logger.logger.info(`Found socket.json at ${jsonPath}`);
  } else {
    logger.logger.info(`No socket.json found at ${cwd}, will generate a new one`);
  }
  logger.logger.log('');
  logger.logger.log('Note: This tool will set up flag and argument defaults for certain');
  logger.logger.log('      CLI commands. You can still override them by explicitly');
  logger.logger.log('      setting the flag. It is meant to be a convenience tool.');
  logger.logger.log('');
  logger.logger.log('This command will generate a `socket.json` file in the target cwd.');
  logger.logger.log('You can choose to add this file to your repo (handy for collab)');
  logger.logger.log('or to add it to the ignored files, or neither. This file is only');
  logger.logger.log('used in CLI workflows.');
  logger.logger.log('');
  logger.logger.log('Note: For details on a flag you can run `socket <cmd> --help`');
  logger.logger.log('');
  const sockJsonCResult = utils.readSocketJsonSync(cwd, defaultOnReadError);
  if (!sockJsonCResult.ok) {
    return sockJsonCResult;
  }
  const sockJson = sockJsonCResult.data;
  if (!sockJson.defaults) {
    sockJson.defaults = {};
  }
  if (!sockJson.defaults.scan) {
    sockJson.defaults.scan = {};
  }
  const targetCommand = await prompts.select({
    message: 'Which scan command do you want to configure?',
    choices: [{
      name: 'socket scan create',
      value: 'create'
    }, {
      name: 'socket scan github',
      value: 'github'
    }, {
      name: '(cancel)',
      value: '',
      description: 'Exit configurator, make no changes'
    }]
  });
  switch (targetCommand) {
    case 'create':
      {
        if (!sockJson.defaults.scan.create) {
          sockJson.defaults.scan.create = {};
        }
        const result = await configureScan(sockJson.defaults.scan.create, cwd);
        if (!result.ok || result.data.canceled) {
          return result;
        }
        break;
      }
    case 'github':
      {
        if (!sockJson.defaults.scan.github) {
          sockJson.defaults.scan.github = {};
        }
        const result = await configureGithub(sockJson.defaults.scan.github);
        if (!result.ok || result.data.canceled) {
          return result;
        }
        break;
      }
    default:
      {
        return canceledByUser();
      }
  }
  logger.logger.log('');
  logger.logger.log('Setup complete. Writing socket.json');
  logger.logger.log('');
  if (await prompts.select({
    message: `Do you want to write the new config to ${jsonPath} ?`,
    choices: [{
      name: 'yes',
      value: true,
      description: 'Update config'
    }, {
      name: 'no',
      value: false,
      description: 'Do not update the config'
    }]
  })) {
    return await utils.writeSocketJson(cwd, sockJson);
  }
  return canceledByUser();
}
async function configureScan(config, cwd = process.cwd()) {
  const defaultRepoName = await prompts.input({
    message: '(--repo) What repo name (slug) should be reported to Socket for this dir?',
    default: config.repo || (await utils.getRepoName(cwd)) || SOCKET_DEFAULT_REPOSITORY,
    required: false
    // validate: async string => bool
  });
  if (defaultRepoName === undefined) {
    return canceledByUser();
  }
  if (defaultRepoName) {
    // Even if it's SOCKET_DEFAULT_REPOSITORY store it because if we change
    // this default then an existing user probably would not expect the change?
    config.repo = defaultRepoName;
  } else {
    delete config.repo;
  }
  const defaultBranchName = await prompts.input({
    message: '(--branch) What branch name (slug) should be reported to Socket for this dir?',
    default: config.branch || (await utils.gitBranch(cwd)) || SOCKET_DEFAULT_BRANCH,
    required: false
    // validate: async string => bool
  });
  if (defaultBranchName === undefined) {
    return canceledByUser();
  }
  if (defaultBranchName) {
    // Even if it's SOCKET_DEFAULT_BRANCH store it because if we change
    // this default then an existing user probably would not expect the change?
    config.branch = defaultBranchName;
  } else {
    delete config.branch;
  }
  const autoManifest = await prompts.select({
    message: '(--autoManifest) Do you want to run `socket manifest auto` before creating a scan? You would need this for sbt, gradle, etc.',
    choices: [{
      name: 'no',
      value: 'no',
      description: 'Do not generate local manifest files'
    }, {
      name: 'yes',
      value: 'yes',
      description: 'Locally generate manifest files for languages like gradle, sbt, and conda (see `socket manifest auto`), before creating a scan'
    }, {
      name: '(leave default)',
      value: '',
      description: 'Do not store a setting for this'
    }],
    default: config.autoManifest === true ? 'yes' : config.autoManifest === false ? 'no' : ''
  });
  if (autoManifest === undefined) {
    return canceledByUser();
  }
  if (autoManifest === 'yes') {
    config.autoManifest = true;
  } else if (autoManifest === 'no') {
    config.autoManifest = false;
  } else {
    delete config.autoManifest;
  }
  const alwaysReport = await prompts.select({
    message: '(--report) Do you want to enable --report by default?',
    choices: [{
      name: 'no',
      value: 'no',
      description: 'Do not wait for Scan result and report by default'
    }, {
      name: 'yes',
      value: 'yes',
      description: 'After submitting a Scan request, wait for scan to complete, then show a report (like --report would)'
    }, {
      name: '(leave default)',
      value: '',
      description: 'Do not store a setting for this'
    }],
    default: config.report === true ? 'yes' : config.report === false ? 'no' : ''
  });
  if (alwaysReport === undefined) {
    return canceledByUser();
  }
  if (alwaysReport === 'yes') {
    config.report = true;
  } else if (alwaysReport === 'no') {
    config.report = false;
  } else {
    delete config.report;
  }
  return notCanceled();
}
async function configureGithub(config) {
  // Do not store the GitHub API token. Just leads to a security rabbit hole.

  const all = await prompts.select({
    message: '(--all) Do you by default want to fetch all repos from the GitHub API and scan all known repos?',
    choices: [{
      name: 'no',
      value: 'no',
      description: 'Fetch repos if not given and ask which repo to run on'
    }, {
      name: 'yes',
      value: 'yes',
      description: 'Run on all remote repos by default'
    }, {
      name: '(leave default)',
      value: '',
      description: 'Do not store a setting for this'
    }],
    default: config.all === true ? 'yes' : config.all === false ? 'no' : ''
  });
  if (all === undefined) {
    return canceledByUser();
  }
  if (all === 'yes') {
    config.all = true;
  } else if (all === 'no') {
    config.all = false;
  } else {
    delete config.all;
  }
  if (!all) {
    const defaultRepos = await prompts.input({
      message: '(--repos) Please enter the default repos to run this on, leave empty (backspace) to fetch from GitHub and ask interactive',
      default: config.repos,
      required: false
      // validate: async string => bool
    });
    if (defaultRepos === undefined) {
      return canceledByUser();
    }
    if (defaultRepos) {
      config.repos = defaultRepos;
    } else {
      delete config.repos;
    }
  }
  const defaultGithubApiUrl = await prompts.input({
    message: '(--githubApiUrl) Do you want to override the default github url?',
    default: config.githubApiUrl ||
    // Lazily access constants.ENV.GITHUB_API_URL.
    constants.ENV.GITHUB_API_URL,
    required: false
    // validate: async string => bool
  });
  if (defaultGithubApiUrl === undefined) {
    return canceledByUser();
  }
  if (defaultGithubApiUrl &&
  // Lazily access constants.ENV.GITHUB_API_URL.
  defaultGithubApiUrl !== constants.ENV.GITHUB_API_URL) {
    config.githubApiUrl = defaultGithubApiUrl;
  } else {
    delete config.githubApiUrl;
  }
  const defaultOrgGithub = await prompts.input({
    message: '(--orgGithub) Do you want to change the org slug that is used when talking to the GitHub API? Defaults to your Socket org slug.',
    default: config.orgGithub || '',
    required: false
    // validate: async string => bool
  });
  if (defaultOrgGithub === undefined) {
    return canceledByUser();
  }
  if (defaultOrgGithub) {
    config.orgGithub = defaultOrgGithub;
  } else {
    delete config.orgGithub;
  }
  return notCanceled();
}
function canceledByUser() {
  logger.logger.log('');
  logger.logger.info('User canceled');
  logger.logger.log('');
  return {
    ok: true,
    data: {
      canceled: true
    }
  };
}
function notCanceled() {
  return {
    ok: true,
    data: {
      canceled: false
    }
  };
}

async function handleScanConfig(cwd, defaultOnReadError = false) {
  const result = await setupScanConfig(cwd, defaultOnReadError);
  await outputScanConfigResult(result);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$4
} = constants;
const config$4 = {
  commandName: 'setup',
  description: 'Start interactive configurator to customize default flag values for `socket scan` in this dir',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    defaultOnReadError: {
      type: 'boolean',
      description: 'If reading the socket.json fails, just use a default config? Warning: This might override the existing json file!'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [CWD=.]

    Options
      ${utils.getFlagListOutput(config.flags)}

    Interactive configurator to create a local json file in the target directory
    that helps to set flag defaults for \`socket scan create\`.

    This helps to configure the (Socket reported) repo and branch names, as well
    as which branch name is the "default branch" (main, master, etc). This way
    you don't have to specify these flags when creating a scan in this dir.

    This generated configuration file will only be used locally by the CLI. You
    can commit it to the repo (useful for collaboration) or choose to add it to
    your .gitignore all the same. Only this CLI will use it.

    Examples

      $ ${command}
      $ ${command} ./proj
  `
};
const cmdScanSetup = {
  description: config$4.description,
  hidden: config$4.hidden,
  run: run$4
};
async function run$4(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$4,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$4);
    return;
  }
  const {
    defaultOnReadError = false
  } = cli.flags;
  let [cwd = '.'] = cli.input;
  // Note: path.resolve vs .join:
  // If given path is absolute then cwd should not affect it.
  cwd = path.resolve(process.cwd(), cwd);
  await handleScanConfig(cwd, Boolean(defaultOnReadError));
}

async function fetchScan(orgSlug, scanId) {
  const result = await utils.queryApiSafeText(`orgs/${orgSlug}/full-scans/${encodeURIComponent(scanId)}`, 'a scan');
  if (!result.ok) {
    return result;
  }
  const jsonsString = result.data;

  // This is nd-json; each line is a json object
  const lines = jsonsString.split('\n').filter(Boolean);
  let ok = true;
  const data = lines.map(line => {
    try {
      return JSON.parse(line);
    } catch (e) {
      ok = false;
      require$$6.debugFn('error', 'caught: JSON.parse error');
      require$$6.debugDir('inspect', {
        error: e,
        line
      });
      return null;
    }
  });
  if (ok) {
    return {
      ok: true,
      data
    };
  }
  return {
    ok: false,
    message: 'Invalid Socket API response',
    cause: 'The Socket API responded with at least one line that was not valid JSON. Please report if this persists.'
  };
}

const {
  SOCKET_WEBSITE_URL
} = constants;
async function outputScanView(result, orgSlug, scanId, filePath, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (!result.ok) {
    if (outputKind === 'json') {
      logger.logger.log(utils.serializeResultJson(result));
      return;
    }
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (outputKind === 'json' || outputKind === 'text' && filePath && filePath.endsWith('.json')) {
    const json = utils.serializeResultJson(result);
    if (filePath && filePath !== '-') {
      logger.logger.info('Writing json results to', filePath);
      try {
        await fs.writeFile(filePath, json, 'utf8');
        logger.logger.info(`Data successfully written to ${filePath}`);
      } catch (e) {
        process.exitCode = 1;
        logger.logger.fail('There was an error trying to write the markdown to disk');
        logger.logger.error(e);
        logger.logger.log(utils.serializeResultJson({
          ok: false,
          message: 'File Write Failure',
          cause: 'Failed to write json to disk'
        }));
      }
      return;
    }
    logger.logger.log(json);
    return;
  }
  const display = result.data.map(art => {
    const author = Array.isArray(art.author) ? `${art.author[0]}${art.author.length > 1 ? ' et.al.' : ''}` : art.author;
    return {
      type: art.type,
      name: art.name,
      version: art.version,
      author,
      score: JSON.stringify(art.score)
    };
  });
  const md = utils.mdTable(display, ['type', 'version', 'name', 'author', 'score']);
  const report = `
# Scan Details

These are the artifacts and their scores found.

Scan ID: ${scanId}

${md}

View this report at: ${SOCKET_WEBSITE_URL}/dashboard/org/${orgSlug}/sbom/${scanId}
  `.trim() + '\n';
  if (filePath && filePath !== '-') {
    try {
      await fs.writeFile(filePath, report, 'utf8');
      logger.logger.log(`Data successfully written to ${filePath}`);
    } catch (e) {
      process.exitCode = 1;
      logger.logger.fail('There was an error trying to write the markdown to disk');
      logger.logger.error(e);
    }
  } else {
    logger.logger.log(report);
  }
}

async function handleScanView(orgSlug, scanId, filePath, outputKind) {
  const data = await fetchScan(orgSlug, scanId);
  await outputScanView(data, orgSlug, scanId, filePath, outputKind);
}

async function streamScan(orgSlug, scanId, options) {
  const {
    file,
    sdkOptions
  } = {
    __proto__: null,
    ...options
  };
  const sockSdkCResult = await utils.setupSdk(sdkOptions);
  if (!sockSdkCResult.ok) {
    return sockSdkCResult;
  }
  const sockSdk = sockSdkCResult.data;
  logger.logger.info('Requesting data from API...');

  // Note: this will write to stdout or target file. It's not a noop
  return await utils.handleApiCall(sockSdk.getOrgFullScan(orgSlug, scanId, file === '-' ? undefined : file), {
    desc: 'a scan'
  });
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$3
} = constants;
const config$3 = {
  commandName: 'view',
  description: 'View the raw results of a scan',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    stream: {
      type: 'boolean',
      default: false,
      description: 'Only valid with --json. Streams the response as "ndjson" (chunks of valid json blobs).'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] <SCAN_ID> [OUTPUT_FILE]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: full-scans:list

    When no output path is given the contents is sent to stdout.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0
      $ ${command} 000aaaa1-0000-0a0a-00a0-00a0000000a0 ./stream.txt
  `
};
const cmdScanView = {
  description: config$3.description,
  hidden: config$3.hidden,
  run: run$3
};
async function run$3(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$3,
    importMeta,
    parentName
  });
  const {
    json,
    markdown,
    org: orgFlag,
    stream
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  const [scanId = '', file = ''] = cli.input;
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'dot is an invalid org, most likely you forgot the org name here?'
  }, {
    test: !!scanId,
    message: 'Scan ID to view',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The `--json` and `--markdown` flags can not be used at the same time',
    fail: 'bad'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  }, {
    nook: true,
    test: !stream || !!json,
    message: 'You can only use --stream when using --json',
    fail: 'Either remove --stream or add --json'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$3);
    return;
  }
  if (json && stream) {
    await streamScan(orgSlug, scanId, {
      file
    });
  } else {
    await handleScanView(orgSlug, scanId, file, outputKind);
  }
}

const description$1 = 'Manage Socket scans';
const cmdScan = {
  description: description$1,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      create: cmdScanCreate,
      del: cmdScanDel,
      diff: cmdScanDiff,
      github: cmdScanGithub,
      list: cmdScanList,
      metadata: cmdScanMetadata,
      reach: cmdScanReach,
      report: cmdScanReport,
      setup: cmdScanSetup,
      view: cmdScanView
    }, {
      aliases: {
        meta: {
          description: cmdScanMetadata.description,
          hidden: true,
          argv: ['metadata']
        },
        reachability: {
          description: cmdScanReach.description,
          hidden: true,
          argv: ['reach']
        }
      },
      argv,
      description: description$1,
      importMeta,
      name: `${parentName} scan`
    });
  }
};

async function fetchThreatFeed({
  direction,
  ecosystem,
  filter,
  orgSlug,
  page,
  perPage,
  pkg,
  version
}) {
  const queryParams = new URLSearchParams([['direction', direction], ['ecosystem', ecosystem], filter ? ['filter', filter] : ['', ''], ['page_cursor', page], ['per_page', String(perPage)], pkg ? ['name', pkg] : ['', ''], version ? ['version', version] : ['', '']]);
  return await utils.queryApiSafeJson(`orgs/${orgSlug}/threat-feed?${queryParams}`, 'the Threat Feed data');
}

const require$1 = require$$5.createRequire(require('node:url').pathToFileURL(__filename).href);
async function outputThreatFeed(result, outputKind) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
  }
  if (outputKind === 'json') {
    logger.logger.log(utils.serializeResultJson(result));
    return;
  }
  if (!result.ok) {
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  if (!result.data?.results?.length) {
    logger.logger.warn('Did not receive any data to display...');
    return;
  }
  const formattedOutput = formatResults(result.data.results);
  const descriptions = result.data.results.map(d => d.description);

  // Note: this temporarily takes over the terminal (just like `man` does).
  const ScreenWidget = /*@__PURE__*/require$1('../external/blessed/lib/widgets/screen.js');
  // Lazily access constants.blessedOptions.
  const screen = new ScreenWidget({
    ...constants.blessedOptions
  });
  // Register these keys first so you can always exit, even when it gets stuck
  // If we don't do this and the code crashes, the user must hard-kill the
  // node process just to exit it. That's very bad UX.
  // eslint-disable-next-line n/no-process-exit
  screen.key(['escape', 'q', 'C-c'], () => process.exit(0));
  const TableWidget = /*@__PURE__*/require$1('../external/blessed-contrib/lib/widget/table.js');
  const detailsBoxHeight = 20; // bottom N rows for details box
  const tipsBoxHeight = 1; // 1 row for tips box

  const table = new TableWidget({
    keys: 'true',
    fg: 'white',
    selectedFg: 'white',
    selectedBg: 'magenta',
    interactive: 'true',
    label: 'Threat feed',
    width: '100%',
    top: 0,
    bottom: detailsBoxHeight + tipsBoxHeight,
    border: {
      type: 'line',
      fg: 'cyan'
    },
    columnWidth: [10, 30, 20, 18, 15, 200],
    // TODO: The truncation doesn't seem to work too well yet but when we add
    //       `pad` alignment fails, when we extend columnSpacing alignment fails.
    columnSpacing: 1,
    truncate: '_'
  });
  const BoxWidget = /*@__PURE__*/require$1('../external/blessed/lib/widgets/box.js');
  const tipsBox = new BoxWidget({
    bottom: detailsBoxHeight,
    // sits just above the details box
    height: tipsBoxHeight,
    width: '100%',
    style: {
      fg: 'yellow',
      bg: 'black'
    },
    tags: true,
    content: '↑/↓: Move    Enter: Select    q/ESC: Quit'
  });
  const detailsBox = new BoxWidget({
    bottom: 0,
    height: detailsBoxHeight,
    width: '100%',
    border: {
      type: 'line',
      fg: 'cyan'
    },
    label: 'Details',
    content: 'Use arrow keys to navigate. Press Enter to select a threat. Press q to exit.',
    style: {
      fg: 'white'
    }
  });
  table.setData({
    headers: [' Ecosystem', ' Name', '  Version', '  Threat type', '  Detected at', ' Details'],
    data: formattedOutput
  });

  // Initialize details box with the first selection if available
  if (formattedOutput.length > 0) {
    const selectedRow = formattedOutput[0];
    if (selectedRow) {
      detailsBox.setContent(formatDetailBox(selectedRow, descriptions, 0));
    }
  }

  // allow control the table with the keyboard
  table.focus();

  // Stacking order: table (top), tipsBox (middle), detailsBox (bottom)
  screen.append(table);
  screen.append(tipsBox);
  screen.append(detailsBox);

  // Update details box when selection changes
  table.rows.on('select item', () => {
    const selectedIndex = table.rows.selected;
    if (selectedIndex !== undefined && selectedIndex >= 0) {
      const selectedRow = formattedOutput[selectedIndex];
      if (selectedRow) {
        // Note: the spacing works around issues with the table; it refuses to pad!
        detailsBox.setContent(formatDetailBox(selectedRow, descriptions, selectedIndex));
        screen.render();
      }
    }
  });
  screen.render();
  screen.key(['return'], () => {
    const selectedIndex = table.rows.selected;
    screen.destroy();
    const selectedRow = formattedOutput[selectedIndex];
    logger.logger.log('Last selection:\n', selectedRow);
  });
}
function formatDetailBox(selectedRow, descriptions, selectedIndex) {
  return `Ecosystem:    ${selectedRow[0]?.trim()}\n` + `Name:         ${selectedRow[1]?.trim()}\n` + `Version:      ${selectedRow[2]?.trim()}\n` + `Threat type:  ${selectedRow[3]?.trim()}\n` + `Detected at:  ${selectedRow[4]?.trim()}\n` + `Details:      ${selectedRow[5]?.trim()}\n` + `Description:  ${descriptions[selectedIndex]?.trim()}`;
}
function formatResults(data) {
  return data.map(d => {
    const ecosystem = d.purl.split('pkg:')[1].split('/')[0];
    const name = d.purl.split('/')[1].split('@')[0];
    const version = d.purl.split('@')[1];
    const timeDiff = utils.msAtHome(d.createdAt);

    // Note: the spacing works around issues with the table; it refuses to pad!
    return [ecosystem, decodeURIComponent(name), ` ${version}`, ` ${d.threatType}`, ` ${timeDiff}`, d.locationHtmlUrl];
  });
}

async function handleThreatFeed({
  direction,
  ecosystem,
  filter,
  orgSlug,
  outputKind,
  page,
  perPage,
  pkg,
  version
}) {
  const data = await fetchThreatFeed({
    direction,
    ecosystem,
    filter,
    orgSlug,
    page,
    perPage,
    pkg,
    version
  });
  await outputThreatFeed(data, outputKind);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$2
} = constants;
const ECOSYSTEMS = new Set(['gem', 'golang', 'maven', 'npm', 'nuget', 'pypi']);
const TYPE_FILTERS = new Set(['anom', 'c', 'fp', 'joke', 'mal', 'secret', 'spy', 'tp', 'typo', 'u', 'vuln']);
const config$2 = {
  commandName: 'threat-feed',
  description: '[Beta] View the threat feed',
  hidden: false,
  flags: {
    ...flags.commonFlags,
    ...flags.outputFlags,
    direction: {
      type: 'string',
      default: 'desc',
      description: 'Order asc or desc by the createdAt attribute'
    },
    eco: {
      type: 'string',
      default: '',
      description: 'Only show threats for a particular ecosystem'
    },
    filter: {
      type: 'string',
      default: 'mal',
      description: 'Filter what type of threats to return'
    },
    interactive: {
      type: 'boolean',
      default: true,
      description: 'Allow for interactive elements, asking for input. Use --no-interactive to prevent any input questions, defaulting them to cancel/no.'
    },
    org: {
      type: 'string',
      description: 'Force override the organization slug, overrides the default org from config'
    },
    page: {
      type: 'string',
      default: '1',
      description: 'Page token'
    },
    perPage: {
      type: 'number',
      shortFlag: 'pp',
      default: 30,
      description: 'Number of items per page'
    },
    pkg: {
      type: 'string',
      default: '',
      description: 'Filter by this package name'
    },
    version: {
      type: 'string',
      default: '',
      description: 'Filter by this package version'
    }
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [ECOSYSTEM] [TYPE_FILTER]

    API Token Requirements
      - Quota: 1 unit
      - Permissions: threat-feed:list
      - Special access

    This feature requires a Threat Feed license. Please contact
    sales@socket.dev if you are interested in purchasing this access.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Valid ecosystems:

      - gem
      - golang
      - maven
      - npm
      - nuget
      - pypi

    Valid type filters:

      - anom    Anomaly
      - c       Do not filter
      - fp      False Positives
      - joke    Joke / Fake
      - mal     Malware and Possible Malware [default]
      - secret  Secrets
      - spy     Telemetry
      - tp      False Positives and Unreviewed
      - typo    Typo-squat
      - u       Unreviewed
      - vuln    Vulnerability

    Note: if you filter by package name or version, it will do so for anything
          unless you also filter by that ecosystem and/or package name. When in
          doubt, look at the threat-feed and see the names in the name/version
          column. That's what you want to search for.

    You can put filters as args instead, we'll try to match the strings with the
    correct filter type but since this would not allow you to search for a package
    called "mal", you can also specify the filters through flags.

    First arg that matches a typo, eco, or version enum is used as such. First arg
    that matches none of them becomes the package name filter. Rest is ignored.

    Note: The version filter is a prefix search, pkg name is a substring search.

    Examples
      $ ${command}
      $ ${command} maven --json
      $ ${command} typo
      $ ${command} npm joke 1.0.0 --perPage=5 --page=2 --direction=asc
  `
};
const cmdThreatFeed = {
  description: config$2.description,
  hidden: config$2.hidden,
  run: run$2
};
async function run$2(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$2,
    importMeta,
    parentName
  });
  const {
    eco,
    json,
    markdown,
    org: orgFlag,
    pkg,
    type: typef,
    version
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  const interactive = !!cli.flags['interactive'];
  let ecoFilter = String(eco || '');
  let versionFilter = String(version || '');
  let typeFilter = String(typef || '');
  let nameFilter = String(pkg || '');
  const argSet = new Set(cli.input);
  cli.input.some(str => {
    if (ECOSYSTEMS.has(str)) {
      ecoFilter = str;
      argSet.delete(str);
      return true;
    }
  });
  cli.input.some(str => {
    if (/^v?\d+\.\d+\.\d+$/.test(str)) {
      versionFilter = str;
      argSet.delete(str);
      return true;
    }
  });
  cli.input.some(str => {
    if (TYPE_FILTERS.has(str)) {
      typeFilter = str;
      argSet.delete(str);
      return true;
    }
  });
  const haves = new Set([ecoFilter, versionFilter, typeFilter]);
  cli.input.some(str => {
    if (!haves.has(str)) {
      nameFilter = str;
      argSet.delete(str);
      return true;
    }
  });
  if (argSet.size) {
    logger.logger.info(`Warning: ignoring these excessive args: ${Array.from(argSet).join(', ')}`);
  }
  const hasApiToken = utils.hasDefaultToken();
  const [orgSlug] = await utils.determineOrgSlug(String(orgFlag || ''), interactive, dryRun);
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    nook: true,
    test: !!orgSlug,
    message: 'Org name by default setting, --org, or auto-discovered',
    fail: 'missing'
  }, {
    nook: true,
    test: !json || !markdown,
    message: 'The json and markdown flags cannot be both set, pick one',
    fail: 'omit one'
  }, {
    nook: true,
    test: hasApiToken,
    message: 'This command requires a Socket API token for access',
    fail: 'try `socket login`'
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$2);
    return;
  }
  await handleThreatFeed({
    direction: String(cli.flags['direction'] || 'desc'),
    ecosystem: ecoFilter,
    filter: typeFilter,
    outputKind,
    orgSlug,
    page: String(cli.flags['page'] || '1'),
    perPage: Number(cli.flags['perPage']) || 30,
    pkg: nameFilter,
    version: versionFilter
  });
}

async function outputUninstallCompletion(result, targetName) {
  if (!result.ok) {
    process.exitCode = result.code ?? 1;
    logger.logger.fail(utils.failMsgWithBadge(result.message, result.cause));
    return;
  }
  logger.logger.log(result.message);
  logger.logger.log('');
  logger.logger.log('To remove the tab completion from the current shell (instance of bash) you');
  logger.logger.log('can run this command (due to a bash limitation NodeJS cannot do this):');
  logger.logger.log('');
  logger.logger.log(`    complete -r ${targetName}`);
  logger.logger.log('');
  logger.logger.log('Next time you open a terminal it should no longer be there, regardless.');
  logger.logger.log('');
  if (result.data.left.length) {
    logger.logger.log('Detected more Socket Alias completions left in bashrc. Run `socket uninstall <cmd>` to remove them too.');
    logger.logger.log('');
    result.data.left.forEach(str => {
      logger.logger.log(`  - \`${str}\``);
    });
    logger.logger.log('');
  }
}

async function teardownTabCompletion(targetName) {
  const result = utils.getBashrcDetails(targetName);
  if (!result.ok) {
    return result;
  }
  const {
    completionCommand,
    sourcingCommand,
    toAddToBashrc
  } = result.data;

  // Remove from ~/.bashrc if found
  // Lazily access constants.homePath
  const bashrc = constants.homePath ? path.join(constants.homePath, '.bashrc') : '';
  if (bashrc && fs$1.existsSync(bashrc)) {
    const content = fs$1.readFileSync(bashrc, 'utf8');
    if (content.includes(toAddToBashrc)) {
      const newContent = content
      // Try to remove the whole thing with comment first
      .replaceAll(toAddToBashrc, '')
      // Comment may have been edited away, try to remove the command at least
      .replaceAll(sourcingCommand, '').replaceAll(completionCommand, '');
      fs$1.writeFileSync(bashrc, newContent, 'utf8');
      return {
        ok: true,
        data: {
          action: 'removed',
          left: findRemainingCompletionSetups(newContent)
        },
        message: 'Removed completion from ~/.bashrc'
      };
    } else {
      const left = findRemainingCompletionSetups(content);
      return {
        ok: true,
        data: {
          action: 'missing',
          left
        },
        message: `Completion was not found in ~/.bashrc${left.length ? ' (you may need to manually edit your .bashrc to clean this up...)' : ''}`
      };
    }
  } else {
    return {
      ok: true,
      // Eh. I think this makes most sense.
      data: {
        action: 'not found',
        left: []
      },
      message: '~/.bashrc not found, skipping'
    };
  }
}
function findRemainingCompletionSetups(bashrc) {
  return bashrc.split('\n').map(s => s.trim()).filter(s => s.startsWith(utils.COMPLETION_CMD_PREFIX)).map(s => s.slice(utils.COMPLETION_CMD_PREFIX.length).trim());
}

async function handleUninstallCompletion(targetName) {
  const result = await teardownTabCompletion(targetName);
  await outputUninstallCompletion(result, targetName);
}

const {
  DRY_RUN_BAILING_NOW: DRY_RUN_BAILING_NOW$1
} = constants;
const config$1 = {
  commandName: 'completion',
  description: 'Uninstall bash completion for Socket CLI',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} [options] [COMMAND_NAME=socket]

    Uninstalls bash tab completion for the Socket CLI. This will:
    1. Remove tab completion from your current shell for given command
    2. Remove the setup for given command from your ~/.bashrc

    The optional name is required if you installed tab completion for an alias
    other than the default "socket". This will NOT remove the command, only the
    tab completion that is registered for it in bash.

    Options
      ${utils.getFlagListOutput(config.flags)}

    Examples

      $ ${command}
      $ ${command} sd
  `
};
const cmdUninstallCompletion = {
  description: config$1.description,
  hidden: config$1.hidden,
  run: run$1
};
async function run$1(argv, importMeta, {
  parentName
}) {
  const cli = utils.meowOrExit({
    argv,
    config: config$1,
    importMeta,
    parentName
  });
  const dryRun = !!cli.flags['dryRun'];
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW$1);
    return;
  }
  const targetName = cli.input[0] || 'socket';
  await handleUninstallCompletion(String(targetName));
}

const description = 'Uninstall Socket CLI tab completion';
const cmdUninstall = {
  description,
  hidden: false,
  async run(argv, importMeta, {
    parentName
  }) {
    await utils.meowWithSubcommands({
      completion: cmdUninstallCompletion
    }, {
      argv,
      description,
      importMeta,
      name: `${parentName} uninstall`
    });
  }
};

function addSocketWrapper(file) {
  return fs$1.appendFile(file, 'alias npm="socket npm"\nalias npx="socket npx"\n', err => {
    if (err) {
      return new Error(`There was an error setting up the alias: ${err}`);
    }
    logger.logger.success(`The alias was added to ${file}. Running 'npm install' will now be wrapped in Socket's "safe npm" 🎉`);
    logger.logger.log(`  If you want to disable it at any time, run \`socket wrapper --disable\``);
    logger.logger.log('');
    logger.logger.info(`This will only be active in new terminal sessions going forward.`);
    logger.logger.log(`  You will need to restart your terminal or run this command to activate the alias in the current session:`);
    logger.logger.log('');
    logger.logger.log(`    source ${file}`);
    logger.logger.log('');
    logger.logger.log(`(You only need to do this once)`);
  });
}

function checkSocketWrapperSetup(file) {
  const fileContent = fs$1.readFileSync(file, 'utf8');
  const linesWithSocketAlias = fileContent.split('\n').filter(l => l === 'alias npm="socket npm"' || l === 'alias npx="socket npx"');
  if (linesWithSocketAlias.length) {
    logger.logger.log(`The Socket npm/npx wrapper is set up in your bash profile (${file}).`);
    logger.logger.log('');
    logger.logger.log(`If you haven't already since enabling; Restart your terminal or run this command to activate it in the current session:`);
    logger.logger.log('');
    logger.logger.log(`    source ${file}`);
    logger.logger.log('');
    return true;
  }
  return false;
}

async function postinstallWrapper() {
  // Lazily access constants.bashRcPath and constants.zshRcPath.
  const {
    bashRcPath,
    zshRcPath
  } = constants;
  const socketWrapperEnabled = fs$1.existsSync(bashRcPath) && checkSocketWrapperSetup(bashRcPath) || fs$1.existsSync(zshRcPath) && checkSocketWrapperSetup(zshRcPath);
  if (!socketWrapperEnabled) {
    await installSafeNpm(`
The Socket CLI is now successfully installed! 🎉

To better protect yourself against supply-chain attacks, our "safe npm" wrapper can warn you about malicious packages whenever you run 'npm install'.

Do you want to install "safe npm" (this will create an alias to the socket-npm command)?
    `.trim());
  }

  // Attempt to update the existing tab completion
  let updatedTabCompletion = false;
  try {
    const details = utils.getBashrcDetails(''); // Note: command is not relevant, we just want the config path
    if (details.ok) {
      if (fs$1.existsSync(details.data.targetPath)) {
        // Replace the file with the one from this installation
        const result = updateInstalledTabCompletionScript(details.data.targetPath);
        if (result.ok) {
          // This will work no matter what alias(es) were registered since that
          // is controlled by bashrc and they all share the same tab script.
          logger.logger.success('Updated the installed Socket tab completion script');
          updatedTabCompletion = true;
        }
      }
    }
  } catch (e) {
    require$$6.debugFn('error', 'caught: tab completion setup error');
    require$$6.debugDir('inspect', {
      error: e
    });
    // Ignore. Skip tab completion setup.
  }
  if (!updatedTabCompletion) {
    // Setting up tab completion requires bashrc modification. I'm not sure if
    // it's cool to just do that from an npm install...
    logger.logger.log('Run `socket install completion` to setup bash tab completion');
  }
}
async function installSafeNpm(query) {
  logger.logger.log(`
 _____         _       _
|   __|___ ___| |_ ___| |_
|__   | . |  _| '_| -_|  _|
|_____|___|___|_,_|___|_|

`);
  if (await prompts.confirm({
    message: query,
    default: true
  })) {
    // Lazily access constants.bashRcPath and constants.zshRcPath.
    const {
      bashRcPath,
      zshRcPath
    } = constants;
    try {
      if (fs$1.existsSync(bashRcPath)) {
        addSocketWrapper(bashRcPath);
      }
      if (fs$1.existsSync(zshRcPath)) {
        addSocketWrapper(zshRcPath);
      }
    } catch (e) {
      throw new Error(`There was an issue setting up the alias: ${e?.['message']}`);
    }
  }
}

function removeSocketWrapper(filepath) {
  let content;
  try {
    content = fs$1.readFileSync(filepath, 'utf8');
  } catch (e) {
    logger.logger.fail(`There was an error removing the alias${e ? ':' : '.'}`);
    if (e) {
      logger.logger.error(e);
    }
    return;
  }
  const linesWithoutSocketAlias = content.split('\n').filter(l => l !== 'alias npm="socket npm"' && l !== 'alias npx="socket npx"');
  const updatedContent = linesWithoutSocketAlias.join('\n');
  try {
    fs$1.writeFileSync(filepath, updatedContent, 'utf8');
  } catch (e) {
    if (e) {
      logger.logger.error(e);
    }
    return;
  }
  logger.logger.success(`The alias was removed from ${filepath}. Running 'npm install' will now run the standard npm command in new terminals going forward.`);
  logger.logger.log('');
  logger.logger.info(`Note: We cannot deactivate the alias from current terminal sessions. You have to restart existing terminal sessions to finalize this step.`);
}

const {
  DRY_RUN_BAILING_NOW
} = constants;
const config = {
  commandName: 'wrapper',
  description: 'Enable or disable the Socket npm/npx wrapper',
  hidden: false,
  flags: {
    ...flags.commonFlags
  },
  help: (command, config) => `
    Usage
      $ ${command} <"on" | "off">

    Options
      ${utils.getFlagListOutput(config.flags)}

    While enabled, the wrapper makes it so that when you call npm/npx on your
    machine, it will automatically actually run \`socket npm\` / \`socket npx\`
    instead.

    Examples
      $ ${command} on
      $ ${command} off
  `
};
const cmdWrapper = {
  description: config.description,
  hidden: config.hidden,
  run
};
async function run(argv, importMeta, {
  parentName
}) {
  // I don't think meow would mess with this but ...
  if (argv[0] === '--postinstall') {
    await postinstallWrapper();
    return;
  }
  const cli = utils.meowOrExit({
    argv,
    config,
    importMeta,
    parentName
  });

  // TODO: Implement json/md further.
  const {
    json,
    markdown
  } = cli.flags;
  const dryRun = !!cli.flags['dryRun'];
  let enable = false;
  let disable = false;
  const [arg] = cli.input;
  if (arg === 'on' || arg === 'enable' || arg === 'enabled') {
    enable = true;
    disable = false;
  } else if (arg === 'off' || arg === 'disable' || arg === 'disabled') {
    enable = false;
    disable = true;
  }
  const outputKind = utils.getOutputKind(json, markdown);
  const wasValidInput = utils.checkCommandInput(outputKind, {
    test: enable || disable,
    message: 'Must specify "on" or "off" argument',
    fail: 'missing'
  }, {
    nook: true,
    test: cli.input.length <= 1,
    message: 'expecting exactly one argument',
    fail: `got multiple`
  });
  if (!wasValidInput) {
    return;
  }
  if (dryRun) {
    logger.logger.log(DRY_RUN_BAILING_NOW);
    return;
  }

  // Lazily access constants.bashRcPath and constants.zshRcPath.
  const {
    bashRcPath,
    zshRcPath
  } = constants;
  if (enable) {
    if (fs$1.existsSync(bashRcPath) && !checkSocketWrapperSetup(bashRcPath)) {
      addSocketWrapper(bashRcPath);
    }
    if (fs$1.existsSync(zshRcPath) && !checkSocketWrapperSetup(zshRcPath)) {
      addSocketWrapper(zshRcPath);
    }
  } else {
    if (fs$1.existsSync(bashRcPath)) {
      removeSocketWrapper(bashRcPath);
    }
    if (fs$1.existsSync(zshRcPath)) {
      removeSocketWrapper(zshRcPath);
    }
  }
  if (!fs$1.existsSync(bashRcPath) && !fs$1.existsSync(zshRcPath)) {
    logger.logger.fail('There was an issue setting up the alias in your bash profile');
  }
}

const rootCommands = {
  analytics: cmdAnalytics,
  'audit-log': cmdAuditLog,
  ci: cmdCI,
  cdxgen: cmdManifestCdxgen,
  config: cmdConfig,
  deps: cmdOrganizationDependencies,
  fix: cmdFix,
  install: cmdInstall,
  json: cmdJson,
  license: cmdOrganizationPolicyLicense,
  login: cmdLogin,
  logout: cmdLogout,
  manifest: cmdManifest,
  npm: cmdNpm,
  npx: cmdNpx,
  oops: cmdOops,
  optimize: cmdOptimize,
  organization: cmdOrganization,
  package: cmdPackage,
  'raw-npm': cmdRawNpm,
  'raw-npx': cmdRawNpx,
  repository: cmdRepository,
  scan: cmdScan,
  security: cmdOrganizationPolicySecurity,
  'threat-feed': cmdThreatFeed,
  uninstall: cmdUninstall,
  wrapper: cmdWrapper
};
const rootAliases = {
  audit: {
    description: cmdAuditLog.description,
    hidden: true,
    argv: ['audit-log']
  },
  auditLog: {
    description: cmdAuditLog.description,
    hidden: true,
    argv: ['audit-log']
  },
  auditLogs: {
    description: cmdAuditLog.description,
    hidden: true,
    argv: ['audit-log']
  },
  ['audit-logs']: {
    description: cmdAuditLog.description,
    hidden: true,
    argv: ['audit-log']
  },
  feed: {
    description: cmdThreatFeed.description,
    hidden: true,
    argv: ['threat-feed']
  },
  org: {
    description: cmdOrganization.description,
    hidden: true,
    argv: ['organization']
  },
  orgs: {
    description: cmdOrganization.description,
    hidden: true,
    argv: ['organization']
  },
  organizations: {
    description: cmdOrganization.description,
    hidden: true,
    argv: ['organization']
  },
  organisation: {
    description: cmdOrganization.description,
    hidden: true,
    argv: ['organization']
  },
  organisations: {
    description: cmdOrganization.description,
    hidden: true,
    argv: ['organization']
  },
  pkg: {
    description: cmdPackage.description,
    hidden: true,
    argv: ['package']
  },
  repo: {
    description: cmdRepository.description,
    hidden: true,
    argv: ['repos']
  },
  repos: {
    description: cmdRepository.description,
    hidden: true,
    argv: ['repos']
  },
  repositories: {
    description: cmdRepository.description,
    hidden: true,
    argv: ['repos']
  }
};

const __filename$1 = require$$0.fileURLToPath(require('node:url').pathToFileURL(__filename).href);
void (async () => {
  const registryUrl = vendor.registryUrl();
  await vendor.updater({
    authInfo: vendor.registryAuthTokenExports(registryUrl, {
      recursive: true
    }),
    // Lazily access constants.SOCKET_CLI_BIN_NAME.
    name: constants.SOCKET_CLI_BIN_NAME,
    registryUrl,
    ttl: 86_400_000 /* 24 hours in milliseconds */,

    // Lazily access constants.ENV.INLINED_SOCKET_CLI_VERSION.
    version: constants.ENV.INLINED_SOCKET_CLI_VERSION
  });
  try {
    await utils.meowWithSubcommands(rootCommands, {
      aliases: rootAliases,
      argv: process.argv.slice(2),
      // Lazily access constants.SOCKET_CLI_BIN_NAME.
      name: constants.SOCKET_CLI_BIN_NAME,
      importMeta: {
        url: `${require$$0.pathToFileURL(__filename$1)}`
      }
    });
  } catch (e) {
    process.exitCode = 1;
    require$$6.debugFn('error', 'Uncaught error (BAD!):');
    require$$6.debugDir('inspect', {
      error: e
    });
    let errorBody;
    let errorTitle;
    let errorMessage = '';
    if (e instanceof utils.AuthError) {
      errorTitle = 'Authentication error';
      errorMessage = e.message;
    } else if (e instanceof utils.InputError) {
      errorTitle = 'Invalid input';
      errorMessage = e.message;
      errorBody = e.body;
    } else if (e instanceof Error) {
      errorTitle = 'Unexpected error';
      errorMessage = vendor.messageWithCauses(e);
      errorBody = vendor.stackWithCauses(e);
    } else {
      errorTitle = 'Unexpected error with no details';
    }

    // Try to parse the flags, find out if --json is set.
    const isJson = (() => {
      const cli = vendor.meow({
        argv: process.argv.slice(2),
        // Prevent meow from potentially exiting early.
        autoHelp: false,
        autoVersion: false,
        flags: {},
        importMeta: {
          url: `${require$$0.pathToFileURL(__filename$1)}`
        }
      });
      return !!cli.flags['json'];
    })();
    if (isJson) {
      logger.logger.log(utils.serializeResultJson({
        ok: false,
        message: errorTitle,
        cause: errorMessage
      }));
    } else {
      // Add 2 newlines in stderr to bump below any spinner.
      logger.logger.error('\n');
      logger.logger.fail(utils.failMsgWithBadge(errorTitle, errorMessage));
      if (errorBody) {
        require$$6.debugDir('inspect', {
          errorBody
        });
      }
    }
    await utils.captureException(e);
  }
})();
//# debugId=179b5d73-a525-4e97-bef0-f189ffa26e77
//# sourceMappingURL=cli.js.map
